{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":773.161209,"end_time":"2025-06-03T09:59:54.014100","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-03T09:47:00.852891","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2025-06-10T09:20:30.916033Z","iopub.execute_input":"2025-06-10T09:20:30.916310Z","iopub.status.idle":"2025-06-10T09:20:30.922862Z","shell.execute_reply.started":"2025-06-10T09:20:30.916288Z","shell.execute_reply":"2025-06-10T09:20:30.922295Z"},"papermill":{"duration":0.016021,"end_time":"2025-06-03T09:47:05.915432","exception":false,"start_time":"2025-06-03T09:47:05.899411","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\n/kaggle/input/drw-crypto-market-prediction/train.parquet\n/kaggle/input/drw-crypto-market-prediction/test.parquet\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# DRW - Crypto Market Prediction\n\nThis notebook documents all the steps done in this project.\n\nTimeline:\n- 10/06/25: 0.05031\n    - Reorganize notebooks.\n    - Test training with GPU - Way faster than CPU.\n    - Implement feature elimination using GPU.\n    - Tested with Linear Models - will be extremely slow in iteration.\n- ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\nfrom scipy.stats import pearsonr\nimport polars as pl\nimport numpy as np\nfrom tqdm import tqdm\nfrom datetime import datetime","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.595897,"end_time":"2025-06-03T09:47:08.514965","exception":false,"start_time":"2025-06-03T09:47:05.919068","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:30.925071Z","iopub.execute_input":"2025-06-10T09:20:30.925244Z","iopub.status.idle":"2025-06-10T09:20:32.415011Z","shell.execute_reply.started":"2025-06-10T09:20:30.925230Z","shell.execute_reply":"2025-06-10T09:20:32.414353Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# 1 Data","metadata":{"papermill":{"duration":0.002673,"end_time":"2025-06-03T09:47:08.520904","exception":false,"start_time":"2025-06-03T09:47:08.518231","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = pl.read_parquet(\n    \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n).sort(\"timestamp\", descending = False)\ndata","metadata":{"papermill":{"duration":15.905967,"end_time":"2025-06-03T09:47:24.429666","exception":false,"start_time":"2025-06-03T09:47:08.523699","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:32.416297Z","iopub.execute_input":"2025-06-10T09:20:32.416958Z","iopub.status.idle":"2025-06-10T09:20:51.269111Z","shell.execute_reply.started":"2025-06-10T09:20:32.416937Z","shell.execute_reply":"2025-06-10T09:20:51.268335Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"shape: (525_887, 897)\n┌─────────┬─────────┬─────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────────┐\n│ bid_qty ┆ ask_qty ┆ buy_qty ┆ sell_qty ┆ … ┆ X889     ┆ X890     ┆ label    ┆ timestamp    │\n│ ---     ┆ ---     ┆ ---     ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---          │\n│ f64     ┆ f64     ┆ f64     ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ datetime[ns] │\n╞═════════╪═════════╪═════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════════╡\n│ 15.283  ┆ 8.425   ┆ 176.405 ┆ 44.984   ┆ … ┆ 0.159183 ┆ 0.530636 ┆ 0.562539 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:00:00     │\n│ 38.59   ┆ 2.336   ┆ 525.846 ┆ 321.95   ┆ … ┆ 0.158963 ┆ 0.530269 ┆ 0.533686 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:01:00     │\n│ 0.442   ┆ 60.25   ┆ 159.227 ┆ 136.369  ┆ … ┆ 0.158744 ┆ 0.529901 ┆ 0.546505 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:02:00     │\n│ 4.865   ┆ 21.016  ┆ 335.742 ┆ 124.963  ┆ … ┆ 0.158524 ┆ 0.529534 ┆ 0.357703 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:03:00     │\n│ 27.158  ┆ 3.451   ┆ 98.411  ┆ 44.407   ┆ … ┆ 0.158304 ┆ 0.529167 ┆ 0.362452 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:04:00     │\n│ …       ┆ …       ┆ …       ┆ …        ┆ … ┆ …        ┆ …        ┆ …        ┆ …            │\n│ 4.163   ┆ 6.805   ┆ 39.037  ┆ 55.351   ┆ … ┆ 0.136494 ┆ 0.243172 ┆ 0.396289 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:55:00     │\n│ 2.29    ┆ 4.058   ┆ 110.201 ┆ 67.171   ┆ … ┆ 0.136305 ┆ 0.243004 ┆ 0.328993 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:56:00     │\n│ 5.237   ┆ 3.64    ┆ 70.499  ┆ 30.753   ┆ … ┆ 0.136117 ┆ 0.242836 ┆ 0.189909 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:57:00     │\n│ 5.731   ┆ 4.901   ┆ 22.365  ┆ 52.195   ┆ … ┆ 0.135928 ┆ 0.242668 ┆ 0.410831 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:58:00     │\n│ 3.925   ┆ 3.865   ┆ 86.585  ┆ 217.137  ┆ … ┆ 0.135741 ┆ 0.242501 ┆ 0.731542 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:59:00     │\n└─────────┴─────────┴─────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (525_887, 897)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>bid_qty</th><th>ask_qty</th><th>buy_qty</th><th>sell_qty</th><th>volume</th><th>X1</th><th>X2</th><th>X3</th><th>X4</th><th>X5</th><th>X6</th><th>X7</th><th>X8</th><th>X9</th><th>X10</th><th>X11</th><th>X12</th><th>X13</th><th>X14</th><th>X15</th><th>X16</th><th>X17</th><th>X18</th><th>X19</th><th>X20</th><th>X21</th><th>X22</th><th>X23</th><th>X24</th><th>X25</th><th>X26</th><th>X27</th><th>X28</th><th>X29</th><th>X30</th><th>X31</th><th>X32</th><th>&hellip;</th><th>X856</th><th>X857</th><th>X858</th><th>X859</th><th>X860</th><th>X861</th><th>X862</th><th>X863</th><th>X864</th><th>X865</th><th>X866</th><th>X867</th><th>X868</th><th>X869</th><th>X870</th><th>X871</th><th>X872</th><th>X873</th><th>X874</th><th>X875</th><th>X876</th><th>X877</th><th>X878</th><th>X879</th><th>X880</th><th>X881</th><th>X882</th><th>X883</th><th>X884</th><th>X885</th><th>X886</th><th>X887</th><th>X888</th><th>X889</th><th>X890</th><th>label</th><th>timestamp</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>datetime[ns]</td></tr></thead><tbody><tr><td>15.283</td><td>8.425</td><td>176.405</td><td>44.984</td><td>221.389</td><td>0.121263</td><td>-0.41769</td><td>0.005399</td><td>0.125948</td><td>0.058359</td><td>0.027359</td><td>0.03578</td><td>0.068219</td><td>1.034825</td><td>-0.029575</td><td>0.327805</td><td>0.485823</td><td>0.668596</td><td>0.617389</td><td>0.770037</td><td>0.857631</td><td>1.754456</td><td>0.572503</td><td>0.883229</td><td>0.58567</td><td>0.816321</td><td>0.529973</td><td>0.508244</td><td>0.448616</td><td>1.341892</td><td>1.406392</td><td>0.953631</td><td>1.183991</td><td>1.474789</td><td>0.774389</td><td>0.660586</td><td>0.269043</td><td>&hellip;</td><td>-0.216525</td><td>0.200508</td><td>0.492433</td><td>-0.51249</td><td>0.541286</td><td>-0.336399</td><td>-1.027483</td><td>0.21857</td><td>0.0</td><td>1.728155</td><td>0.62414</td><td>0.0</td><td>-0.051211</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.691754</td><td>0.242124</td><td>2.096157</td><td>3.369195</td><td>0.244667</td><td>0.286611</td><td>0.722679</td><td>0.901931</td><td>1.000007</td><td>1.925423</td><td>1.847943</td><td>0.005676</td><td>0.190791</td><td>0.369691</td><td>0.37763</td><td>0.210153</td><td>0.159183</td><td>0.530636</td><td>0.562539</td><td>2023-03-01 00:00:00</td></tr><tr><td>38.59</td><td>2.336</td><td>525.846</td><td>321.95</td><td>847.796</td><td>0.302841</td><td>-0.049576</td><td>0.356667</td><td>0.481087</td><td>0.237954</td><td>0.208359</td><td>0.217057</td><td>0.249624</td><td>0.948694</td><td>-0.183488</td><td>0.150526</td><td>0.308421</td><td>0.492232</td><td>0.529787</td><td>0.682958</td><td>0.770965</td><td>1.686504</td><td>0.273357</td><td>0.591695</td><td>0.442391</td><td>0.674792</td><td>0.460741</td><td>0.439681</td><td>0.380399</td><td>1.304113</td><td>1.003783</td><td>0.776628</td><td>1.015943</td><td>1.312735</td><td>0.696895</td><td>0.584217</td><td>0.231104</td><td>&hellip;</td><td>-0.180112</td><td>0.213252</td><td>0.479806</td><td>-0.180527</td><td>0.450331</td><td>-0.31915</td><td>-1.024055</td><td>0.088014</td><td>0.0</td><td>1.665698</td><td>0.622775</td><td>0.0</td><td>-0.079621</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.691665</td><td>0.242091</td><td>2.46103</td><td>4.127584</td><td>0.321394</td><td>0.31246</td><td>0.746452</td><td>0.912371</td><td>1.003153</td><td>1.928569</td><td>1.849468</td><td>0.005227</td><td>0.18466</td><td>0.363642</td><td>0.374515</td><td>0.209573</td><td>0.158963</td><td>0.530269</td><td>0.533686</td><td>2023-03-01 00:01:00</td></tr><tr><td>0.442</td><td>60.25</td><td>159.227</td><td>136.369</td><td>295.596</td><td>0.167462</td><td>-0.291212</td><td>0.083138</td><td>0.206881</td><td>0.101727</td><td>0.072778</td><td>0.081564</td><td>0.114166</td><td>0.896459</td><td>-0.261779</td><td>0.044571</td><td>0.200608</td><td>0.384558</td><td>0.476229</td><td>0.629848</td><td>0.718232</td><td>1.656707</td><td>0.140156</td><td>0.457268</td><td>0.376524</td><td>0.610116</td><td>0.429751</td><td>0.409316</td><td>0.350359</td><td>1.28325</td><td>0.760801</td><td>0.670816</td><td>0.917205</td><td>1.219124</td><td>0.653355</td><td>0.541739</td><td>0.210095</td><td>&hellip;</td><td>-0.265966</td><td>0.191734</td><td>0.440207</td><td>-0.108209</td><td>0.420681</td><td>-0.316953</td><td>-1.024056</td><td>-0.147363</td><td>0.0</td><td>1.666893</td><td>0.621414</td><td>0.0</td><td>-0.080427</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.691674</td><td>0.242093</td><td>2.493249</td><td>4.182112</td><td>0.326701</td><td>0.314636</td><td>0.746681</td><td>0.911129</td><td>1.002502</td><td>1.928047</td><td>1.849282</td><td>0.004796</td><td>0.178719</td><td>0.357689</td><td>0.371424</td><td>0.208993</td><td>0.158744</td><td>0.529901</td><td>0.546505</td><td>2023-03-01 00:02:00</td></tr><tr><td>4.865</td><td>21.016</td><td>335.742</td><td>124.963</td><td>460.705</td><td>0.072944</td><td>-0.43659</td><td>-0.102483</td><td>0.017551</td><td>0.007149</td><td>-0.021681</td><td>-0.012936</td><td>0.019634</td><td>0.732634</td><td>-0.535845</td><td>-0.273947</td><td>-0.124959</td><td>0.056438</td><td>0.311539</td><td>0.465377</td><td>0.554022</td><td>1.663491</td><td>0.152084</td><td>0.468778</td><td>0.383696</td><td>0.618529</td><td>0.435326</td><td>0.415523</td><td>0.356895</td><td>1.319538</td><td>0.955549</td><td>0.789646</td><td>1.044941</td><td>1.353001</td><td>0.72392</td><td>0.613462</td><td>0.246212</td><td>&hellip;</td><td>-0.322244</td><td>0.183687</td><td>0.404295</td><td>-0.169373</td><td>0.386584</td><td>-0.314775</td><td>-1.024058</td><td>-0.09459</td><td>0.0</td><td>1.735322</td><td>0.620057</td><td>0.0</td><td>-0.094702</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.69121</td><td>0.24193</td><td>2.525526</td><td>4.292975</td><td>0.350791</td><td>0.32357</td><td>0.753829</td><td>0.913363</td><td>1.002985</td><td>1.928621</td><td>1.849608</td><td>0.004398</td><td>0.172967</td><td>0.351832</td><td>0.368358</td><td>0.208416</td><td>0.158524</td><td>0.529534</td><td>0.357703</td><td>2023-03-01 00:03:00</td></tr><tr><td>27.158</td><td>3.451</td><td>98.411</td><td>44.407</td><td>142.818</td><td>0.17382</td><td>-0.213489</td><td>0.096067</td><td>0.215709</td><td>0.107133</td><td>0.078976</td><td>0.087818</td><td>0.120426</td><td>0.763537</td><td>-0.430945</td><td>-0.205298</td><td>-0.062118</td><td>0.117266</td><td>0.341493</td><td>0.495591</td><td>0.584519</td><td>1.668419</td><td>0.156177</td><td>0.472732</td><td>0.3871</td><td>0.623192</td><td>0.439034</td><td>0.419868</td><td>0.361572</td><td>1.324595</td><td>0.90546</td><td>0.78375</td><td>1.047708</td><td>1.36188</td><td>0.732001</td><td>0.622712</td><td>0.251095</td><td>&hellip;</td><td>-0.369625</td><td>0.192377</td><td>0.415438</td><td>-0.198976</td><td>0.389969</td><td>-0.312628</td><td>-1.02406</td><td>0.162221</td><td>0.0</td><td>1.712096</td><td>0.618703</td><td>0.0</td><td>-0.091884</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.691207</td><td>0.241928</td><td>2.52443</td><td>4.306694</td><td>0.335599</td><td>0.31907</td><td>0.747533</td><td>0.908904</td><td>1.001286</td><td>1.927084</td><td>1.84895</td><td>0.004008</td><td>0.167391</td><td>0.346066</td><td>0.365314</td><td>0.207839</td><td>0.158304</td><td>0.529167</td><td>0.362452</td><td>2023-03-01 00:04:00</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>4.163</td><td>6.805</td><td>39.037</td><td>55.351</td><td>94.388</td><td>0.020155</td><td>0.076565</td><td>0.228994</td><td>0.288856</td><td>0.151634</td><td>0.108347</td><td>0.088073</td><td>0.073729</td><td>0.071211</td><td>0.460379</td><td>0.494391</td><td>0.372694</td><td>0.270015</td><td>0.089571</td><td>0.096287</td><td>0.109409</td><td>0.78486</td><td>2.031313</td><td>1.673446</td><td>0.641512</td><td>0.772499</td><td>0.773449</td><td>0.956571</td><td>0.899399</td><td>0.477649</td><td>0.120275</td><td>0.017187</td><td>0.166928</td><td>0.519881</td><td>0.607589</td><td>0.797609</td><td>0.31223</td><td>&hellip;</td><td>0.035728</td><td>0.551648</td><td>1.233784</td><td>-0.252775</td><td>-0.040562</td><td>-0.347201</td><td>-0.21873</td><td>0.636555</td><td>0.0</td><td>0.533902</td><td>0.142461</td><td>0.0</td><td>-0.768709</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.260958</td><td>0.197249</td><td>1.776607</td><td>2.702758</td><td>0.204378</td><td>0.270656</td><td>0.750338</td><td>1.060258</td><td>1.450851</td><td>3.219345</td><td>3.340686</td><td>0.008679</td><td>0.224656</td><td>0.401595</td><td>0.393726</td><td>0.212651</td><td>0.136494</td><td>0.243172</td><td>0.396289</td><td>2024-02-29 23:55:00</td></tr><tr><td>2.29</td><td>4.058</td><td>110.201</td><td>67.171</td><td>177.372</td><td>0.016262</td><td>0.062527</td><td>0.214072</td><td>0.276463</td><td>0.146521</td><td>0.104164</td><td>0.084063</td><td>0.069788</td><td>0.024066</td><td>0.332808</td><td>0.387194</td><td>0.27384</td><td>0.174273</td><td>0.042308</td><td>0.049073</td><td>0.06222</td><td>0.811096</td><td>1.942052</td><td>1.721022</td><td>0.682607</td><td>0.818153</td><td>0.79747</td><td>0.981444</td><td>0.924992</td><td>0.496542</td><td>0.246743</td><td>0.089766</td><td>0.238524</td><td>0.590531</td><td>0.643586</td><td>0.834236</td><td>0.330893</td><td>&hellip;</td><td>0.052891</td><td>0.572652</td><td>1.236672</td><td>-0.253142</td><td>-0.027268</td><td>-0.220352</td><td>-0.206114</td><td>0.649406</td><td>0.0</td><td>0.544298</td><td>0.149768</td><td>0.0</td><td>-0.779659</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.260872</td><td>0.197186</td><td>1.807592</td><td>2.742924</td><td>0.213951</td><td>0.271922</td><td>0.748216</td><td>1.056653</td><td>1.448602</td><td>3.216719</td><td>3.339353</td><td>0.007928</td><td>0.217422</td><td>0.395019</td><td>0.390476</td><td>0.212063</td><td>0.136305</td><td>0.243004</td><td>0.328993</td><td>2024-02-29 23:56:00</td></tr><tr><td>5.237</td><td>3.64</td><td>70.499</td><td>30.753</td><td>101.252</td><td>0.045407</td><td>0.109834</td><td>0.263577</td><td>0.329266</td><td>0.174214</td><td>0.13294</td><td>0.113052</td><td>0.098865</td><td>-0.05737</td><td>0.154488</td><td>0.217087</td><td>0.10915</td><td>0.011308</td><td>-0.039019</td><td>-0.032317</td><td>-0.019202</td><td>0.498355</td><td>0.628261</td><td>0.454894</td><td>0.056188</td><td>0.191078</td><td>0.483386</td><td>0.667775</td><td>0.611826</td><td>0.458054</td><td>-0.055595</td><td>-0.062112</td><td>0.083189</td><td>0.432974</td><td>0.565042</td><td>0.756211</td><td>0.292203</td><td>&hellip;</td><td>0.024071</td><td>0.54706</td><td>1.191918</td><td>-0.342808</td><td>-0.065439</td><td>-0.220704</td><td>-0.206118</td><td>0.535593</td><td>0.0</td><td>0.498593</td><td>0.150411</td><td>0.0</td><td>-0.805622</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.260703</td><td>0.197062</td><td>1.7447</td><td>2.73189</td><td>0.210581</td><td>0.268868</td><td>0.741793</td><td>1.050909</td><td>1.445661</td><td>3.213444</td><td>3.33774</td><td>0.007243</td><td>0.210421</td><td>0.388549</td><td>0.387252</td><td>0.211477</td><td>0.136117</td><td>0.242836</td><td>0.189909</td><td>2024-02-29 23:57:00</td></tr><tr><td>5.731</td><td>4.901</td><td>22.365</td><td>52.195</td><td>74.56</td><td>0.124783</td><td>0.244168</td><td>0.408704</td><td>0.480016</td><td>0.251493</td><td>0.211727</td><td>0.19216</td><td>0.178116</td><td>0.111335</td><td>0.44718</td><td>0.53661</td><td>0.439239</td><td>0.345835</td><td>0.129327</td><td>0.136199</td><td>0.149399</td><td>0.803323</td><td>1.680122</td><td>1.620743</td><td>0.655204</td><td>0.794395</td><td>0.78617</td><td>0.971394</td><td>0.916158</td><td>0.496689</td><td>0.230439</td><td>0.089445</td><td>0.23383</td><td>0.582657</td><td>0.640532</td><td>0.832325</td><td>0.330608</td><td>&hellip;</td><td>0.035102</td><td>0.568606</td><td>1.212147</td><td>-0.369407</td><td>-0.035077</td><td>-0.221043</td><td>-0.206122</td><td>0.647059</td><td>0.0</td><td>0.528757</td><td>0.151051</td><td>0.0</td><td>-0.774165</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.260957</td><td>0.197257</td><td>1.74259</td><td>2.643514</td><td>0.203284</td><td>0.264413</td><td>0.733953</td><td>1.044452</td><td>1.442484</td><td>3.209945</td><td>3.33603</td><td>0.006608</td><td>0.203642</td><td>0.382184</td><td>0.384054</td><td>0.210892</td><td>0.135928</td><td>0.242668</td><td>0.410831</td><td>2024-02-29 23:58:00</td></tr><tr><td>3.925</td><td>3.865</td><td>86.585</td><td>217.137</td><td>303.722</td><td>0.368659</td><td>0.665382</td><td>0.867538</td><td>0.951903</td><td>0.491276</td><td>0.454342</td><td>0.435431</td><td>0.4217</td><td>0.330298</td><td>0.804642</td><td>0.9431</td><td>0.862786</td><td>0.777285</td><td>0.347325</td><td>0.354669</td><td>0.368107</td><td>1.128738</td><td>2.710711</td><td>2.828132</td><td>1.284624</td><td>1.433281</td><td>1.108506</td><td>1.295011</td><td>1.240712</td><td>0.602761</td><td>0.980926</td><td>0.497163</td><td>0.647331</td><td>0.998625</td><td>0.850315</td><td>1.043021</td><td>0.436378</td><td>&hellip;</td><td>-0.052765</td><td>0.597701</td><td>1.450874</td><td>-0.436284</td><td>0.202887</td><td>-0.221368</td><td>-0.206125</td><td>0.331538</td><td>0.0</td><td>0.669032</td><td>0.151687</td><td>0.0</td><td>-0.66805</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.261324</td><td>0.197537</td><td>1.790457</td><td>2.627113</td><td>0.232998</td><td>0.272984</td><td>0.739299</td><td>1.044576</td><td>1.441416</td><td>3.208415</td><td>3.335166</td><td>0.006072</td><td>0.197096</td><td>0.375931</td><td>0.380886</td><td>0.21031</td><td>0.135741</td><td>0.242501</td><td>0.731542</td><td>2024-02-29 23:59:00</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"## 1.1 Pre-processing / Feature Engineering\n\n**Pre-Processing**\n1. inf/-inf columns: `['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']`\n2. columns with NaN values: `[]`\n3. 0 std columns : `['X864', 'X867', 'X869', 'X870', 'X871', 'X872']`\n\n\n**Feature Engineering**\n1. `bidask_ratio`\n2. `buysell_ratio`\n3. `bidask_delta`\n4. `buysell_delta`\n5. `buysell_size`\n6. `bidask_size`","metadata":{}},{"cell_type":"code","source":"def get_cols_inf(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names that contain any positive or negative infinity.\n    \"\"\"\n    cols = []\n    for col in df.columns:\n        # df[col] is a Series; .is_infinite() → Boolean Series; .any() → Python bool\n        try:\n            if df[col].is_infinite().any():\n                cols.append(col)\n        except Exception:\n            # if the column isn’t numeric, .is_infinite() might error—just skip it\n            continue\n    return cols\n\ndef get_nan_columns(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names with any NaN/null values.\n    \"\"\"\n    cols = []\n    for col in df.columns:\n        if df.select(pl.col(col).is_null().any()).item():\n            cols.append(col)\n    return cols\n\ndef get_cols_zerostd(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names whose standard deviation is zero\n    (or whose std returns None because all values are null).\n    Non-numeric columns (e.g. datetime) are skipped.\n    \"\"\"\n    cols = []\n    for col, dtype in zip(df.columns, df.dtypes):\n        # Only attempt std() on numeric dtypes\n        if dtype.is_numeric():  \n            # df[col] is a Series; .std() returns a Python float or None\n            std_val = df[col].std()\n            if std_val == 0.0 or std_val is None:\n                cols.append(col)\n    return cols\n\n\ndef feature_engineering(df: pl.DataFrame) -> pl.DataFrame:\n    # Feature engineering\n    df = df.with_columns([\n        # bidask_ratio = bid_qty / ask_qty\n        (pl.col(\"bid_qty\") / pl.col(\"ask_qty\")).alias(\"bidask_ratio\"),\n\n        # buysell_ratio = 0 if volume == 0 else buy_qty / sell_qty\n        pl.when(pl.col(\"volume\") == 0)\n        .then(0)\n        .otherwise(pl.col(\"buy_qty\") / pl.col(\"sell_qty\"))\n        .alias(\"buysell_ratio\"),\n\n        # bidask_delta = bid_qty - ask_qty\n        (pl.col(\"bid_qty\") - pl.col(\"ask_qty\")).alias(\"bidask_delta\"),\n\n        # buysell_delta = buy_qty - sell_qty\n        (pl.col(\"buy_qty\") - pl.col(\"sell_qty\")).alias(\"buysell_delta\"),\n\n        # buysell_size = buy_qty + sell_qty\n        (pl.col(\"buy_qty\") + pl.col(\"sell_qty\")).alias(\"buysell_size\"),\n\n        # bidask_size = bid_qty + ask_qty\n        (pl.col(\"bid_qty\") + pl.col(\"ask_qty\")).alias(\"bidask_size\"),\n    ])\n    return df\ndef preprocess_train(train: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n    \"\"\"\n    Mirror of the original pandas workflow, but using polars.\n    1. Identify columns with infinite, NaN, or zero‐std and drop them.\n    2. Drop any user‐specified columns (e.g. label or order‐book columns).\n    3. (You can add normalized/scaling steps here if needed.)\n    \"\"\"\n    df = train.clone()\n\n    df = feature_engineering(df)\n    \n    #### Preprocessing\n    cols_inf = get_cols_inf(df)\n    print(\"Columns with infinite values:\", cols_inf)\n\n    cols_nan = get_nan_columns(df)\n    print(\"Columns with NaN values:\", cols_nan)\n\n    cols_zerostd = get_cols_zerostd(df)\n    print(\"Columns with zero standard deviation:\", cols_zerostd)\n    # Drop columns with infinite, NaN, or zero‐std values\n    drop_columns = list(set(cols_inf) | set(cols_nan) | set(cols_zerostd) | set(columns_to_drop))\n    if drop_columns:\n        df = df.drop(drop_columns)\n    # df = df.sort(\"timestamp\", descending=False)\n    return df, drop_columns\n\ndef preprocess_test(test: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n    df = test.clone()\n    df = feature_engineering(df)\n    df = df.drop(columns_to_drop)\n    print(\"Columns dropped from test set:\", columns_to_drop)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:51.269812Z","iopub.execute_input":"2025-06-10T09:20:51.270012Z","iopub.status.idle":"2025-06-10T09:20:51.281705Z","shell.execute_reply.started":"2025-06-10T09:20:51.269996Z","shell.execute_reply":"2025-06-10T09:20:51.281126Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"y = data[\"label\"]\nX, drop_columns = preprocess_train(\n    data,\n    columns_to_drop=[\"label\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n)\nX","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:51.283080Z","iopub.execute_input":"2025-06-10T09:20:51.283298Z","iopub.status.idle":"2025-06-10T09:20:53.195669Z","shell.execute_reply.started":"2025-06-10T09:20:51.283279Z","shell.execute_reply":"2025-06-10T09:20:53.195103Z"}},"outputs":[{"name":"stdout","text":"Columns with infinite values: ['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']\nColumns with NaN values: []\nColumns with zero standard deviation: ['X864', 'X867', 'X869', 'X870', 'X871', 'X872']\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"shape: (525_887, 871)\n┌─────────┬──────────┬───────────┬───────────┬───┬────────────┬────────────┬───────────┬───────────┐\n│ volume  ┆ X1       ┆ X2        ┆ X3        ┆ … ┆ bidask_del ┆ buysell_de ┆ buysell_s ┆ bidask_si │\n│ ---     ┆ ---      ┆ ---       ┆ ---       ┆   ┆ ta         ┆ lta        ┆ ize       ┆ ze        │\n│ f64     ┆ f64      ┆ f64       ┆ f64       ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n│         ┆          ┆           ┆           ┆   ┆ f64        ┆ f64        ┆ f64       ┆ f64       │\n╞═════════╪══════════╪═══════════╪═══════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n│ 221.389 ┆ 0.121263 ┆ -0.41769  ┆ 0.005399  ┆ … ┆ 6.858      ┆ 131.421    ┆ 221.389   ┆ 23.708    │\n│ 847.796 ┆ 0.302841 ┆ -0.049576 ┆ 0.356667  ┆ … ┆ 36.254     ┆ 203.896    ┆ 847.796   ┆ 40.926    │\n│ 295.596 ┆ 0.167462 ┆ -0.291212 ┆ 0.083138  ┆ … ┆ -59.808    ┆ 22.858     ┆ 295.596   ┆ 60.692    │\n│ 460.705 ┆ 0.072944 ┆ -0.43659  ┆ -0.102483 ┆ … ┆ -16.151    ┆ 210.779    ┆ 460.705   ┆ 25.881    │\n│ 142.818 ┆ 0.17382  ┆ -0.213489 ┆ 0.096067  ┆ … ┆ 23.707     ┆ 54.004     ┆ 142.818   ┆ 30.609    │\n│ …       ┆ …        ┆ …         ┆ …         ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n│ 94.388  ┆ 0.020155 ┆ 0.076565  ┆ 0.228994  ┆ … ┆ -2.642     ┆ -16.314    ┆ 94.388    ┆ 10.968    │\n│ 177.372 ┆ 0.016262 ┆ 0.062527  ┆ 0.214072  ┆ … ┆ -1.768     ┆ 43.03      ┆ 177.372   ┆ 6.348     │\n│ 101.252 ┆ 0.045407 ┆ 0.109834  ┆ 0.263577  ┆ … ┆ 1.597      ┆ 39.746     ┆ 101.252   ┆ 8.877     │\n│ 74.56   ┆ 0.124783 ┆ 0.244168  ┆ 0.408704  ┆ … ┆ 0.83       ┆ -29.83     ┆ 74.56     ┆ 10.632    │\n│ 303.722 ┆ 0.368659 ┆ 0.665382  ┆ 0.867538  ┆ … ┆ 0.06       ┆ -130.552   ┆ 303.722   ┆ 7.79      │\n└─────────┴──────────┴───────────┴───────────┴───┴────────────┴────────────┴───────────┴───────────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (525_887, 871)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>volume</th><th>X1</th><th>X2</th><th>X3</th><th>X4</th><th>X5</th><th>X6</th><th>X7</th><th>X8</th><th>X9</th><th>X10</th><th>X11</th><th>X12</th><th>X13</th><th>X14</th><th>X15</th><th>X16</th><th>X17</th><th>X18</th><th>X19</th><th>X20</th><th>X21</th><th>X22</th><th>X23</th><th>X24</th><th>X25</th><th>X26</th><th>X27</th><th>X28</th><th>X29</th><th>X30</th><th>X31</th><th>X32</th><th>X33</th><th>X34</th><th>X35</th><th>X36</th><th>&hellip;</th><th>X855</th><th>X856</th><th>X857</th><th>X858</th><th>X859</th><th>X860</th><th>X861</th><th>X862</th><th>X863</th><th>X865</th><th>X866</th><th>X868</th><th>X873</th><th>X874</th><th>X875</th><th>X876</th><th>X877</th><th>X878</th><th>X879</th><th>X880</th><th>X881</th><th>X882</th><th>X883</th><th>X884</th><th>X885</th><th>X886</th><th>X887</th><th>X888</th><th>X889</th><th>X890</th><th>timestamp</th><th>bidask_ratio</th><th>buysell_ratio</th><th>bidask_delta</th><th>buysell_delta</th><th>buysell_size</th><th>bidask_size</th></tr><tr><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>&hellip;</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>datetime[ns]</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>221.389</td><td>0.121263</td><td>-0.41769</td><td>0.005399</td><td>0.125948</td><td>0.058359</td><td>0.027359</td><td>0.03578</td><td>0.068219</td><td>1.034825</td><td>-0.029575</td><td>0.327805</td><td>0.485823</td><td>0.668596</td><td>0.617389</td><td>0.770037</td><td>0.857631</td><td>1.754456</td><td>0.572503</td><td>0.883229</td><td>0.58567</td><td>0.816321</td><td>0.529973</td><td>0.508244</td><td>0.448616</td><td>1.341892</td><td>1.406392</td><td>0.953631</td><td>1.183991</td><td>1.474789</td><td>0.774389</td><td>0.660586</td><td>0.269043</td><td>0.962802</td><td>0.966513</td><td>0.952759</td><td>0.952916</td><td>&hellip;</td><td>0.418618</td><td>-0.216525</td><td>0.200508</td><td>0.492433</td><td>-0.51249</td><td>0.541286</td><td>-0.336399</td><td>-1.027483</td><td>0.21857</td><td>1.728155</td><td>0.62414</td><td>-0.051211</td><td>0.691754</td><td>0.242124</td><td>2.096157</td><td>3.369195</td><td>0.244667</td><td>0.286611</td><td>0.722679</td><td>0.901931</td><td>1.000007</td><td>1.925423</td><td>1.847943</td><td>0.005676</td><td>0.190791</td><td>0.369691</td><td>0.37763</td><td>0.210153</td><td>0.159183</td><td>0.530636</td><td>2023-03-01 00:00:00</td><td>1.814006</td><td>3.921505</td><td>6.858</td><td>131.421</td><td>221.389</td><td>23.708</td></tr><tr><td>847.796</td><td>0.302841</td><td>-0.049576</td><td>0.356667</td><td>0.481087</td><td>0.237954</td><td>0.208359</td><td>0.217057</td><td>0.249624</td><td>0.948694</td><td>-0.183488</td><td>0.150526</td><td>0.308421</td><td>0.492232</td><td>0.529787</td><td>0.682958</td><td>0.770965</td><td>1.686504</td><td>0.273357</td><td>0.591695</td><td>0.442391</td><td>0.674792</td><td>0.460741</td><td>0.439681</td><td>0.380399</td><td>1.304113</td><td>1.003783</td><td>0.776628</td><td>1.015943</td><td>1.312735</td><td>0.696895</td><td>0.584217</td><td>0.231104</td><td>0.935145</td><td>0.938957</td><td>0.918275</td><td>0.919065</td><td>&hellip;</td><td>0.424977</td><td>-0.180112</td><td>0.213252</td><td>0.479806</td><td>-0.180527</td><td>0.450331</td><td>-0.31915</td><td>-1.024055</td><td>0.088014</td><td>1.665698</td><td>0.622775</td><td>-0.079621</td><td>0.691665</td><td>0.242091</td><td>2.46103</td><td>4.127584</td><td>0.321394</td><td>0.31246</td><td>0.746452</td><td>0.912371</td><td>1.003153</td><td>1.928569</td><td>1.849468</td><td>0.005227</td><td>0.18466</td><td>0.363642</td><td>0.374515</td><td>0.209573</td><td>0.158963</td><td>0.530269</td><td>2023-03-01 00:01:00</td><td>16.519692</td><td>1.633316</td><td>36.254</td><td>203.896</td><td>847.796</td><td>40.926</td></tr><tr><td>295.596</td><td>0.167462</td><td>-0.291212</td><td>0.083138</td><td>0.206881</td><td>0.101727</td><td>0.072778</td><td>0.081564</td><td>0.114166</td><td>0.896459</td><td>-0.261779</td><td>0.044571</td><td>0.200608</td><td>0.384558</td><td>0.476229</td><td>0.629848</td><td>0.718232</td><td>1.656707</td><td>0.140156</td><td>0.457268</td><td>0.376524</td><td>0.610116</td><td>0.429751</td><td>0.409316</td><td>0.350359</td><td>1.28325</td><td>0.760801</td><td>0.670816</td><td>0.917205</td><td>1.219124</td><td>0.653355</td><td>0.541739</td><td>0.210095</td><td>0.932614</td><td>0.936476</td><td>0.919497</td><td>0.92028</td><td>&hellip;</td><td>0.409942</td><td>-0.265966</td><td>0.191734</td><td>0.440207</td><td>-0.108209</td><td>0.420681</td><td>-0.316953</td><td>-1.024056</td><td>-0.147363</td><td>1.666893</td><td>0.621414</td><td>-0.080427</td><td>0.691674</td><td>0.242093</td><td>2.493249</td><td>4.182112</td><td>0.326701</td><td>0.314636</td><td>0.746681</td><td>0.911129</td><td>1.002502</td><td>1.928047</td><td>1.849282</td><td>0.004796</td><td>0.178719</td><td>0.357689</td><td>0.371424</td><td>0.208993</td><td>0.158744</td><td>0.529901</td><td>2023-03-01 00:02:00</td><td>0.007336</td><td>1.167619</td><td>-59.808</td><td>22.858</td><td>295.596</td><td>60.692</td></tr><tr><td>460.705</td><td>0.072944</td><td>-0.43659</td><td>-0.102483</td><td>0.017551</td><td>0.007149</td><td>-0.021681</td><td>-0.012936</td><td>0.019634</td><td>0.732634</td><td>-0.535845</td><td>-0.273947</td><td>-0.124959</td><td>0.056438</td><td>0.311539</td><td>0.465377</td><td>0.554022</td><td>1.663491</td><td>0.152084</td><td>0.468778</td><td>0.383696</td><td>0.618529</td><td>0.435326</td><td>0.415523</td><td>0.356895</td><td>1.319538</td><td>0.955549</td><td>0.789646</td><td>1.044941</td><td>1.353001</td><td>0.72392</td><td>0.613462</td><td>0.246212</td><td>0.936911</td><td>0.942204</td><td>0.940304</td><td>0.942497</td><td>&hellip;</td><td>0.400075</td><td>-0.322244</td><td>0.183687</td><td>0.404295</td><td>-0.169373</td><td>0.386584</td><td>-0.314775</td><td>-1.024058</td><td>-0.09459</td><td>1.735322</td><td>0.620057</td><td>-0.094702</td><td>0.69121</td><td>0.24193</td><td>2.525526</td><td>4.292975</td><td>0.350791</td><td>0.32357</td><td>0.753829</td><td>0.913363</td><td>1.002985</td><td>1.928621</td><td>1.849608</td><td>0.004398</td><td>0.172967</td><td>0.351832</td><td>0.368358</td><td>0.208416</td><td>0.158524</td><td>0.529534</td><td>2023-03-01 00:03:00</td><td>0.23149</td><td>2.686731</td><td>-16.151</td><td>210.779</td><td>460.705</td><td>25.881</td></tr><tr><td>142.818</td><td>0.17382</td><td>-0.213489</td><td>0.096067</td><td>0.215709</td><td>0.107133</td><td>0.078976</td><td>0.087818</td><td>0.120426</td><td>0.763537</td><td>-0.430945</td><td>-0.205298</td><td>-0.062118</td><td>0.117266</td><td>0.341493</td><td>0.495591</td><td>0.584519</td><td>1.668419</td><td>0.156177</td><td>0.472732</td><td>0.3871</td><td>0.623192</td><td>0.439034</td><td>0.419868</td><td>0.361572</td><td>1.324595</td><td>0.90546</td><td>0.78375</td><td>1.047708</td><td>1.36188</td><td>0.732001</td><td>0.622712</td><td>0.251095</td><td>0.931761</td><td>0.936818</td><td>0.928362</td><td>0.930464</td><td>&hellip;</td><td>0.391759</td><td>-0.369625</td><td>0.192377</td><td>0.415438</td><td>-0.198976</td><td>0.389969</td><td>-0.312628</td><td>-1.02406</td><td>0.162221</td><td>1.712096</td><td>0.618703</td><td>-0.091884</td><td>0.691207</td><td>0.241928</td><td>2.52443</td><td>4.306694</td><td>0.335599</td><td>0.31907</td><td>0.747533</td><td>0.908904</td><td>1.001286</td><td>1.927084</td><td>1.84895</td><td>0.004008</td><td>0.167391</td><td>0.346066</td><td>0.365314</td><td>0.207839</td><td>0.158304</td><td>0.529167</td><td>2023-03-01 00:04:00</td><td>7.869603</td><td>2.216115</td><td>23.707</td><td>54.004</td><td>142.818</td><td>30.609</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>94.388</td><td>0.020155</td><td>0.076565</td><td>0.228994</td><td>0.288856</td><td>0.151634</td><td>0.108347</td><td>0.088073</td><td>0.073729</td><td>0.071211</td><td>0.460379</td><td>0.494391</td><td>0.372694</td><td>0.270015</td><td>0.089571</td><td>0.096287</td><td>0.109409</td><td>0.78486</td><td>2.031313</td><td>1.673446</td><td>0.641512</td><td>0.772499</td><td>0.773449</td><td>0.956571</td><td>0.899399</td><td>0.477649</td><td>0.120275</td><td>0.017187</td><td>0.166928</td><td>0.519881</td><td>0.607589</td><td>0.797609</td><td>0.31223</td><td>0.459993</td><td>0.47937</td><td>0.48488</td><td>0.454508</td><td>&hellip;</td><td>0.807352</td><td>0.035728</td><td>0.551648</td><td>1.233784</td><td>-0.252775</td><td>-0.040562</td><td>-0.347201</td><td>-0.21873</td><td>0.636555</td><td>0.533902</td><td>0.142461</td><td>-0.768709</td><td>0.260958</td><td>0.197249</td><td>1.776607</td><td>2.702758</td><td>0.204378</td><td>0.270656</td><td>0.750338</td><td>1.060258</td><td>1.450851</td><td>3.219345</td><td>3.340686</td><td>0.008679</td><td>0.224656</td><td>0.401595</td><td>0.393726</td><td>0.212651</td><td>0.136494</td><td>0.243172</td><td>2024-02-29 23:55:00</td><td>0.611756</td><td>0.705263</td><td>-2.642</td><td>-16.314</td><td>94.388</td><td>10.968</td></tr><tr><td>177.372</td><td>0.016262</td><td>0.062527</td><td>0.214072</td><td>0.276463</td><td>0.146521</td><td>0.104164</td><td>0.084063</td><td>0.069788</td><td>0.024066</td><td>0.332808</td><td>0.387194</td><td>0.27384</td><td>0.174273</td><td>0.042308</td><td>0.049073</td><td>0.06222</td><td>0.811096</td><td>1.942052</td><td>1.721022</td><td>0.682607</td><td>0.818153</td><td>0.79747</td><td>0.981444</td><td>0.924992</td><td>0.496542</td><td>0.246743</td><td>0.089766</td><td>0.238524</td><td>0.590531</td><td>0.643586</td><td>0.834236</td><td>0.330893</td><td>0.446043</td><td>0.478758</td><td>0.474246</td><td>0.461256</td><td>&hellip;</td><td>0.813372</td><td>0.052891</td><td>0.572652</td><td>1.236672</td><td>-0.253142</td><td>-0.027268</td><td>-0.220352</td><td>-0.206114</td><td>0.649406</td><td>0.544298</td><td>0.149768</td><td>-0.779659</td><td>0.260872</td><td>0.197186</td><td>1.807592</td><td>2.742924</td><td>0.213951</td><td>0.271922</td><td>0.748216</td><td>1.056653</td><td>1.448602</td><td>3.216719</td><td>3.339353</td><td>0.007928</td><td>0.217422</td><td>0.395019</td><td>0.390476</td><td>0.212063</td><td>0.136305</td><td>0.243004</td><td>2024-02-29 23:56:00</td><td>0.564317</td><td>1.640604</td><td>-1.768</td><td>43.03</td><td>177.372</td><td>6.348</td></tr><tr><td>101.252</td><td>0.045407</td><td>0.109834</td><td>0.263577</td><td>0.329266</td><td>0.174214</td><td>0.13294</td><td>0.113052</td><td>0.098865</td><td>-0.05737</td><td>0.154488</td><td>0.217087</td><td>0.10915</td><td>0.011308</td><td>-0.039019</td><td>-0.032317</td><td>-0.019202</td><td>0.498355</td><td>0.628261</td><td>0.454894</td><td>0.056188</td><td>0.191078</td><td>0.483386</td><td>0.667775</td><td>0.611826</td><td>0.458054</td><td>-0.055595</td><td>-0.062112</td><td>0.083189</td><td>0.432974</td><td>0.565042</td><td>0.756211</td><td>0.292203</td><td>0.365089</td><td>0.420895</td><td>0.399799</td><td>0.422313</td><td>&hellip;</td><td>0.803276</td><td>0.024071</td><td>0.54706</td><td>1.191918</td><td>-0.342808</td><td>-0.065439</td><td>-0.220704</td><td>-0.206118</td><td>0.535593</td><td>0.498593</td><td>0.150411</td><td>-0.805622</td><td>0.260703</td><td>0.197062</td><td>1.7447</td><td>2.73189</td><td>0.210581</td><td>0.268868</td><td>0.741793</td><td>1.050909</td><td>1.445661</td><td>3.213444</td><td>3.33774</td><td>0.007243</td><td>0.210421</td><td>0.388549</td><td>0.387252</td><td>0.211477</td><td>0.136117</td><td>0.242836</td><td>2024-02-29 23:57:00</td><td>1.438736</td><td>2.292427</td><td>1.597</td><td>39.746</td><td>101.252</td><td>8.877</td></tr><tr><td>74.56</td><td>0.124783</td><td>0.244168</td><td>0.408704</td><td>0.480016</td><td>0.251493</td><td>0.211727</td><td>0.19216</td><td>0.178116</td><td>0.111335</td><td>0.44718</td><td>0.53661</td><td>0.439239</td><td>0.345835</td><td>0.129327</td><td>0.136199</td><td>0.149399</td><td>0.803323</td><td>1.680122</td><td>1.620743</td><td>0.655204</td><td>0.794395</td><td>0.78617</td><td>0.971394</td><td>0.916158</td><td>0.496689</td><td>0.230439</td><td>0.089445</td><td>0.23383</td><td>0.582657</td><td>0.640532</td><td>0.832325</td><td>0.330608</td><td>0.441305</td><td>0.467176</td><td>0.460681</td><td>0.446152</td><td>&hellip;</td><td>0.807143</td><td>0.035102</td><td>0.568606</td><td>1.212147</td><td>-0.369407</td><td>-0.035077</td><td>-0.221043</td><td>-0.206122</td><td>0.647059</td><td>0.528757</td><td>0.151051</td><td>-0.774165</td><td>0.260957</td><td>0.197257</td><td>1.74259</td><td>2.643514</td><td>0.203284</td><td>0.264413</td><td>0.733953</td><td>1.044452</td><td>1.442484</td><td>3.209945</td><td>3.33603</td><td>0.006608</td><td>0.203642</td><td>0.382184</td><td>0.384054</td><td>0.210892</td><td>0.135928</td><td>0.242668</td><td>2024-02-29 23:58:00</td><td>1.169353</td><td>0.428489</td><td>0.83</td><td>-29.83</td><td>74.56</td><td>10.632</td></tr><tr><td>303.722</td><td>0.368659</td><td>0.665382</td><td>0.867538</td><td>0.951903</td><td>0.491276</td><td>0.454342</td><td>0.435431</td><td>0.4217</td><td>0.330298</td><td>0.804642</td><td>0.9431</td><td>0.862786</td><td>0.777285</td><td>0.347325</td><td>0.354669</td><td>0.368107</td><td>1.128738</td><td>2.710711</td><td>2.828132</td><td>1.284624</td><td>1.433281</td><td>1.108506</td><td>1.295011</td><td>1.240712</td><td>0.602761</td><td>0.980926</td><td>0.497163</td><td>0.647331</td><td>0.998625</td><td>0.850315</td><td>1.043021</td><td>0.436378</td><td>0.58741</td><td>0.606344</td><td>0.59148</td><td>0.555941</td><td>&hellip;</td><td>0.776366</td><td>-0.052765</td><td>0.597701</td><td>1.450874</td><td>-0.436284</td><td>0.202887</td><td>-0.221368</td><td>-0.206125</td><td>0.331538</td><td>0.669032</td><td>0.151687</td><td>-0.66805</td><td>0.261324</td><td>0.197537</td><td>1.790457</td><td>2.627113</td><td>0.232998</td><td>0.272984</td><td>0.739299</td><td>1.044576</td><td>1.441416</td><td>3.208415</td><td>3.335166</td><td>0.006072</td><td>0.197096</td><td>0.375931</td><td>0.380886</td><td>0.21031</td><td>0.135741</td><td>0.242501</td><td>2024-02-29 23:59:00</td><td>1.015524</td><td>0.398757</td><td>0.06</td><td>-130.552</td><td>303.722</td><td>7.79</td></tr></tbody></table></div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# 1.2 Time Series Split Functions\n\n1. `split_rollingwindow` - rolling window.\n3. `split_overlapwindow` - overlapping.","metadata":{"papermill":{"duration":0.003008,"end_time":"2025-06-03T09:47:24.436311","exception":false,"start_time":"2025-06-03T09:47:24.433303","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def split_rollingwindow(X, n_splits=5, train_ratio=0.8):\n    \"\"\"\n    Rolling window time series splitter with fixed train/test ratio and number of splits.\n    \n    Parameters\n    ----------\n    X : array-like or DataFrame\n        Dataset with time-ordering preserved.\n    n_splits : int\n        Number of rolling splits.\n    train_ratio : float\n        Proportion of each window used for training (e.g. 0.8 for 80/20 split).\n    \n    Yields\n    ------\n    train_idx, test_idx : range, range\n        Index ranges for training and testing.\n    \"\"\"\n    n_obs = len(X)\n    window_size = n_obs // (n_splits + 1)\n    train_size = int(train_ratio * window_size)\n    test_size = window_size - train_size\n    \n    for i in range(n_splits):\n        start = i * window_size\n        train_start = start\n        train_end = train_start + train_size\n        test_start = train_end\n        test_end = test_start + test_size\n\n        if test_end > n_obs:\n            break\n\n        yield range(train_start, train_end), range(test_start, test_end)\n\n# splits = rolling_window_split(X=X, n_splits = 5, train_ratio=0.5)\n# for train_idx, test_idx in splits:\n#     print(train_idx, test_idx)\n# #     df_train = X.slice(train_idx.start, len(train_idx))\n# #     df_test = X.slice(test_idx.start, len(test_idx))\n\n# for i, (train_idx, test_idx) in enumerate(splits):\n#     if i == 1:  # second batch (index 1)\n#         df_train = X.slice(train_idx.start, len(train_idx))\n#         df_test = X.slice(test_idx.start, len(test_idx))\n#         break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:53.196243Z","iopub.execute_input":"2025-06-10T09:20:53.196423Z","iopub.status.idle":"2025-06-10T09:20:53.201434Z","shell.execute_reply.started":"2025-06-10T09:20:53.196409Z","shell.execute_reply":"2025-06-10T09:20:53.200889Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def split_overlapwindow(X, train_size, test_size, step):\n    \"\"\"\n    Rolling window splitter with overlapping train/test splits.\n\n    Parameters\n    ----------\n    X : array-like or DataFrame\n        Dataset with time-ordering preserved.\n    train_size : int\n        Number of observations in each training window.\n    test_size : int\n        Number of observations in each test window.\n    step : int\n        Forward step size between each split.\n\n    Yields\n    ------\n    train_idx, test_idx : range, range\n        Index ranges for training and testing.\n    \"\"\"\n    n_obs = len(X)\n    start = 0\n    while (start + train_size + test_size) <= n_obs:\n        train_start = start\n        train_end = train_start + train_size\n        test_start = train_end\n        test_end = test_start + test_size\n\n        yield range(train_start, train_end), range(test_start, test_end)\n        start += step\n\ntrain_size = 100000\ntest_size = 100000\nstep = 10000\nsplits = split_overlapwindow(X, train_size, test_size, step)\nfor train_idx, test_idx in splits:\n    print(train_idx, test_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:53.202158Z","iopub.execute_input":"2025-06-10T09:20:53.202401Z","iopub.status.idle":"2025-06-10T09:20:53.224315Z","shell.execute_reply.started":"2025-06-10T09:20:53.202379Z","shell.execute_reply":"2025-06-10T09:20:53.223662Z"}},"outputs":[{"name":"stdout","text":"range(0, 100000) range(100000, 200000)\nrange(10000, 110000) range(110000, 210000)\nrange(20000, 120000) range(120000, 220000)\nrange(30000, 130000) range(130000, 230000)\nrange(40000, 140000) range(140000, 240000)\nrange(50000, 150000) range(150000, 250000)\nrange(60000, 160000) range(160000, 260000)\nrange(70000, 170000) range(170000, 270000)\nrange(80000, 180000) range(180000, 280000)\nrange(90000, 190000) range(190000, 290000)\nrange(100000, 200000) range(200000, 300000)\nrange(110000, 210000) range(210000, 310000)\nrange(120000, 220000) range(220000, 320000)\nrange(130000, 230000) range(230000, 330000)\nrange(140000, 240000) range(240000, 340000)\nrange(150000, 250000) range(250000, 350000)\nrange(160000, 260000) range(260000, 360000)\nrange(170000, 270000) range(270000, 370000)\nrange(180000, 280000) range(280000, 380000)\nrange(190000, 290000) range(290000, 390000)\nrange(200000, 300000) range(300000, 400000)\nrange(210000, 310000) range(310000, 410000)\nrange(220000, 320000) range(320000, 420000)\nrange(230000, 330000) range(330000, 430000)\nrange(240000, 340000) range(340000, 440000)\nrange(250000, 350000) range(350000, 450000)\nrange(260000, 360000) range(360000, 460000)\nrange(270000, 370000) range(370000, 470000)\nrange(280000, 380000) range(380000, 480000)\nrange(290000, 390000) range(390000, 490000)\nrange(300000, 400000) range(400000, 500000)\nrange(310000, 410000) range(410000, 510000)\nrange(320000, 420000) range(420000, 520000)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# 2 Iterative Feature Selection","metadata":{}},{"cell_type":"markdown","source":"## 2.1 Base Model","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBRegressor\n\n# model = XGBRegressor(\n#     tree_method=\"gpu_hist\",\n#     predictor=\"gpu_predictor\",\n#     n_estimators=100,\n#     max_depth=8,\n#     learning_rate=0.05,\n#     random_state=42,\n#     n_jobs=-1\n# )\n\nmodel = XGBRegressor(\n    tree_method = \"hist\", \n    device = \"cuda\",\n    n_estimators=100,\n    max_depth=8,\n    learning_rate=0.05,\n    random_state=42,\n    n_jobs=-1\n)\n\n\n# Convert your data to GPU arrays\n# X_gpu = cp.asarray(X.to_numpy())\n# y_gpu = cp.asarray(y.to_numpy())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:53.225064Z","iopub.execute_input":"2025-06-10T09:20:53.225247Z","iopub.status.idle":"2025-06-10T09:20:54.016472Z","shell.execute_reply.started":"2025-06-10T09:20:53.225232Z","shell.execute_reply":"2025-06-10T09:20:54.015943Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## 2.2 Iteration","metadata":{}},{"cell_type":"code","source":"from sklearn.base import clone\nfrom sklearn.metrics import mean_squared_error\nimport uuid\nimport cupy as cp  # You'll need cupy installed\n\n# def iterative_featureselection_onesplit(model, X, y, split_fn, scoring, drop_fraction = 0.1, min_features = 10):\n#     results = []\n#     splits = list(split_fn(X))\n#     n_splits = len(splits)\n\n#     current_X = X.copy()\n#     iteration = 0\n    \n#     while current_X.shape[1] > min_features:\n#         iteration += 1\n#         print(f\"\\n[+] Iteration {iteration} - {current_X.shape[1]} features\")\n        \n#         training_model = clone(model)\n#         training_model.fit(current_X, y)\n#         importances = pd.Series(fitted_model.feature_importances_, index=current_X.columns)\n\n#         scores = []\n#         for i, (train_idx, test_idx) in enumerate(splits, start=1):\n#             print(f\"    [Fold {i}/{n_splits}] ...\", end=\"\\r\")\n            \n            \n#             X_train, y_train = current_X.iloc[train_idx], y.iloc[train_idx]\n#             X_test, y_test = current_X.iloc[test_idx], y.iloc[test_idx]\n        \n#             fold_model = clone(model)\n#             fold_model.fit(X_train, y_train)\n        \n#             preds = fold_model.predict(X_test)\n#             score = scoring(y_test, preds)\n#             scores.append(score)\n        \n#         mean_score = np.mean(score)\n\n#         results.append({\n#             \"iteration\" : iteration,\n#             \"model_name\": model.__class__.__name__\n#             \"num_features\": current_X.shape[1],\n#             \"features\": current_X.columns.tolist(),\n#             \"score_mean\": mean_score,\n#         })\n#         n_drop = max(10, int(len(importances) * drop_fraction))\n#         drop_cols = importances.nsmallest(n_drop).index\n#         current_X = current_X.drop(columns=drop_cols)\n\n#     print(f\"\\n[*] Finished: {current_X.shape[1]} features remaining (≤ min_features={min_features})\")\n#     return results\n\n# results = iterative_featureselection(\n#     model=model,\n#     X=X,\n#     y=y,\n#     split_fn=lambda X: rolling_window_split(X, train_size=100_000, test_size=20_000, step=20_000),\n#     drop_fraction=0.1,\n#     min_features=50,\n#     scoring=pearson_corr\n# )\n\ndef iterative_featureselection(\n    model,\n    X: pl.DataFrame,\n    y: pl.Series,\n    split_fn,\n    scorers: dict,\n    drop_fraction: float = 0.1,\n    min_features: int = 10,\n    use_shap: bool = False,\n    use_gpu : bool = True\n) -> list:\n    ID = uuid.uuid4().hex\n    results = []\n    splits = list(split_fn(X))\n    n_splits = len(splits)\n\n    current_X = X.clone()\n    feature_cols = current_X.columns\n    iteration = 0\n\n    # Convert y to appropriate format\n    if use_gpu:\n        try:\n            import cupy as cp\n            y_np = cp.asarray(y.to_numpy().flatten())\n        except ImportError:\n            print(\"CuPy not available, falling back to CPU\")\n            y_np = y.to_numpy().flatten()\n            use_gpu = False\n\n    while len(feature_cols) > min_features:\n        iteration += 1\n        print(f\"\\n[+] Iteration {iteration} - {len(feature_cols)} features\")\n\n        # Extract feature matrix\n        X_np = current_X.select(feature_cols).to_numpy()\n        if use_gpu:\n            X_np = cp.asarray(X_np)\n            \n        # Fit full model for feature importances\n        training_model = clone(model)\n        training_model.fit(X_np, y_np)\n\n        # 1. Compute raw importances as a NumPy array\n        if use_shap:\n            explainer   = shap.TreeExplainer(training_model)\n            shap_vals   = explainer.shap_values(X_np)          # shape (n_samples, n_features)\n            imp_array   = np.abs(shap_vals).mean(axis=0)        # mean(|SHAP|) per feature\n        else:\n            imp_array   = np.array(training_model.feature_importances_)\n        \n        # 2. Build a Polars DataFrame of (feature, importance)\n        importances_df = pl.DataFrame({\n            \"feature\": feature_cols,\n            \"importance\": imp_array.tolist()\n        })\n        \n        # Cross-validation\n        metrics = {k: [] for k in scorers}\n\n        for i, (train_idx, test_idx) in enumerate(splits, start=1):\n            print(f\"    [Fold {i}/{n_splits}] ...\", end=\"\\r\")\n\n            X_train, y_train = X_np[train_idx], y_np[train_idx]\n            X_test, y_test = X_np[test_idx], y_np[test_idx]\n\n            fold_model = clone(model)\n            fold_model.fit(X_train, y_train)\n            y_pred = fold_model.predict(X_test)\n\n            for name, func in scorers.items():\n                y_test = y_test.get() if hasattr(y_test, 'get') else y_test\n                y_pred = y_pred.get() if hasattr(y_pred, 'get') else y_pred\n                metrics[name].append(func(y_test, y_pred))\n\n        # Mean metric values\n        mean_metrics = {name: np.mean(vals) for name, vals in metrics.items()}\n\n        results.append({\n            \"ID\": ID,\n            \"params\": {\n                \"drop_fraction\" : drop_fraction,\n                \"min_features\": min_features,\n                \"use_shap\" : use_shap,\n                \"use_gpu\" : use_gpu\n            },\n            \"num_features\": len(feature_cols),\n            \"features\": feature_cols.copy(),\n            **{f\"scores_{name}_mean\": val for name, val in mean_metrics.items()}\n        })\n\n        print(\"    → \" + \" | \".join([f\"{k.upper()}: {v:.4f}\" for k, v in mean_metrics.items()]))\n\n        # 3. Identify the n_drop least important features\n        n_drop    = max(10, int(len(feature_cols) * drop_fraction))\n        drop_cols = (\n            importances_df\n            .sort(\"importance\")        # ascending\n            .head(n_drop)              # take the smallest n_drop\n            .get_column(\"feature\")     # extract the feature column\n            .to_list()                 # into a Python list for filtering\n        )\n        \n        # 4. Filter out dropped features in your Polars DataFrame\n        feature_cols = [f for f in feature_cols if f not in drop_cols]\n        current_X    = current_X.select(feature_cols)\n\n    print(f\"\"\"\\n {\"[\"+ \"*\" * 10 + \"]\"} Finished: {len(feature_cols)} features remaining (≤ min_features={min_features}) {\"[\"+ \"*\" * 10 + \"]\"}\"\"\")\n    return results\n    \ndef pearson_corr(y_true, y_pred):\n    return pearsonr(y_true, y_pred)[0]\n\nscorers = {\n    \"pearson\": pearson_corr,\n    \"mse\": mean_squared_error,\n    \"mae\": mean_absolute_error\n}\n\n# Test\n# results = iterative_featureselection(\n#     model=model,\n#     X=X,\n#     y=y,\n#     split_fn=lambda X:split_rollingwindow(X=X, n_splits = 2, train_ratio=0.5),\n#     scorers = scorers,\n#     drop_fraction=0.9,\n#     min_features=100,\n#     use_shap = False\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:21:32.306036Z","iopub.execute_input":"2025-06-10T09:21:32.306883Z","iopub.status.idle":"2025-06-10T09:21:36.058478Z","shell.execute_reply.started":"2025-06-10T09:21:32.306851Z","shell.execute_reply":"2025-06-10T09:21:36.057665Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import json\n\ndef save_results_to_json(results, out_path):\n    \"\"\"Save results to JSON file, appending if file exists\"\"\"\n    \n    if os.path.exists(out_path):\n        try:\n            # Load existing data\n            with open(out_path, 'r') as f:\n                existing_data = json.load(f)\n            \n            # Ensure existing_data is a list\n            if not isinstance(existing_data, list):\n                existing_data = [existing_data]\n            \n            # Append new results\n            existing_data.extend(results)\n            \n        except json.JSONDecodeError:\n            # If existing file is corrupted, start fresh\n            print(\"Warning: Existing JSON file corrupted, starting fresh\")\n            existing_data = results\n        \n        # Save combined data\n        with open(out_path, 'w') as f:\n            json.dump(existing_data, f, indent=2)\n    else:\n        # Create new file\n        with open(out_path, 'w') as f:\n            json.dump(results, f, indent=2)\n            \n# out_path = \"/kaggle/working/iterative_featuresselection.json\"\n# save_results_to_json(results, out_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:54.031793Z","iopub.status.idle":"2025-06-10T09:20:54.032107Z","shell.execute_reply.started":"2025-06-10T09:20:54.031915Z","shell.execute_reply":"2025-06-10T09:20:54.031931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(\"Training for Iteration 1 (shap=True)...\")\n# iter_1 = iterative_featureselection(\n#     model=model,\n#     X=X,\n#     y=y,\n#     split_fn=lambda X:split_rollingwindow(X=X, n_splits = 10, train_ratio=0.5),\n#     # split_fn=lambda X:split_overlapwindow(X, 50000, 50000, 25000),\n#     scorers = scorers,\n#     drop_fraction=0.1,\n#     min_features=10,\n#     use_shap = True\n# )\n# out_path = \"/kaggle/working/iterative_featuresselection.json\"\n# save_results_to_json(iter_1, out_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:20:54.033176Z","iopub.status.idle":"2025-06-10T09:20:54.033520Z","shell.execute_reply.started":"2025-06-10T09:20:54.033346Z","shell.execute_reply":"2025-06-10T09:20:54.033362Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training for Iteration 2 (shap = False)...\")\niter_2 = iterative_featureselection(\n    model=model,\n    X=X,\n    y=y,\n    # split_fn=lambda X:split_overlapwindow(X, 50000, 50000, 25000),\n    split_fn=lambda X:split_rollingwindow(X=X, n_splits = 10, train_ratio=0.5),\n    scorers = scorers,\n    drop_fraction=0.1,\n    min_features=10,\n    use_shap = False\n)\nout_path = \"/kaggle/working/iterative_featuresselection.json\"\nsave_results_to_json(iter_2, out_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T09:21:36.059465Z","iopub.execute_input":"2025-06-10T09:21:36.059872Z"}},"outputs":[{"name":"stdout","text":"Training for Iteration 2 (shap = False)...\n\n[+] Iteration 1 - 871 features\n    [Fold 1/10] ...\r","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"# Current Results","metadata":{}},{"cell_type":"code","source":"with open(\"/kaggle/working/iterative_featuresselection.json\", \"r\") as f:\n    current_results = json.load(f)\n\ncurrent_results_df = pl.DataFrame(current_results)\ncurrent_results_df","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
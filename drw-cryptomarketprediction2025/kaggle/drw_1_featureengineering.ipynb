{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":773.161209,"end_time":"2025-06-03T09:59:54.014100","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-03T09:47:00.852891","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","metadata":{"execution":{"iopub.status.busy":"2025-06-14T08:40:16.214185Z","iopub.execute_input":"2025-06-14T08:40:16.215053Z","iopub.status.idle":"2025-06-14T08:40:16.224145Z","shell.execute_reply.started":"2025-06-14T08:40:16.215014Z","shell.execute_reply":"2025-06-14T08:40:16.223341Z"},"papermill":{"duration":0.016021,"end_time":"2025-06-03T09:47:05.915432","exception":false,"start_time":"2025-06-03T09:47:05.899411","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\n/kaggle/input/drw-crypto-market-prediction/train.parquet\n/kaggle/input/drw-crypto-market-prediction/test.parquet\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"os.environ.get(\"USER\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T08:40:16.225355Z","iopub.execute_input":"2025-06-14T08:40:16.225663Z","iopub.status.idle":"2025-06-14T08:40:16.242132Z","shell.execute_reply.started":"2025-06-14T08:40:16.225637Z","shell.execute_reply":"2025-06-14T08:40:16.241565Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# DRW - Crypto Market Prediction\n\nThis notebook documents all the steps done in this project.\n\nTimeline:\n- 10/06/25: 0.05031\n    - Reorganize notebooks.\n    - Test training with GPU - Way faster than CPU.\n    - Implement feature elimination using GPU.\n    - Tested with Linear Models - will be extremely slow in iteration.\n    - Develop feature engineering pipeline\n- 14/06/25\n    - redevelop feature engineering pipeline - pipe results into downloadable results","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\nfrom scipy.stats import pearsonr\nimport polars as pl\nimport numpy as np\nfrom tqdm import tqdm\nfrom datetime import datetime\n\ntrain_splits = {\n    \"full\" : pl.datetime(2023, 3, 1, 0, 0, 0),\n    \"last_9m\" : pl.datetime(2023, 6, 1, 0, 0, 0),\n    \"last_6m\" : pl.datetime(2023, 9, 1, 0, 0, 0),\n    \"last_3m\" : pl.datetime(2023, 12, 1, 0, 0, 0),\n    \"last_1m\": pl.datetime(2024, 2, 1, 0, 0, 0),\n    \"last_2w\": pl.datetime(2024, 2, 15, 0, 0, 0),\n}\n\nPATHS = {\n    \"TRAIN_PATH\" :\"/kaggle/input/drw-crypto-market-prediction/train.parquet\",\n    \"TEST_PATH\" : \"/kaggle/input/drw-crypto-market-prediction/test.parquet\",\n    \"SUBMISSION_PATH\" : \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\",\n}\n\n# features = []\n\ndef load_data(TRAIN_PATH: str, TEST_PATH: str):\n    if os.environ.get(\"USER\"):\n        TRAIN_PATH = \".\" + TRAIN_PATH\n        TEST_PATH = \".\" + TEST_PATH\n    train_data = pl.read_parquet(TRAIN_PATH).sort(\"timestamp\", descending = False)\n    test_data = pl.read_parquet(TEST_PATH)\n    # print(f\"Train data shape: {train_data.shape}\")\n    # print(f\"Test data shape: {test_data.shape}\")\n    return train_data, test_data\n\ntrain_data, test_data = load_data(\n    TRAIN_PATH = PATHS[\"TRAIN_PATH\"],\n    TEST_PATH = PATHS[\"TEST_PATH\"],\n)\nprint(train_data)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-06-14T08:40:16.242749Z","iopub.execute_input":"2025-06-14T08:40:16.242900Z","iopub.status.idle":"2025-06-14T08:40:44.034131Z","shell.execute_reply.started":"2025-06-14T08:40:16.242887Z","shell.execute_reply":"2025-06-14T08:40:44.033340Z"},"papermill":{"duration":2.595897,"end_time":"2025-06-03T09:47:08.514965","exception":false,"start_time":"2025-06-03T09:47:05.919068","status":"completed"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","text":"shape: (525_887, 897)\n┌─────────┬─────────┬─────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────────┐\n│ bid_qty ┆ ask_qty ┆ buy_qty ┆ sell_qty ┆ … ┆ X889     ┆ X890     ┆ label    ┆ timestamp    │\n│ ---     ┆ ---     ┆ ---     ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---          │\n│ f64     ┆ f64     ┆ f64     ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ datetime[ns] │\n╞═════════╪═════════╪═════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════════╡\n│ 15.283  ┆ 8.425   ┆ 176.405 ┆ 44.984   ┆ … ┆ 0.159183 ┆ 0.530636 ┆ 0.562539 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:00:00     │\n│ 38.59   ┆ 2.336   ┆ 525.846 ┆ 321.95   ┆ … ┆ 0.158963 ┆ 0.530269 ┆ 0.533686 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:01:00     │\n│ 0.442   ┆ 60.25   ┆ 159.227 ┆ 136.369  ┆ … ┆ 0.158744 ┆ 0.529901 ┆ 0.546505 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:02:00     │\n│ 4.865   ┆ 21.016  ┆ 335.742 ┆ 124.963  ┆ … ┆ 0.158524 ┆ 0.529534 ┆ 0.357703 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:03:00     │\n│ 27.158  ┆ 3.451   ┆ 98.411  ┆ 44.407   ┆ … ┆ 0.158304 ┆ 0.529167 ┆ 0.362452 ┆ 2023-03-01   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:04:00     │\n│ …       ┆ …       ┆ …       ┆ …        ┆ … ┆ …        ┆ …        ┆ …        ┆ …            │\n│ 4.163   ┆ 6.805   ┆ 39.037  ┆ 55.351   ┆ … ┆ 0.136494 ┆ 0.243172 ┆ 0.396289 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:55:00     │\n│ 2.29    ┆ 4.058   ┆ 110.201 ┆ 67.171   ┆ … ┆ 0.136305 ┆ 0.243004 ┆ 0.328993 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:56:00     │\n│ 5.237   ┆ 3.64    ┆ 70.499  ┆ 30.753   ┆ … ┆ 0.136117 ┆ 0.242836 ┆ 0.189909 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:57:00     │\n│ 5.731   ┆ 4.901   ┆ 22.365  ┆ 52.195   ┆ … ┆ 0.135928 ┆ 0.242668 ┆ 0.410831 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:58:00     │\n│ 3.925   ┆ 3.865   ┆ 86.585  ┆ 217.137  ┆ … ┆ 0.135741 ┆ 0.242501 ┆ 0.731542 ┆ 2024-02-29   │\n│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:59:00     │\n└─────────┴─────────┴─────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────────┘\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# 1 Data","metadata":{"papermill":{"duration":0.002673,"end_time":"2025-06-03T09:47:08.520904","exception":false,"start_time":"2025-06-03T09:47:08.518231","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## 1.1 Pre-processing / Feature Engineering\n\n**Pre-Processing**\n1. inf/-inf columns: `['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']`\n2. columns with NaN values: `[]`\n3. 0 std columns : `['X864', 'X867', 'X869', 'X870', 'X871', 'X872']`\n\n\n**Feature Engineering**\n1. `bidask_ratio`\n2. `buysell_ratio`\n3. `bidask_delta`\n4. `buysell_delta`\n5. `buysell_size`\n6. `bidask_size`","metadata":{}},{"cell_type":"code","source":"def get_cols_inf(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names that contain any positive or negative infinity.\n    \"\"\"\n    cols = []\n    for col in df.columns:\n        # df[col] is a Series; .is_infinite() → Boolean Series; .any() → Python bool\n        try:\n            if df[col].is_infinite().any():\n                cols.append(col)\n        except Exception:\n            # if the column isn’t numeric, .is_infinite() might error—just skip it\n            continue\n    return cols\n\ndef get_nan_columns(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names with any NaN/null values.\n    \"\"\"\n    cols = []\n    for col in df.columns:\n        if df.select(pl.col(col).is_null().any()).item():\n            cols.append(col)\n    return cols\n\ndef get_cols_zerostd(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names whose standard deviation is zero\n    (or whose std returns None because all values are null).\n    Non-numeric columns (e.g. datetime) are skipped.\n    \"\"\"\n    cols = []\n    for col, dtype in zip(df.columns, df.dtypes):\n        # Only attempt std() on numeric dtypes\n        if dtype.is_numeric():  \n            # df[col] is a Series; .std() returns a Python float or None\n            std_val = df[col].std()\n            if std_val == 0.0 or std_val is None:\n                cols.append(col)\n    return cols\n\n\ndef feature_engineering(df: pl.DataFrame) -> pl.DataFrame:\n    # Feature engineering\n    df = df.with_columns([\n        # bidask_ratio = bid_qty / ask_qty\n        (pl.col(\"bid_qty\") / pl.col(\"ask_qty\")).alias(\"bidask_ratio\"),\n\n        # buysell_ratio = 0 if volume == 0 else buy_qty / sell_qty\n        pl.when(pl.col(\"volume\") == 0)\n        .then(0)\n        .otherwise(pl.col(\"buy_qty\") / pl.col(\"sell_qty\"))\n        .alias(\"buysell_ratio\"),\n\n        # bidask_delta = bid_qty - ask_qty\n        (pl.col(\"bid_qty\") - pl.col(\"ask_qty\")).alias(\"bidask_delta\"),\n\n        # buysell_delta = buy_qty - sell_qty\n        (pl.col(\"buy_qty\") - pl.col(\"sell_qty\")).alias(\"buysell_delta\"),\n\n        # buysell_size = buy_qty + sell_qty\n        (pl.col(\"buy_qty\") + pl.col(\"sell_qty\")).alias(\"buysell_size\"),\n\n        # bidask_size = bid_qty + ask_qty\n        (pl.col(\"bid_qty\") + pl.col(\"ask_qty\")).alias(\"bidask_size\"),\n    ])\n    return df\ndef preprocess_train(train: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n    \"\"\"\n    Mirror of the original pandas workflow, but using polars.\n    1. Identify columns with infinite, NaN, or zero‐std and drop them.\n    2. Drop any user‐specified columns (e.g. label or order‐book columns).\n    3. (You can add normalized/scaling steps here if needed.)\n    \"\"\"\n    df = train.clone()\n\n    df = feature_engineering(df)\n    \n    #### Preprocessing\n    cols_inf = get_cols_inf(df)\n    print(\"Columns with infinite values:\", cols_inf)\n\n    cols_nan = get_nan_columns(df)\n    print(\"Columns with NaN values:\", cols_nan)\n\n    cols_zerostd = get_cols_zerostd(df)\n    print(\"Columns with zero standard deviation:\", cols_zerostd)\n    # Drop columns with infinite, NaN, or zero‐std values\n    drop_columns = list(set(cols_inf) | set(cols_nan) | set(cols_zerostd) | set(columns_to_drop))\n    if drop_columns:\n        df = df.drop(drop_columns)\n    # df = df.sort(\"timestamp\", descending=False)\n    return df, drop_columns\n\ndef preprocess_test(test: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n    df = test.clone()\n    df = feature_engineering(df)\n    df = df.drop(columns_to_drop)\n    print(\"Columns dropped from test set:\", columns_to_drop)\n    return df","metadata":{"execution":{"iopub.status.busy":"2025-06-14T08:40:44.035859Z","iopub.execute_input":"2025-06-14T08:40:44.036161Z","iopub.status.idle":"2025-06-14T08:40:44.049031Z","shell.execute_reply.started":"2025-06-14T08:40:44.036143Z","shell.execute_reply":"2025-06-14T08:40:44.048352Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# 2 Feature Selection Model","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_selection import (\n    VarianceThreshold, SelectKBest, SelectPercentile,\n    GenericUnivariateSelect, SelectFpr, SelectFdr, SelectFwe,\n    RFE, RFECV, SelectFromModel, SequentialFeatureSelector,\n    f_regression, mutual_info_regression\n)\nfrom sklearn.decomposition import (\n    PCA, IncrementalPCA, TruncatedSVD,\n    FastICA, SparsePCA, MiniBatchSparsePCA,\n    DictionaryLearning, MiniBatchDictionaryLearning,\n    FactorAnalysis, NMF, LatentDirichletAllocation\n)\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom tqdm.auto import tqdm\nfrom xgboost import XGBRegressor\nfrom sklearn.base import clone\n\nclass SklearnFeatureEngineeringRegression:\n    def __init__(self, X: pd.DataFrame, y: pd.Series):\n        \"\"\"\n        Parameters\n        ----------\n        X : pd.DataFrame\n            Predictor matrix (n_samples × P features)\n        y : pd.Series\n            Target vector (n_samples,)\n        \"\"\"\n        self.X = X.copy()\n        self.y = y.copy()\n        self.X_np = X.to_numpy()\n        self.y_np = y.to_numpy()\n        self.features = X.columns.tolist()\n        self.results_df: pd.DataFrame = pd.DataFrame()\n        self.use_gpu = os.environ.get(\"USER\") == None\n\n        # … your import logic deciding CPU vs GPU …\n        if self.use_gpu:\n            print(\"*\" * 20 + \"GPU \" * 5 + \"*\" * 20)\n            import cupy as cp\n            self.X_np = cp.asarray(self.X_np)\n            self.y_np = cp.asarray(self.y_np)\n            self.tree_model = XGBRegressor(\n                tree_method =\"hist\", device=\"cuda\",\n                n_estimators=10, max_depth=10,\n                learning_rate=0.1, random_state=42, n_jobs=-1\n            )\n        else:\n            print(\"*\" * 20 + \"CPU \" * 5 + \"*\" * 20)\n            self.tree_model = XGBRegressor(\n                tree_method=\"hist\", n_estimators=10, max_depth=10,\n                learning_rate=0.05, random_state=42, n_jobs=-1\n            )\n        print(f\"\"\"SklearnFeatureEngineering Instantiated\"\"\")\n        print(f\"... Fitting {self.tree_model.__class__.__name__} on {self.X.shape[0]} rows and {self.X.shape[1]} features.\")\n        self.tree_model.fit(self.X_np, self.y_np)\n\n    # ---------- Filter methods ----------\n    # VarianceThreshold is a filter method that removes features with low variance.\n    def _variance_threshold(self, thresh: float = 0.0) -> pd.Series:\n        sel = VarianceThreshold(threshold=thresh).fit(self.X)\n        return pd.Series(sel.get_support(), index=self.features).astype(int)\n    # SelectKBest and SelectPercentile are filter methods that select features based on univariate statistical tests.\n    def _select_kbest_freg(self, k: int = 10) -> pd.Series:\n        sel = SelectKBest(score_func=f_regression, k=min(k, self.X.shape[1]))\n        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n    def _select_kbest_mutualinfo(self, k: int = 10) -> pd.Series:\n        sel = SelectKBest(score_func=mutual_info_regression, k=min(k, self.X.shape[1]))\n        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n    def _select_percentile_freg(self, p: float = 10) -> pd.Series:\n        sel = SelectPercentile(score_func=f_regression, percentile=min(p, 100))\n        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n    def _select_percentile_mutualinfo(self, p: float = 10) -> pd.Series:\n        sel = SelectPercentile(score_func=mutual_info_regression, percentile=min(p, 100))\n        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n    # # GenericUnivariateSelect is a filter method that allows for more flexible selection criteria.\n    # def _generic_univariate(self, k: int = 10) -> pd.Series:\n    #     sel = GenericUnivariateSelect(score_func=f_regression, mode='k_best', param=min(k, self.X.shape[1]))\n    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n\n    def _select_fpr_freg(self, alpha: float = 0.05) -> pd.Series:\n        sel = SelectFpr(score_func=f_regression, alpha=alpha)\n        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n\n    def _select_fdr_freg(self, alpha: float = 0.05) -> pd.Series:\n        sel = SelectFdr(score_func=f_regression, alpha=alpha)\n        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n\n    def _select_fwe_freg(self, alpha: float = 0.05) -> pd.Series:\n        sel = SelectFwe(score_func=f_regression, alpha=alpha)\n        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n\n    # ---------- Wrapper and Embedded methods ----------\n    # def _rfe_lasso(self, n_features: int = 10) -> pd.Series:\n    #     estimator = LassoCV(cv=5, max_iter=5000).fit(self.X, self.y)\n    #     sel = RFE(estimator, n_features_to_select=min(n_features, self.X.shape[1]))\n    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n\n    # def _rfecv_lasso(self) -> pd.Series:\n    #     estimator = LassoCV(cv=5, max_iter=5000).fit(self.X, self.y)\n    #     sel = RFECV(estimator, cv=5)\n    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n\n    def _rfe_xgb(self, n_features: int = 10) -> pd.Series:\n        \"\"\"\n        Recursive Feature Elimination with a fresh clone of self.tree_model.\n        \"\"\"\n        # clone preserves GPU/CPU config + hyper‐parameters\n        estimator = clone(self.tree_model)\n        sel = RFE(estimator, n_features_to_select=min(n_features, self.X.shape[1]))\n        \n        X_fit = self.X_np.get() if self.use_gpu else self.X_np\n        y_fit = self.y_np.get() if self.use_gpu else self.y_np\n\n        mask = sel.fit(X_fit, y_fit).get_support()\n        return pd.Series(mask.astype(int), index=self.features).astype(int)\n\n    def _rfecv_xgb(self, cv: int = 5) -> pd.Series:\n        \"\"\"\n        RFECV with a fresh clone of self.tree_model.\n        \"\"\"\n        estimator = clone(self.tree_model)\n        sel = RFECV(estimator, cv=cv, scoring=\"neg_mean_squared_error\")\n        \n        X_fit = self.X_np.get() if self.use_gpu else self.X_np\n        y_fit = self.y_np.get() if self.use_gpu else self.y_np\n        \n        mask = sel.fit(X_fit, y_fit).get_support()\n        return pd.Series(mask.astype(int), index=self.features).astype(int)\n    \n    def _select_from_model_tree(self, threshold=\"median\") -> pd.Series:\n        \"\"\"\n        Uses the pre‐fitted self.tree_model via prefit=True.\n        \"\"\"\n        sel = SelectFromModel(self.tree_model, threshold=threshold, prefit=True)\n        mask = sel.get_support()\n        return pd.Series(mask.astype(int), index=self.features)\n\n    def _sequential_tree(\n        self,\n        n_features: int = 10,\n        direction: str = \"forward\",\n        cv: int = 5,\n        n_jobs: int = -1\n    ) -> pd.Series:\n        \"\"\"\n        SequentialFeatureSelector with XGBRegressor.\n\n        - Clones self.tree_model to preserve GPU/CPU config.\n        - direction: 'forward' or 'backward'\n        - cv: number of cross‐validation folds\n        - n_jobs: parallel jobs for CV\n        \"\"\"\n        # ensure we don't modify the original fitted model\n        estimator = clone(self.tree_model)\n\n        sfs = SequentialFeatureSelector(\n            estimator,\n            n_features_to_select=min(n_features, self.X.shape[1]),\n            direction=direction,\n            cv=cv,\n            n_jobs=n_jobs\n        )\n        \n        X_fit = self.X_np.get() if self.use_gpu else self.X_np\n        y_fit = self.y_np.get() if self.use_gpu else self.y_np\n        \n        mask = sfs.fit(X_fit, y_fit).get_support()\n        return pd.Series(mask.astype(int), index=self.features)\n\n    # ---------- Decomposition methods ----------\n    def _pca(self, n_components: float = 10, svd_solver : str = \"covariance_eigh\") -> pd.Series:\n        pca = PCA(n_components=n_components).fit(self.X)\n        load = pd.DataFrame(pca.components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _incremental_pca(self, n_components: float = 10) -> pd.Series:\n        ipca = IncrementalPCA(n_components=n_components)\n        load = pd.DataFrame(ipca.fit(self.X).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _truncated_svd(self, n_components: int = 10) -> pd.Series:\n        k = min(n_components, self.X.shape[1])\n        ts = TruncatedSVD(n_components=k)\n        load = pd.DataFrame(ts.fit(self.X).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _fast_ica(self, n_components: int = 10) -> pd.Series:\n        ic = FastICA(n_components=min(n_components, self.X.shape[1]), max_iter=200)\n        load = pd.DataFrame(ic.fit(self.X).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _sparse_pca(self, n_components: int = 10) -> pd.Series:\n        spca = SparsePCA(n_components=min(n_components, self.X.shape[1]), alpha=1, max_iter=1000)\n        load = pd.DataFrame(spca.fit(self.X).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _minibatch_sparse_pca(self, n_components: int = 10) -> pd.Series:\n        mbspca = MiniBatchSparsePCA(n_components=min(n_components, self.X.shape[1]), alpha=1)\n        load = pd.DataFrame(mbspca.fit(self.X).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _dict_learning(self, n_components: int = 10) -> pd.Series:\n        dl = DictionaryLearning(n_components=min(n_components, self.X.shape[1]), alpha=1, max_iter=1000)\n        load = pd.DataFrame(dl.fit(self.X).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _minibatch_dict_learning(self, n_components: int = 10) -> pd.Series:\n        mbdl = MiniBatchDictionaryLearning(n_components=min(n_components, self.X.shape[1]), alpha=1)\n        load = pd.DataFrame(mbdl.fit(self.X).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _factor_analysis(self, n_components: int = 10) -> pd.Series:\n        fa = FactorAnalysis(n_components=min(n_components, self.X.shape[1]))\n        load = pd.DataFrame(fa.fit(self.X).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _nmf(self, n_components: int = 10) -> pd.Series:\n        nmf = NMF(n_components=min(n_components, self.X.shape[1]), init='nndsvda', max_iter=500)\n        data_pos = self.X.clip(lower=0)\n        load = pd.DataFrame(nmf.fit(data_pos).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    def _lda(self, n_components: int = 10) -> pd.Series:\n        lda = LatentDirichletAllocation(n_components=min(n_components, self.X.shape[1]), max_iter=5)\n        data_pos = self.X.clip(lower=0)\n        load = pd.DataFrame(lda.fit(data_pos).components_.T, index=self.features)\n        thresh = load.abs().mean().mean()\n        return (load.abs().max(axis=1) >= thresh).astype(int)\n\n    # ---- Random projections ----\n    def _gaussian_random_projection(self, n_components: int = 10) -> pd.Series:\n        rp = GaussianRandomProjection(n_components=min(n_components, self.X.shape[1]))\n        comp = pd.DataFrame(rp.fit(self.X).components_.T, index=self.features)\n        return self._decomp_mask(comp)\n\n    def _sparse_random_projection(self, n_components: int = 10) -> pd.Series:\n        srp = SparseRandomProjection(n_components=min(n_components, self.X.shape[1]))\n        comp = pd.DataFrame(srp.fit(self.X).components_.T, index=self.features)\n        return self._decomp_mask(comp)\n\n    # ---- Feature grouping ----\n    def _feature_agglomeration(self, n_clusters: int = 10) -> pd.Series:\n        # Fit agglomeration on the array\n        agg = FeatureAgglomeration(n_clusters=min(n_clusters, self.X.shape[1]))\n        agg.fit(self.X)\n        labels       = agg.labels_\n        cluster_data = agg.transform(self.X)\n\n        # Build a temporary DataFrame for cluster components\n        df_clust = pd.DataFrame(\n            cluster_data,\n            columns=[f\"clus_{i}\" for i in range(cluster_data.shape[1])],\n            index=self.X.index\n        )\n\n        # For each cluster, pick the feature with highest corr to its component\n        mask = pd.Series(0, index=self.features)\n        for j in range(cluster_data.shape[1]):\n            members = [f for f, l in zip(self.features, labels) if l == j]\n            corrs   = self.X[members].corrwith(df_clust.iloc[:, j]).abs()\n            mask[corrs.idxmax()] = 1\n\n        return mask\n    \n    def run(self, methods=None) -> None:\n        \"\"\"\n        Execute each selection/decomposition method with a progress bar,\n        accepting either:\n          - a dict mapping method names to parameter dicts, or\n          - a list of method names (no parameters).\n\n        Assembles `results_df` with columns ['model', features..., 'total_score'].\n        \"\"\"\n        # default methods with default params\n        default_list = [\n            '_variance_threshold',\n            '_select_kbest_freg','_select_percentile_freg',\n            '_select_kbest_mutualinfo','_select_percentile_mutualinfo',\n            '_select_fpr_freg','_select_fdr_freg','_select_fwe_freg',\n            '_rfe_xgb','_rfecv_xgb',\n            '_select_from_model_tree','_sequential_tree',\n            '_pca','_incremental_pca','_truncated_svd','_fast_ica',\n            '_sparse_pca','_minibatch_sparse_pca','_dict_learning',\n            '_minibatch_dict_learning','_factor_analysis','_nmf','_lda',\n            '_gaussian_random_projection','_sparse_random_projection',\n            '_feature_agglomeration'\n        ]\n        if methods is None:\n            methods = {m: {} for m in default_list}\n        elif isinstance(methods, list):\n            methods = {m: {} for m in methods}\n        elif not isinstance(methods, dict):\n            raise ValueError(\"`methods` must be None, list, or dict\")\n\n        records = []\n        for m, params in tqdm(methods.items(), desc='Running feature selection'):\n            try:\n                if not hasattr(self, m):\n                    raise KeyError(f\"Method {m} not found in class\")\n                    \n                start = time.time()\n                \n                fn = getattr(self, m)\n                mask = fn(**params)\n                records.append({'model': m.lstrip('_'), **mask.to_dict(), 'total_score': int(mask.sum())})\n                end = time.time()\n                print(f\"\"\"Function : `{m.lstrip('_')}` completed. Time taken : {(end - start) / 60} minutes.\"\"\")\n            except Exception as e:\n                print(f\"Error running method {m}: {e}\")\n        self.results_df = pd.DataFrame.from_records(records)\n\n    def get_results(self) -> pd.DataFrame:\n        \"\"\"Return the raw selection matrix (n_models × (P + 2)).\"\"\"\n        return self.results_df\n\n    def get_top_features(self, N: int) -> pd.DataFrame:\n        \"\"\"\n        Aggregate across models (vote count = sum of 1’s per feature),\n        and return the top N feature names.\n        \"\"\"\n        return pd.DataFrame(self.results_df.loc[:, self.features].sum().sort_values(ascending=False), columns=['score']).head(N)\n\n    def get_model_featuresincluded(self) -> pd.DataFrame:\n        \"\"\"\n        Returns a DataFrame with number of features included per model.\n        \"\"\"\n        model_features = self.results_df.set_index(\"model\").sum(axis=1).reset_index()\n        model_features.columns = [\"model\", \"features_included\"]\n        return model_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T08:54:00.392252Z","iopub.execute_input":"2025-06-14T08:54:00.392959Z","iopub.status.idle":"2025-06-14T08:54:00.450257Z","shell.execute_reply.started":"2025-06-14T08:54:00.392927Z","shell.execute_reply":"2025-06-14T08:54:00.449621Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Workflow","metadata":{}},{"cell_type":"code","source":"train_data_filtered = train_data.filter(\n    pl.col(\"timestamp\") >= train_splits[\"last_2w\"]\n)\n\ny = train_data_filtered[\"label\"]\nX, drop_columns = preprocess_train(\n    train_data_filtered,\n    columns_to_drop=[\"label\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n)\nprint(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T08:40:45.235025Z","iopub.execute_input":"2025-06-14T08:40:45.235340Z","iopub.status.idle":"2025-06-14T08:40:45.478375Z","shell.execute_reply.started":"2025-06-14T08:40:45.235322Z","shell.execute_reply":"2025-06-14T08:40:45.477633Z"}},"outputs":[{"name":"stdout","text":"Columns with infinite values: ['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']\nColumns with NaN values: []\nColumns with zero standard deviation: ['X864', 'X867', 'X869', 'X870', 'X871', 'X872']\nshape: (21_599, 871)\n┌─────────┬───────────┬───────────┬──────────┬───┬────────────┬────────────┬───────────┬───────────┐\n│ volume  ┆ X1        ┆ X2        ┆ X3       ┆ … ┆ bidask_del ┆ buysell_de ┆ buysell_s ┆ bidask_si │\n│ ---     ┆ ---       ┆ ---       ┆ ---      ┆   ┆ ta         ┆ lta        ┆ ize       ┆ ze        │\n│ f64     ┆ f64       ┆ f64       ┆ f64      ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n│         ┆           ┆           ┆          ┆   ┆ f64        ┆ f64        ┆ f64       ┆ f64       │\n╞═════════╪═══════════╪═══════════╪══════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n│ 181.331 ┆ -0.606128 ┆ -0.026132 ┆ 0.240236 ┆ … ┆ 1.125      ┆ -60.097    ┆ 181.331   ┆ 8.443     │\n│ 110.604 ┆ -0.613099 ┆ -0.03643  ┆ 0.218995 ┆ … ┆ -10.517    ┆ -47.308    ┆ 110.604   ┆ 10.901    │\n│ 70.052  ┆ -0.47534  ┆ 0.217352  ┆ 0.478561 ┆ … ┆ 3.957      ┆ 1.746      ┆ 70.052    ┆ 11.881    │\n│ 124.021 ┆ -0.51895  ┆ 0.118303  ┆ 0.378718 ┆ … ┆ -4.846     ┆ 28.543     ┆ 124.021   ┆ 13.68     │\n│ 131.966 ┆ -0.693    ┆ -0.208907 ┆ 0.02963  ┆ … ┆ 6.577      ┆ -5.672     ┆ 131.966   ┆ 11.149    │\n│ …       ┆ …         ┆ …         ┆ …        ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n│ 94.388  ┆ 0.020155  ┆ 0.076565  ┆ 0.228994 ┆ … ┆ -2.642     ┆ -16.314    ┆ 94.388    ┆ 10.968    │\n│ 177.372 ┆ 0.016262  ┆ 0.062527  ┆ 0.214072 ┆ … ┆ -1.768     ┆ 43.03      ┆ 177.372   ┆ 6.348     │\n│ 101.252 ┆ 0.045407  ┆ 0.109834  ┆ 0.263577 ┆ … ┆ 1.597      ┆ 39.746     ┆ 101.252   ┆ 8.877     │\n│ 74.56   ┆ 0.124783  ┆ 0.244168  ┆ 0.408704 ┆ … ┆ 0.83       ┆ -29.83     ┆ 74.56     ┆ 10.632    │\n│ 303.722 ┆ 0.368659  ┆ 0.665382  ┆ 0.867538 ┆ … ┆ 0.06       ┆ -130.552   ┆ 303.722   ┆ 7.79      │\n└─────────┴───────────┴───────────┴──────────┴───┴────────────┴────────────┴───────────┴───────────┘\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"fe_regression = SklearnFeatureEngineeringRegression(\n    X = X.drop([\"timestamp\"]).to_pandas(),\n    y = y.to_pandas()\n)\nfe_regression.run()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T08:54:03.139921Z","iopub.execute_input":"2025-06-14T08:54:03.140164Z"}},"outputs":[{"name":"stdout","text":"********************GPU GPU GPU GPU GPU ********************\nSklearnFeatureEngineering Instantiated\n... Fitting XGBRegressor on 21599 rows and 870 features.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Running feature selection:   0%|          | 0/26 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d8e46b4b0a44e7381ab4d32d5ea26ec"}},"metadata":{}},{"name":"stdout","text":"Error running method _rfe_xgb: name 'use_gpu' is not defined\nError running method _rfecv_xgb: name 'use_gpu' is not defined\nError running method _sequential_tree: name 'use_gpu' is not defined\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_fastica.py:542: FutureWarning: Starting in v1.3, whiten='unit-variance' will be used by default.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"fe_regression.get_results()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T08:43:34.546653Z","iopub.status.idle":"2025-06-14T08:43:34.546900Z","shell.execute_reply.started":"2025-06-14T08:43:34.546795Z","shell.execute_reply":"2025-06-14T08:43:34.546805Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fe_regression.results_df.to_xlsx(\"/kaggle/working/feature_engineering_results.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T08:43:34.547871Z","iopub.status.idle":"2025-06-14T08:43:34.548183Z","shell.execute_reply.started":"2025-06-14T08:43:34.548027Z","shell.execute_reply":"2025-06-14T08:43:34.548041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"fe_regression.get_top_features(50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T08:43:34.549711Z","iopub.status.idle":"2025-06-14T08:43:34.549959Z","shell.execute_reply.started":"2025-06-14T08:43:34.549852Z","shell.execute_reply":"2025-06-14T08:43:34.549862Z"}},"outputs":[],"execution_count":null}]}
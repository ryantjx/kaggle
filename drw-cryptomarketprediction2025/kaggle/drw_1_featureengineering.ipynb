{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:40:16.215053Z",
     "iopub.status.busy": "2025-06-14T08:40:16.214185Z",
     "iopub.status.idle": "2025-06-14T08:40:16.224145Z",
     "shell.execute_reply": "2025-06-14T08:40:16.223341Z",
     "shell.execute_reply.started": "2025-06-14T08:40:16.215014Z"
    },
    "papermill": {
     "duration": 0.016021,
     "end_time": "2025-06-03T09:47:05.915432",
     "exception": false,
     "start_time": "2025-06-03T09:47:05.899411",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:40:16.225663Z",
     "iopub.status.busy": "2025-06-14T08:40:16.225355Z",
     "iopub.status.idle": "2025-06-14T08:40:16.242132Z",
     "shell.execute_reply": "2025-06-14T08:40:16.241565Z",
     "shell.execute_reply.started": "2025-06-14T08:40:16.225637Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ryant'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get(\"USER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRW - Crypto Market Prediction\n",
    "\n",
    "This notebook documents all the steps done in this project.\n",
    "\n",
    "Timeline:\n",
    "- 10/06/25: 0.05031\n",
    "    - Reorganize notebooks.\n",
    "    - Test training with GPU - Way faster than CPU.\n",
    "    - Implement feature elimination using GPU.\n",
    "    - Tested with Linear Models - will be extremely slow in iteration.\n",
    "    - Develop feature engineering pipeline\n",
    "- 14/06/25\n",
    "    - redevelop feature engineering pipeline - pipe results into downloadable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-14T08:40:16.242900Z",
     "iopub.status.busy": "2025-06-14T08:40:16.242749Z",
     "iopub.status.idle": "2025-06-14T08:40:44.034131Z",
     "shell.execute_reply": "2025-06-14T08:40:44.033340Z",
     "shell.execute_reply.started": "2025-06-14T08:40:16.242887Z"
    },
    "papermill": {
     "duration": 2.595897,
     "end_time": "2025-06-03T09:47:08.514965",
     "exception": false,
     "start_time": "2025-06-03T09:47:05.919068",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (525_887, 897)\n",
      "┌─────────┬─────────┬─────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────────┐\n",
      "│ bid_qty ┆ ask_qty ┆ buy_qty ┆ sell_qty ┆ … ┆ X889     ┆ X890     ┆ label    ┆ timestamp    │\n",
      "│ ---     ┆ ---     ┆ ---     ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---          │\n",
      "│ f64     ┆ f64     ┆ f64     ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ datetime[ns] │\n",
      "╞═════════╪═════════╪═════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════════╡\n",
      "│ 15.283  ┆ 8.425   ┆ 176.405 ┆ 44.984   ┆ … ┆ 0.159183 ┆ 0.530636 ┆ 0.562539 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:00:00     │\n",
      "│ 38.59   ┆ 2.336   ┆ 525.846 ┆ 321.95   ┆ … ┆ 0.158963 ┆ 0.530269 ┆ 0.533686 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:01:00     │\n",
      "│ 0.442   ┆ 60.25   ┆ 159.227 ┆ 136.369  ┆ … ┆ 0.158744 ┆ 0.529901 ┆ 0.546505 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:02:00     │\n",
      "│ 4.865   ┆ 21.016  ┆ 335.742 ┆ 124.963  ┆ … ┆ 0.158524 ┆ 0.529534 ┆ 0.357703 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:03:00     │\n",
      "│ 27.158  ┆ 3.451   ┆ 98.411  ┆ 44.407   ┆ … ┆ 0.158304 ┆ 0.529167 ┆ 0.362452 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:04:00     │\n",
      "│ …       ┆ …       ┆ …       ┆ …        ┆ … ┆ …        ┆ …        ┆ …        ┆ …            │\n",
      "│ 4.163   ┆ 6.805   ┆ 39.037  ┆ 55.351   ┆ … ┆ 0.136494 ┆ 0.243172 ┆ 0.396289 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:55:00     │\n",
      "│ 2.29    ┆ 4.058   ┆ 110.201 ┆ 67.171   ┆ … ┆ 0.136305 ┆ 0.243004 ┆ 0.328993 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:56:00     │\n",
      "│ 5.237   ┆ 3.64    ┆ 70.499  ┆ 30.753   ┆ … ┆ 0.136117 ┆ 0.242836 ┆ 0.189909 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:57:00     │\n",
      "│ 5.731   ┆ 4.901   ┆ 22.365  ┆ 52.195   ┆ … ┆ 0.135928 ┆ 0.242668 ┆ 0.410831 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:58:00     │\n",
      "│ 3.925   ┆ 3.865   ┆ 86.585  ┆ 217.137  ┆ … ┆ 0.135741 ┆ 0.242501 ┆ 0.731542 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:59:00     │\n",
      "└─────────┴─────────┴─────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "train_splits = {\n",
    "    \"full\" : pl.datetime(2023, 3, 1, 0, 0, 0),\n",
    "    \"last_9m\" : pl.datetime(2023, 6, 1, 0, 0, 0),\n",
    "    \"last_6m\" : pl.datetime(2023, 9, 1, 0, 0, 0),\n",
    "    \"last_3m\" : pl.datetime(2023, 12, 1, 0, 0, 0),\n",
    "    \"last_1m\": pl.datetime(2024, 2, 1, 0, 0, 0),\n",
    "    \"last_2w\": pl.datetime(2024, 2, 15, 0, 0, 0),\n",
    "}\n",
    "\n",
    "PATHS = {\n",
    "    \"TRAIN_PATH\" :\"/kaggle/input/drw-crypto-market-prediction/train.parquet\",\n",
    "    \"TEST_PATH\" : \"/kaggle/input/drw-crypto-market-prediction/test.parquet\",\n",
    "    \"SUBMISSION_PATH\" : \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\",\n",
    "}\n",
    "\n",
    "# features = []\n",
    "\n",
    "def load_data(TRAIN_PATH: str, TEST_PATH: str):\n",
    "    if os.environ.get(\"USER\"):\n",
    "        TRAIN_PATH = \".\" + TRAIN_PATH\n",
    "        TEST_PATH = \".\" + TEST_PATH\n",
    "    train_data = pl.read_parquet(TRAIN_PATH).sort(\"timestamp\", descending = False)\n",
    "    test_data = pl.read_parquet(TEST_PATH)\n",
    "    # print(f\"Train data shape: {train_data.shape}\")\n",
    "    # print(f\"Test data shape: {test_data.shape}\")\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_data(\n",
    "    TRAIN_PATH = PATHS[\"TRAIN_PATH\"],\n",
    "    TEST_PATH = PATHS[\"TEST_PATH\"],\n",
    ")\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002673,
     "end_time": "2025-06-03T09:47:08.520904",
     "exception": false,
     "start_time": "2025-06-03T09:47:08.518231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pre-processing / Feature Engineering\n",
    "\n",
    "**Pre-Processing**\n",
    "1. inf/-inf columns: `['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']`\n",
    "2. columns with NaN values: `[]`\n",
    "3. 0 std columns : `['X864', 'X867', 'X869', 'X870', 'X871', 'X872']`\n",
    "\n",
    "\n",
    "**Feature Engineering**\n",
    "1. `bidask_ratio`\n",
    "2. `buysell_ratio`\n",
    "3. `bidask_delta`\n",
    "4. `buysell_delta`\n",
    "5. `buysell_size`\n",
    "6. `bidask_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:40:44.036161Z",
     "iopub.status.busy": "2025-06-14T08:40:44.035859Z",
     "iopub.status.idle": "2025-06-14T08:40:44.049031Z",
     "shell.execute_reply": "2025-06-14T08:40:44.048352Z",
     "shell.execute_reply.started": "2025-06-14T08:40:44.036143Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_cols_inf(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names that contain any positive or negative infinity.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        # df[col] is a Series; .is_infinite() → Boolean Series; .any() → Python bool\n",
    "        try:\n",
    "            if df[col].is_infinite().any():\n",
    "                cols.append(col)\n",
    "        except Exception:\n",
    "            # if the column isn’t numeric, .is_infinite() might error—just skip it\n",
    "            continue\n",
    "    return cols\n",
    "\n",
    "def get_nan_columns(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names with any NaN/null values.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        if df.select(pl.col(col).is_null().any()).item():\n",
    "            cols.append(col)\n",
    "    return cols\n",
    "\n",
    "def get_cols_zerostd(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names whose standard deviation is zero\n",
    "    (or whose std returns None because all values are null).\n",
    "    Non-numeric columns (e.g. datetime) are skipped.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col, dtype in zip(df.columns, df.dtypes):\n",
    "        # Only attempt std() on numeric dtypes\n",
    "        if dtype.is_numeric():  \n",
    "            # df[col] is a Series; .std() returns a Python float or None\n",
    "            std_val = df[col].std()\n",
    "            if std_val == 0.0 or std_val is None:\n",
    "                cols.append(col)\n",
    "    return cols\n",
    "\n",
    "\n",
    "def feature_engineering(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Feature engineering\n",
    "    df = df.with_columns([\n",
    "        # bidask_ratio = bid_qty / ask_qty\n",
    "        (pl.col(\"bid_qty\") / pl.col(\"ask_qty\")).alias(\"bidask_ratio\"),\n",
    "\n",
    "        # buysell_ratio = 0 if volume == 0 else buy_qty / sell_qty\n",
    "        pl.when(pl.col(\"volume\") == 0)\n",
    "        .then(0)\n",
    "        .otherwise(pl.col(\"buy_qty\") / pl.col(\"sell_qty\"))\n",
    "        .alias(\"buysell_ratio\"),\n",
    "\n",
    "        # bidask_delta = bid_qty - ask_qty\n",
    "        (pl.col(\"bid_qty\") - pl.col(\"ask_qty\")).alias(\"bidask_delta\"),\n",
    "\n",
    "        # buysell_delta = buy_qty - sell_qty\n",
    "        (pl.col(\"buy_qty\") - pl.col(\"sell_qty\")).alias(\"buysell_delta\"),\n",
    "\n",
    "        # buysell_size = buy_qty + sell_qty\n",
    "        (pl.col(\"buy_qty\") + pl.col(\"sell_qty\")).alias(\"buysell_size\"),\n",
    "\n",
    "        # bidask_size = bid_qty + ask_qty\n",
    "        (pl.col(\"bid_qty\") + pl.col(\"ask_qty\")).alias(\"bidask_size\"),\n",
    "    ])\n",
    "    return df\n",
    "def preprocess_train(train: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Mirror of the original pandas workflow, but using polars.\n",
    "    1. Identify columns with infinite, NaN, or zero‐std and drop them.\n",
    "    2. Drop any user‐specified columns (e.g. label or order‐book columns).\n",
    "    3. (You can add normalized/scaling steps here if needed.)\n",
    "    \"\"\"\n",
    "    df = train.clone()\n",
    "\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    #### Preprocessing\n",
    "    cols_inf = get_cols_inf(df)\n",
    "    print(\"Columns with infinite values:\", cols_inf)\n",
    "\n",
    "    cols_nan = get_nan_columns(df)\n",
    "    print(\"Columns with NaN values:\", cols_nan)\n",
    "\n",
    "    cols_zerostd = get_cols_zerostd(df)\n",
    "    print(\"Columns with zero standard deviation:\", cols_zerostd)\n",
    "    # Drop columns with infinite, NaN, or zero‐std values\n",
    "    drop_columns = list(set(cols_inf) | set(cols_nan) | set(cols_zerostd) | set(columns_to_drop))\n",
    "    if drop_columns:\n",
    "        df = df.drop(drop_columns)\n",
    "    # df = df.sort(\"timestamp\", descending=False)\n",
    "    return df, drop_columns\n",
    "\n",
    "def preprocess_test(test: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n",
    "    df = test.clone()\n",
    "    df = feature_engineering(df)\n",
    "    df = df.drop(columns_to_drop)\n",
    "    print(\"Columns dropped from test set:\", columns_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Feature Selection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:54:00.392959Z",
     "iopub.status.busy": "2025-06-14T08:54:00.392252Z",
     "iopub.status.idle": "2025-06-14T08:54:00.450257Z",
     "shell.execute_reply": "2025-06-14T08:54:00.449621Z",
     "shell.execute_reply.started": "2025-06-14T08:54:00.392927Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import (\n",
    "    VarianceThreshold, SelectKBest, SelectPercentile,\n",
    "    GenericUnivariateSelect, SelectFpr, SelectFdr, SelectFwe,\n",
    "    RFE, RFECV, SelectFromModel, SequentialFeatureSelector,\n",
    "    f_regression, mutual_info_regression\n",
    ")\n",
    "from sklearn.decomposition import (\n",
    "    PCA, IncrementalPCA, TruncatedSVD,\n",
    "    FastICA, SparsePCA, MiniBatchSparsePCA,\n",
    "    DictionaryLearning, MiniBatchDictionaryLearning,\n",
    "    FactorAnalysis, NMF, LatentDirichletAllocation\n",
    ")\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm.auto import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import clone\n",
    "import time\n",
    "\n",
    "class SklearnFeatureEngineeringRegression:\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series, use_gpu: bool = False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Predictor matrix (n_samples × P features)\n",
    "        y : pd.Series\n",
    "            Target vector (n_samples,)\n",
    "        \"\"\"\n",
    "        self.X = X.copy()\n",
    "        self.y = y.copy()\n",
    "        self.X_np = X.to_numpy()\n",
    "        self.y_np = y.to_numpy()\n",
    "        self.features = X.columns.tolist()\n",
    "        self.results_df: pd.DataFrame = pd.DataFrame()\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        # … your import logic deciding CPU vs GPU …\n",
    "        if self.use_gpu:\n",
    "            print(\"*\" * 20 + \"GPU \" * 5 + \"*\" * 20)\n",
    "            import cupy as cp\n",
    "            self.X_np = cp.asarray(self.X_np)\n",
    "            self.y_np = cp.asarray(self.y_np)\n",
    "            self.tree_model = XGBRegressor(\n",
    "                tree_method =\"hist\", device=\"cuda\",\n",
    "                n_estimators=10, max_depth=10,\n",
    "                learning_rate=0.1, random_state=42, n_jobs=-1\n",
    "            )\n",
    "        else:\n",
    "            print(\"*\" * 20 + \"CPU \" * 5 + \"*\" * 20)\n",
    "            self.tree_model = XGBRegressor(\n",
    "                tree_method=\"hist\", n_estimators=10, max_depth=10,\n",
    "                learning_rate=0.05, random_state=42, n_jobs=-1\n",
    "            )\n",
    "        print(f\"\"\"SklearnFeatureEngineering Instantiated\"\"\")\n",
    "        print(f\"... Fitting {self.tree_model.__class__.__name__} on {self.X.shape[0]} rows and {self.X.shape[1]} features.\")\n",
    "        self.tree_model.fit(self.X_np, self.y_np)\n",
    "\n",
    "    # ---------- Filter methods ----------\n",
    "    # VarianceThreshold is a filter method that removes features with low variance.\n",
    "    def _variance_threshold(self, thresh: float = 0.0) -> pd.Series:\n",
    "        sel = VarianceThreshold(threshold=thresh).fit(self.X)\n",
    "        return pd.Series(sel.get_support(), index=self.features).astype(int)\n",
    "    # SelectKBest and SelectPercentile are filter methods that select features based on univariate statistical tests.\n",
    "    def _select_kbest_freg(self, k: int = 10) -> pd.Series:\n",
    "        sel = SelectKBest(score_func=f_regression, k=min(k, self.X.shape[1]))\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "    def _select_kbest_mutualinfo(self, k: int = 10) -> pd.Series:\n",
    "        sel = SelectKBest(score_func=mutual_info_regression, k=min(k, self.X.shape[1]))\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "    def _select_percentile_freg(self, p: float = 10) -> pd.Series:\n",
    "        sel = SelectPercentile(score_func=f_regression, percentile=min(p, 100))\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "    def _select_percentile_mutualinfo(self, p: float = 10) -> pd.Series:\n",
    "        sel = SelectPercentile(score_func=mutual_info_regression, percentile=min(p, 100))\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "    # # GenericUnivariateSelect is a filter method that allows for more flexible selection criteria.\n",
    "    # def _generic_univariate(self, k: int = 10) -> pd.Series:\n",
    "    #     sel = GenericUnivariateSelect(score_func=f_regression, mode='k_best', param=min(k, self.X.shape[1]))\n",
    "    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    def _select_fpr_freg(self, alpha: float = 0.05) -> pd.Series:\n",
    "        sel = SelectFpr(score_func=f_regression, alpha=alpha)\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    def _select_fdr_freg(self, alpha: float = 0.05) -> pd.Series:\n",
    "        sel = SelectFdr(score_func=f_regression, alpha=alpha)\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    def _select_fwe_freg(self, alpha: float = 0.05) -> pd.Series:\n",
    "        sel = SelectFwe(score_func=f_regression, alpha=alpha)\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    # ---------- Wrapper and Embedded methods ----------\n",
    "    # def _rfe_lasso(self, n_features: int = 10) -> pd.Series:\n",
    "    #     estimator = LassoCV(cv=5, max_iter=5000).fit(self.X, self.y)\n",
    "    #     sel = RFE(estimator, n_features_to_select=min(n_features, self.X.shape[1]))\n",
    "    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    # def _rfecv_lasso(self) -> pd.Series:\n",
    "    #     estimator = LassoCV(cv=5, max_iter=5000).fit(self.X, self.y)\n",
    "    #     sel = RFECV(estimator, cv=5)\n",
    "    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    def _rfe_xgb(self, n_features: int = 10) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Recursive Feature Elimination with a fresh clone of self.tree_model.\n",
    "        \"\"\"\n",
    "        # clone preserves GPU/CPU config + hyper‐parameters\n",
    "        estimator = clone(self.tree_model)\n",
    "        sel = RFE(estimator, n_features_to_select=min(n_features, self.X.shape[1]))\n",
    "        \n",
    "        X_fit = self.X_np.get() if self.use_gpu else self.X_np\n",
    "        y_fit = self.y_np.get() if self.use_gpu else self.y_np\n",
    "\n",
    "        mask = sel.fit(X_fit, y_fit).get_support()\n",
    "        return pd.Series(mask.astype(int), index=self.features).astype(int)\n",
    "\n",
    "    def _rfecv_xgb(self, cv: int = 5) -> pd.Series:\n",
    "        \"\"\"\n",
    "        RFECV with a fresh clone of self.tree_model.\n",
    "        \"\"\"\n",
    "        estimator = clone(self.tree_model)\n",
    "        sel = RFECV(estimator, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        \n",
    "        X_fit = self.X_np.get() if self.use_gpu else self.X_np\n",
    "        y_fit = self.y_np.get() if self.use_gpu else self.y_np\n",
    "        \n",
    "        mask = sel.fit(X_fit, y_fit).get_support()\n",
    "        return pd.Series(mask.astype(int), index=self.features).astype(int)\n",
    "    \n",
    "    def _select_from_model_tree(self, threshold=\"median\") -> pd.Series:\n",
    "        \"\"\"\n",
    "        Uses the pre‐fitted self.tree_model via prefit=True.\n",
    "        \"\"\"\n",
    "        sel = SelectFromModel(self.tree_model, threshold=threshold, prefit=True)\n",
    "        mask = sel.get_support()\n",
    "        return pd.Series(mask.astype(int), index=self.features)\n",
    "\n",
    "    def _sequential_tree(\n",
    "        self,\n",
    "        n_features: int = 10,\n",
    "        direction: str = \"forward\",\n",
    "        cv: int = 5,\n",
    "        n_jobs: int = -1\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        SequentialFeatureSelector with XGBRegressor.\n",
    "\n",
    "        - Clones self.tree_model to preserve GPU/CPU config.\n",
    "        - direction: 'forward' or 'backward'\n",
    "        - cv: number of cross‐validation folds\n",
    "        - n_jobs: parallel jobs for CV\n",
    "        \"\"\"\n",
    "        # ensure we don't modify the original fitted model\n",
    "        estimator = clone(self.tree_model)\n",
    "\n",
    "        sfs = SequentialFeatureSelector(\n",
    "            estimator,\n",
    "            n_features_to_select=min(n_features, self.X.shape[1]),\n",
    "            direction=direction,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        \n",
    "        X_fit = self.X_np.get() if self.use_gpu else self.X_np\n",
    "        y_fit = self.y_np.get() if self.use_gpu else self.y_np\n",
    "        \n",
    "        mask = sfs.fit(X_fit, y_fit).get_support()\n",
    "        return pd.Series(mask.astype(int), index=self.features)\n",
    "\n",
    "    # ---------- Decomposition methods ----------\n",
    "    def _pca(self, n_components: float = 10, svd_solver : str = \"covariance_eigh\") -> pd.Series:\n",
    "        pca = PCA(n_components=n_components).fit(self.X)\n",
    "        load = pd.DataFrame(pca.components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _incremental_pca(self, n_components: float = 10) -> pd.Series:\n",
    "        ipca = IncrementalPCA(n_components=n_components)\n",
    "        load = pd.DataFrame(ipca.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _truncated_svd(self, n_components: int = 10) -> pd.Series:\n",
    "        k = min(n_components, self.X.shape[1])\n",
    "        ts = TruncatedSVD(n_components=k)\n",
    "        load = pd.DataFrame(ts.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _fast_ica(self, n_components: int = 10) -> pd.Series:\n",
    "        ic = FastICA(n_components=min(n_components, self.X.shape[1]), max_iter=200)\n",
    "        load = pd.DataFrame(ic.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _sparse_pca(self, n_components: int = 10) -> pd.Series:\n",
    "        spca = SparsePCA(n_components=min(n_components, self.X.shape[1]), alpha=1, max_iter=1000)\n",
    "        load = pd.DataFrame(spca.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _minibatch_sparse_pca(self, n_components: int = 10) -> pd.Series:\n",
    "        mbspca = MiniBatchSparsePCA(n_components=min(n_components, self.X.shape[1]), alpha=1)\n",
    "        load = pd.DataFrame(mbspca.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _dict_learning(self, n_components: int = 10) -> pd.Series:\n",
    "        dl = DictionaryLearning(n_components=min(n_components, self.X.shape[1]), alpha=1, max_iter=1000)\n",
    "        load = pd.DataFrame(dl.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _minibatch_dict_learning(self, n_components: int = 10) -> pd.Series:\n",
    "        mbdl = MiniBatchDictionaryLearning(n_components=min(n_components, self.X.shape[1]), alpha=1)\n",
    "        load = pd.DataFrame(mbdl.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _factor_analysis(self, n_components: int = 10) -> pd.Series:\n",
    "        fa = FactorAnalysis(n_components=min(n_components, self.X.shape[1]))\n",
    "        load = pd.DataFrame(fa.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _nmf(self, n_components: int = 10) -> pd.Series:\n",
    "        nmf = NMF(n_components=min(n_components, self.X.shape[1]), init='nndsvda', max_iter=500)\n",
    "        data_pos = self.X.clip(lower=0)\n",
    "        load = pd.DataFrame(nmf.fit(data_pos).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _lda(self, n_components: int = 10) -> pd.Series:\n",
    "        lda = LatentDirichletAllocation(n_components=min(n_components, self.X.shape[1]), max_iter=5)\n",
    "        data_pos = self.X.clip(lower=0)\n",
    "        load = pd.DataFrame(lda.fit(data_pos).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    # ---- Random projections ----\n",
    "    def _gaussian_random_projection(self, n_components: int = 10) -> pd.Series:\n",
    "        rp = GaussianRandomProjection(n_components=min(n_components, self.X.shape[1]))\n",
    "        comp = pd.DataFrame(rp.fit(self.X).components_.T, index=self.features)\n",
    "        return self._decomp_mask(comp)\n",
    "\n",
    "    def _sparse_random_projection(self, n_components: int = 10) -> pd.Series:\n",
    "        srp = SparseRandomProjection(n_components=min(n_components, self.X.shape[1]))\n",
    "        comp = pd.DataFrame(srp.fit(self.X).components_.T, index=self.features)\n",
    "        return self._decomp_mask(comp)\n",
    "\n",
    "    # ---- Feature grouping ----\n",
    "    def _feature_agglomeration(self, n_clusters: int = 10) -> pd.Series:\n",
    "        # Fit agglomeration on the array\n",
    "        agg = FeatureAgglomeration(n_clusters=min(n_clusters, self.X.shape[1]))\n",
    "        agg.fit(self.X)\n",
    "        labels       = agg.labels_\n",
    "        cluster_data = agg.transform(self.X)\n",
    "\n",
    "        # Build a temporary DataFrame for cluster components\n",
    "        df_clust = pd.DataFrame(\n",
    "            cluster_data,\n",
    "            columns=[f\"clus_{i}\" for i in range(cluster_data.shape[1])],\n",
    "            index=self.X.index\n",
    "        )\n",
    "\n",
    "        # For each cluster, pick the feature with highest corr to its component\n",
    "        mask = pd.Series(0, index=self.features)\n",
    "        for j in range(cluster_data.shape[1]):\n",
    "            members = [f for f, l in zip(self.features, labels) if l == j]\n",
    "            corrs   = self.X[members].corrwith(df_clust.iloc[:, j]).abs()\n",
    "            mask[corrs.idxmax()] = 1\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def run(self, methods=None) -> None:\n",
    "        \"\"\"\n",
    "        Execute each selection/decomposition method with a progress bar,\n",
    "        accepting either:\n",
    "          - a dict mapping method names to parameter dicts, or\n",
    "          - a list of method names (no parameters).\n",
    "\n",
    "        Assembles `results_df` with columns ['model', features..., 'total_score'].\n",
    "        \"\"\"\n",
    "        # default methods with default params\n",
    "        default_list = [\n",
    "            '_variance_threshold',\n",
    "            '_select_kbest_freg','_select_percentile_freg',\n",
    "            '_select_kbest_mutualinfo','_select_percentile_mutualinfo',\n",
    "            '_select_fpr_freg','_select_fdr_freg','_select_fwe_freg',\n",
    "            '_rfe_xgb','_rfecv_xgb',\n",
    "            '_select_from_model_tree','_sequential_tree',\n",
    "            '_pca','_incremental_pca','_truncated_svd','_fast_ica',\n",
    "            '_sparse_pca','_minibatch_sparse_pca','_dict_learning',\n",
    "            '_minibatch_dict_learning','_factor_analysis','_nmf','_lda',\n",
    "            '_gaussian_random_projection','_sparse_random_projection',\n",
    "            '_feature_agglomeration'\n",
    "        ]\n",
    "        if methods is None:\n",
    "            methods = {m: {} for m in default_list}\n",
    "        elif isinstance(methods, list):\n",
    "            methods = {m: {} for m in methods}\n",
    "        elif not isinstance(methods, dict):\n",
    "            raise ValueError(\"`methods` must be None, list, or dict\")\n",
    "\n",
    "        records = []\n",
    "        for method_name, params in tqdm(methods.items(), desc='Running feature selection'):\n",
    "            try:\n",
    "                if not hasattr(self, method_name):\n",
    "                    raise KeyError(f\"Method {method_name} not found in class\")\n",
    "                print(f\"Running method: `{method_name.lstrip('_')}` with params: {params}\")\n",
    "                start = time.time()\n",
    "                fn = getattr(self, method_name)\n",
    "                mask = fn(**params)\n",
    "                records.append({\n",
    "                    'model': method_name.lstrip('_'),\n",
    "                    **mask.to_dict(),\n",
    "                    'total_score': int(mask.sum())\n",
    "                })\n",
    "                elapsed = time.time() - start\n",
    "                print(\n",
    "                    f\"Function `{method_name.lstrip('_')}` completed \"\n",
    "                    f\"in {elapsed/60:.2f} minutes.\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error running method `{method_name}`: {e}\")\n",
    "\n",
    "        self.results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    def get_results(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the raw selection matrix (n_models × (P + 2)).\"\"\"\n",
    "        return self.results_df\n",
    "\n",
    "    def get_top_features(self, N: int = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate across models (vote count = sum of 1’s per feature),\n",
    "        and return the top N feature names.\n",
    "        \"\"\"\n",
    "        N = N or self.X.shape[1]  # Default to all features if N is None\n",
    "        return pd.DataFrame(self.results_df.loc[:, self.features].sum().sort_values(ascending=False), columns=['score']).head(N).reset_index().rename(columns={\"index\" : \"feature\"})\n",
    "\n",
    "    def get_model_featuresincluded(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame with number of features included per model.\n",
    "        \"\"\"\n",
    "        model_features = self.results_df.set_index(\"model\").sum(axis=1).reset_index()\n",
    "        model_features.columns = [\"model\", \"features_included\"]\n",
    "        return model_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:40:45.235340Z",
     "iopub.status.busy": "2025-06-14T08:40:45.235025Z",
     "iopub.status.idle": "2025-06-14T08:40:45.478375Z",
     "shell.execute_reply": "2025-06-14T08:40:45.477633Z",
     "shell.execute_reply.started": "2025-06-14T08:40:45.235322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with infinite values: ['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']\n",
      "Columns with NaN values: []\n",
      "Columns with zero standard deviation: ['X864', 'X867', 'X869', 'X870', 'X871', 'X872']\n",
      "shape: (21_599, 871)\n",
      "┌─────────┬───────────┬───────────┬──────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ volume  ┆ X1        ┆ X2        ┆ X3       ┆ … ┆ bidask_del ┆ buysell_de ┆ buysell_s ┆ bidask_si │\n",
      "│ ---     ┆ ---       ┆ ---       ┆ ---      ┆   ┆ ta         ┆ lta        ┆ ize       ┆ ze        │\n",
      "│ f64     ┆ f64       ┆ f64       ┆ f64      ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
      "│         ┆           ┆           ┆          ┆   ┆ f64        ┆ f64        ┆ f64       ┆ f64       │\n",
      "╞═════════╪═══════════╪═══════════╪══════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ 181.331 ┆ -0.606128 ┆ -0.026132 ┆ 0.240236 ┆ … ┆ 1.125      ┆ -60.097    ┆ 181.331   ┆ 8.443     │\n",
      "│ 110.604 ┆ -0.613099 ┆ -0.03643  ┆ 0.218995 ┆ … ┆ -10.517    ┆ -47.308    ┆ 110.604   ┆ 10.901    │\n",
      "│ 70.052  ┆ -0.47534  ┆ 0.217352  ┆ 0.478561 ┆ … ┆ 3.957      ┆ 1.746      ┆ 70.052    ┆ 11.881    │\n",
      "│ 124.021 ┆ -0.51895  ┆ 0.118303  ┆ 0.378718 ┆ … ┆ -4.846     ┆ 28.543     ┆ 124.021   ┆ 13.68     │\n",
      "│ 131.966 ┆ -0.693    ┆ -0.208907 ┆ 0.02963  ┆ … ┆ 6.577      ┆ -5.672     ┆ 131.966   ┆ 11.149    │\n",
      "│ …       ┆ …         ┆ …         ┆ …        ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n",
      "│ 94.388  ┆ 0.020155  ┆ 0.076565  ┆ 0.228994 ┆ … ┆ -2.642     ┆ -16.314    ┆ 94.388    ┆ 10.968    │\n",
      "│ 177.372 ┆ 0.016262  ┆ 0.062527  ┆ 0.214072 ┆ … ┆ -1.768     ┆ 43.03      ┆ 177.372   ┆ 6.348     │\n",
      "│ 101.252 ┆ 0.045407  ┆ 0.109834  ┆ 0.263577 ┆ … ┆ 1.597      ┆ 39.746     ┆ 101.252   ┆ 8.877     │\n",
      "│ 74.56   ┆ 0.124783  ┆ 0.244168  ┆ 0.408704 ┆ … ┆ 0.83       ┆ -29.83     ┆ 74.56     ┆ 10.632    │\n",
      "│ 303.722 ┆ 0.368659  ┆ 0.665382  ┆ 0.867538 ┆ … ┆ 0.06       ┆ -130.552   ┆ 303.722   ┆ 7.79      │\n",
      "└─────────┴───────────┴───────────┴──────────┴───┴────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "train_data_filtered = train_data.filter(\n",
    "    pl.col(\"timestamp\") >= train_splits[\"last_2w\"]\n",
    ")\n",
    "\n",
    "y = train_data_filtered[\"label\"]\n",
    "X, drop_columns = preprocess_train(\n",
    "    train_data_filtered,\n",
    "    columns_to_drop=[\"label\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n",
    ")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T08:54:03.140164Z",
     "iopub.status.busy": "2025-06-14T08:54:03.139921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************CPU CPU CPU CPU CPU ********************\n",
      "SklearnFeatureEngineering Instantiated\n",
      "... Fitting XGBRegressor on 21599 rows and 870 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66c5ff6be34547688fc0250cd7b5f3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running feature selection:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function `pca` completed in 0.01 minutes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cpu_methods = {\n",
    "    '_variance_threshold':             {'thresh': 1e-3},\n",
    "    '_select_kbest_freg':              {'k': 30},\n",
    "    '_select_kbest_mutualinfo':        {'k': 30},\n",
    "    '_select_percentile_freg':         {'p': 10},\n",
    "    '_select_percentile_mutualinfo':   {'p': 10},\n",
    "    '_select_fpr_freg':                {'alpha': 0.05},\n",
    "    '_select_fdr_freg':                {'alpha': 0.05},\n",
    "    '_select_fwe_freg':                {'alpha': 0.05},\n",
    "    '_pca':                            {'n_components': 0.95},\n",
    "    '_incremental_pca':                {'n_components': 87},\n",
    "    '_truncated_svd':                  {'n_components': 87},\n",
    "    '_fast_ica':                       {'n_components': 87},\n",
    "    '_sparse_pca':                     {'n_components': 87},\n",
    "    '_minibatch_sparse_pca':           {'n_components': 87},\n",
    "    '_dict_learning':                  {'n_components': 87},\n",
    "    '_minibatch_dict_learning':        {'n_components': 87},\n",
    "    '_factor_analysis':                {'n_components': 87},\n",
    "    '_nmf':                            {'n_components': 50},\n",
    "    '_lda':                            {'n_components': 50},\n",
    "    '_gaussian_random_projection':     {'n_components': 174},\n",
    "    '_sparse_random_projection':       {'n_components': 174},\n",
    "    '_feature_agglomeration':          {'n_clusters': 30},\n",
    "}\n",
    "\n",
    "gpu_methods = {\n",
    "    '_rfe_xgb':                        {'n_features': 30},\n",
    "    '_rfecv_xgb':                      {'cv': 10},\n",
    "    '_select_from_model_tree':         {'threshold': 'median'},\n",
    "    '_sequential_tree':                {\n",
    "        'n_features': 30,\n",
    "        'direction': 'forward',\n",
    "        'cv': 10,\n",
    "        'n_jobs': -1\n",
    "    },\n",
    "}\n",
    "\n",
    "fe_regression = SklearnFeatureEngineeringRegression(\n",
    "    X = X.drop([\"timestamp\"]).to_pandas(),\n",
    "    y = y.to_pandas(),\n",
    "    use_gpu=False  # Set to True if you want to use GPU\n",
    ")\n",
    "fe_regression.run(methods=cpu_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T08:43:34.547871Z",
     "iopub.status.idle": "2025-06-14T08:43:34.548183Z",
     "shell.execute_reply": "2025-06-14T08:43:34.548041Z",
     "shell.execute_reply.started": "2025-06-14T08:43:34.548027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if os.environ.get(\"USER\"):\n",
    "    fe_regression.results_df.to_excel(\"./kaggle/working/feature_engineering_results.xlsx\", index = False)\n",
    "    fe_regression.get_top_features().to_excel(\n",
    "        \"./kaggle/working/top_features.xlsx\", index = True\n",
    "    )\n",
    "else:\n",
    "    fe_regression.results_df.to_excel(\"/kaggle/working/feature_engineering_results.xlsx\", index=False)\n",
    "    fe_regression.get_top_features().to_excel(\n",
    "        \"/kaggle/working/top_features.xlsx\", index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T08:43:34.549711Z",
     "iopub.status.idle": "2025-06-14T08:43:34.549959Z",
     "shell.execute_reply": "2025-06-14T08:43:34.549862Z",
     "shell.execute_reply.started": "2025-06-14T08:43:34.549852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>volume</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>X581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>X557</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>X558</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>X559</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>X618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>X620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>X621</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>X623</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>X642</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>870 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature  score\n",
       "0    volume      1\n",
       "1      X581      1\n",
       "2      X557      1\n",
       "3      X558      1\n",
       "4      X559      1\n",
       "..      ...    ...\n",
       "865    X618      0\n",
       "866    X620      0\n",
       "867    X621      0\n",
       "868    X623      0\n",
       "869    X642      0\n",
       "\n",
       "[870 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_regression.get_top_features().reset_index().rename(columns={\"index\" : \"feature\"})"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11418275,
     "sourceId": 96164,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 773.161209,
   "end_time": "2025-06-03T09:59:54.014100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-03T09:47:00.852891",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":96164,"databundleVersionId":11418275,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":773.161209,"end_time":"2025-06-03T09:59:54.014100","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-03T09:47:00.852891","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2025-06-09T12:35:36.013729Z","iopub.execute_input":"2025-06-09T12:35:36.013955Z"},"papermill":{"duration":0.016021,"end_time":"2025-06-03T09:47:05.915432","exception":false,"start_time":"2025-06-03T09:47:05.899411","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\nfrom scipy.stats import pearsonr, spearmanr\nimport polars as pl\nimport numpy as np\nfrom tqdm import tqdm\n\ndef get_cols_inf(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names that contain any positive or negative infinity.\n    \"\"\"\n    cols = []\n    for col in df.columns:\n        # df[col] is a Series; .is_infinite() → Boolean Series; .any() → Python bool\n        try:\n            if df[col].is_infinite().any():\n                cols.append(col)\n        except Exception:\n            # if the column isn’t numeric, .is_infinite() might error—just skip it\n            continue\n    return cols\n\ndef get_nan_columns(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names with any NaN/null values.\n    \"\"\"\n    cols = []\n    for col in df.columns:\n        if df.select(pl.col(col).is_null().any()).item():\n            cols.append(col)\n    return cols\n\ndef get_cols_zerostd(df: pl.DataFrame) -> list[str]:\n    \"\"\"\n    Returns a list of column names whose standard deviation is zero\n    (or whose std returns None because all values are null).\n    Non-numeric columns (e.g. datetime) are skipped.\n    \"\"\"\n    cols = []\n    for col, dtype in zip(df.columns, df.dtypes):\n        # Only attempt std() on numeric dtypes\n        if dtype.is_numeric():  \n            # df[col] is a Series; .std() returns a Python float or None\n            std_val = df[col].std()\n            if std_val == 0.0 or std_val is None:\n                cols.append(col)\n    return cols\n\n\ndef feature_engineering(df: pl.DataFrame) -> pl.DataFrame:\n    # Feature engineering\n    df = df.with_columns([\n        # bidask_ratio = bid_qty / ask_qty\n        (pl.col(\"bid_qty\") / pl.col(\"ask_qty\")).alias(\"bidask_ratio\"),\n\n        # buysell_ratio = 0 if volume == 0 else buy_qty / sell_qty\n        pl.when(pl.col(\"volume\") == 0)\n        .then(0)\n        .otherwise(pl.col(\"buy_qty\") / pl.col(\"sell_qty\"))\n        .alias(\"buysell_ratio\"),\n\n        # bidask_delta = bid_qty - ask_qty\n        (pl.col(\"bid_qty\") - pl.col(\"ask_qty\")).alias(\"bidask_delta\"),\n\n        # buysell_delta = buy_qty - sell_qty\n        (pl.col(\"buy_qty\") - pl.col(\"sell_qty\")).alias(\"buysell_delta\"),\n\n        # buysell_size = buy_qty + sell_qty\n        (pl.col(\"buy_qty\") + pl.col(\"sell_qty\")).alias(\"buysell_size\"),\n\n        # bidask_size = bid_qty + ask_qty\n        (pl.col(\"bid_qty\") + pl.col(\"ask_qty\")).alias(\"bidask_size\"),\n    ])\n    return df\ndef preprocess_train(train: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n    \"\"\"\n    Mirror of the original pandas workflow, but using polars.\n    1. Identify columns with infinite, NaN, or zero‐std and drop them.\n    2. Drop any user‐specified columns (e.g. label or order‐book columns).\n    3. (You can add normalized/scaling steps here if needed.)\n    \"\"\"\n    df = train.clone()\n\n    df = feature_engineering(df)\n    \n    #### Preprocessing\n    cols_inf = get_cols_inf(df)\n    print(\"Columns with infinite values:\", cols_inf)\n\n    cols_nan = get_nan_columns(df)\n    print(\"Columns with NaN values:\", cols_nan)\n\n    cols_zerostd = get_cols_zerostd(df)\n    print(\"Columns with zero standard deviation:\", cols_zerostd)\n    # Drop columns with infinite, NaN, or zero‐std values\n    drop_columns = list(set(cols_inf) | set(cols_nan) | set(cols_zerostd) | set(columns_to_drop))\n    if drop_columns:\n        df = df.drop(drop_columns)\n    # df = df.sort(\"timestamp\", descending=False)\n    return df, drop_columns\n\ndef preprocess_test(test: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n    df = test.clone()\n    df = feature_engineering(df)\n    df = df.drop(columns_to_drop)\n    print(\"Columns dropped from test set:\", columns_to_drop)\n    return df","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":2.595897,"end_time":"2025-06-03T09:47:08.514965","exception":false,"start_time":"2025-06-03T09:47:05.919068","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data","metadata":{"papermill":{"duration":0.002673,"end_time":"2025-06-03T09:47:08.520904","exception":false,"start_time":"2025-06-03T09:47:08.518231","status":"completed"},"tags":[]}},{"cell_type":"code","source":"data = pl.read_parquet(\n    \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n)\n# data = pl.read_parquet(\n#     source = \"./data/train.parquet\",\n# )\n\ny = data[\"label\"]\nX, drop_columns = preprocess_train(\n    data,\n    columns_to_drop=[\"label\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n)\nX","metadata":{"papermill":{"duration":15.905967,"end_time":"2025-06-03T09:47:24.429666","exception":false,"start_time":"2025-06-03T09:47:08.523699","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"papermill":{"duration":0.003008,"end_time":"2025-06-03T09:47:24.436311","exception":false,"start_time":"2025-06-03T09:47:24.433303","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Define your date range as Python datetime objects:\nfrom datetime import datetime\nstart = datetime(2023, 3, 1, 0, 0)\nend   = datetime(2024, 2, 29, 23, 59, 59)\n\nfeatures = ['X35', 'X96', 'X113', 'X126', 'X261', 'X539', 'X666', 'X690', 'X696']\n\n# === filter on the timestamp column ===\n# Assume “ts” is the datetime column in X.\nX_period = X.filter(\n    (pl.col(\"timestamp\") >= pl.lit(start)) & (pl.col(\"timestamp\") <= pl.lit(end))\n).select([\"timestamp\"] + features)\n\n# Extract y_period likewise:\n# In polars, “y” is still a column expression, so filter the original `data`:\ny_period = data.filter(\n    (pl.col(\"timestamp\") >= pl.lit(start)) & (pl.col(\"timestamp\") <= pl.lit(end))\n)[\"label\"]  # This returns a polars Series.\n\n# If you need numpy for scikit‐learn, convert now:\nX_np = X_period.drop([\"timestamp\"]).to_numpy()\ny_np = y_period.to_numpy()","metadata":{"papermill":{"duration":2.138489,"end_time":"2025-06-03T09:47:26.577968","exception":false,"start_time":"2025-06-03T09:47:24.439479","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{"papermill":{"duration":0.005394,"end_time":"2025-06-03T09:47:26.589282","exception":false,"start_time":"2025-06-03T09:47:26.583888","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## LightGBM","metadata":{}},{"cell_type":"code","source":"# from lightgbm import LGBMRegressor\n\n# def fit_lightgbm_regression(\n#     X: np.ndarray,\n#     y: np.ndarray,\n#     n_estimators: int = 100,\n#     learning_rate: float = 0.1,\n#     num_leaves: int = 31,\n#     **kwargs\n# ) -> LGBMRegressor:\n#     \"\"\"\n#     X and y must be NumPy arrays. Since polars DataFrames are not\n#     directly accepted by scikit‐learn/lightgbm, we pass .to_numpy().\n#     \"\"\"\n#     model = LGBMRegressor(\n#         n_estimators=n_estimators,\n#         learning_rate=learning_rate,\n#         num_leaves=num_leaves,\n#         **kwargs\n#     )\n#     model.fit(X, y)\n#     return model\n\n# model = fit_lightgbm_regression(\n#     X_np,\n#     y_np,\n#     # 1. Learning rate: much lower than 0.3 to allow gradual fitting.\n#     learning_rate=0.05,\n\n#     # 2. Number of trees: increase so that η·T is roughly O(50–100) in practice.\n#     #    Here, 1000 trees × 0.05 = 50 “effective steps” of gradient boosting.\n#     n_estimators=1000,  \n\n#     # 3. num_leaves: controls maximum number of terminal nodes per tree.\n#     #    A rule of thumb is ~2^(max_depth). For dataset with moderate complexity,\n#     #    num_leaves=64 (≈2^6) is common; if features are very noisy, reduce it.\n#     num_leaves=64,      \n\n#     # 4. max_depth: optional cap on tree depth—keeps each tree from growing too deep.\n#     #    If you set max_depth=10, then num_leaves is effectively ≤ 2^10, but\n#     #    most practitioners leave max_depth unset when they tune num_leaves directly.\n#     max_depth=10,        \n\n#     # 5. min_data_in_leaf (min_child_samples): ensures a leaf has enough observations.\n#     #    E.g., if you have 100 k rows total, min_data_in_leaf=20 or 50 prevents overfitting.\n#     min_data_in_leaf=20, \n\n#     # 6. subsample (a.k.a. bagging_fraction): to reduce variance, randomly sample rows.\n#     #    0.8 means each tree sees 80 % of data. Coupled with subsample_freq=1 (every tree).\n#     subsample=0.8,       \n#     subsample_freq=1,    \n\n#     # 7. colsample_bytree (a.k.a. feature_fraction): randomly sample 80 % of features per tree.\n#     colsample_bytree=0.8,\n\n#     # 8. Regularization: L1 or L2 to further guard against overfitting.\n#     reg_alpha=0.1,   # L1 regularization\n#     reg_lambda=1.0,  # L2 regularization\n\n#     # 9. Other sensible defaults:\n#     n_jobs=-1,\n#     random_state=42,\n#     verbosity=1,\n# )\n\n# model = fit_lightgbm_regression(\n#     X_np,\n#     y_np,\n#     learning_rate=0.05,\n#     n_estimators=1000,  \n#     num_leaves=64,      \n#     max_depth=10,        \n#     min_data_in_leaf=20, \n#     subsample=0.8,       \n#     subsample_freq=1,\n#     colsample_bytree=0.8,\n#     reg_alpha=0.1,   # L1 regularization\n#     reg_lambda=1.0,  # L2 regularization\n\n#     n_jobs=-1,\n#     random_state=42,\n#     verbosity=1,\n    # device='gpu',             # enable CUDA\n# )","metadata":{"papermill":{"duration":697.031203,"end_time":"2025-06-03T09:59:03.623836","exception":false,"start_time":"2025-06-03T09:47:26.592633","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor(\n    n_estimators=10000,  # Number of trees\n    max_depth=10,       # Maximum depth of each tree\n    min_samples_split=5,  # Minimum samples required to split an internal node\n    min_samples_leaf=2,   # Minimum samples required to be at a leaf node\n    max_features='sqrt',   # Use square root of features for splitting\n    random_state=42,       # For reproducibility\n    n_jobs=-1              # Use all available cores\n)\nmodel.fit(X_np, y_np)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict","metadata":{"papermill":{"duration":0.003528,"end_time":"2025-06-03T09:59:03.633280","exception":false,"start_time":"2025-06-03T09:59:03.629752","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test = pl.read_parquet(\"/kaggle/input/drw-crypto-market-prediction/test.parquet\")\n# test = pl.read_parquet(\n#     source = \"./data/test.parquet\",\n# )\ntest = test.with_row_index(\"ID\", offset=1)\nX_test = preprocess_test(test, columns_to_drop=drop_columns).drop([\"ID\"])\nX_test = X_test.select(features)\nX_test","metadata":{"papermill":{"duration":14.950757,"end_time":"2025-06-03T09:59:18.589823","exception":false,"start_time":"2025-06-03T09:59:03.639066","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_pred = model.predict(X_test.to_numpy())\nsubmission = pl.DataFrame({\n    \"ID\": test[\"ID\"],\n    \"prediction\": y_pred\n})\nsubmission","metadata":{"papermill":{"duration":31.685641,"end_time":"2025-06-03T09:59:50.279720","exception":false,"start_time":"2025-06-03T09:59:18.594079","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission\n\nsample_submission.csv\n- ID\n- prediction","metadata":{"papermill":{"duration":0.003669,"end_time":"2025-06-03T09:59:50.287389","exception":false,"start_time":"2025-06-03T09:59:50.283720","status":"completed"},"tags":[]}},{"cell_type":"code","source":"submission.write_csv(\"/kaggle/working/submission.csv\")","metadata":{"papermill":{"duration":0.124426,"end_time":"2025-06-03T09:59:50.415662","exception":false,"start_time":"2025-06-03T09:59:50.291236","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}
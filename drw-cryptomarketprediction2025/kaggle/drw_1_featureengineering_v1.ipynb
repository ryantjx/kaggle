{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T03:25:26.352182Z",
     "iopub.status.busy": "2025-06-14T03:25:26.351500Z",
     "iopub.status.idle": "2025-06-14T03:25:26.359121Z",
     "shell.execute_reply": "2025-06-14T03:25:26.358216Z",
     "shell.execute_reply.started": "2025-06-14T03:25:26.352154Z"
    },
    "papermill": {
     "duration": 0.016021,
     "end_time": "2025-06-03T09:47:05.915432",
     "exception": false,
     "start_time": "2025-06-03T09:47:05.899411",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_results_to_json(results, out_path):\n",
    "    \"\"\"Save results to JSON file, appending if file exists\"\"\"\n",
    "    \n",
    "    if os.path.exists(out_path):\n",
    "        try:\n",
    "            # Load existing data\n",
    "            with open(out_path, 'r') as f:\n",
    "                existing_data = json.load(f)\n",
    "            \n",
    "            # Ensure existing_data is a list\n",
    "            if not isinstance(existing_data, list):\n",
    "                existing_data = [existing_data]\n",
    "            \n",
    "            # Append new results\n",
    "            existing_data.extend(results)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            # If existing file is corrupted, start fresh\n",
    "            print(\"Warning: Existing JSON file corrupted, starting fresh\")\n",
    "            existing_data = results\n",
    "        \n",
    "        # Save combined data\n",
    "        with open(out_path, 'w') as f:\n",
    "            json.dump(existing_data, f, indent=2)\n",
    "    else:\n",
    "        # Create new file\n",
    "        with open(out_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/ryant/Documents/Github/kaggle-challenges/drw-cryptomarketprediction2025/kaggle'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv('user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRW - Crypto Market Prediction\n",
    "\n",
    "This notebook documents all the steps done in this project.\n",
    "\n",
    "Timeline:\n",
    "- 10/06/25: 0.05031\n",
    "    - Reorganize notebooks.\n",
    "    - Test training with GPU - Way faster than CPU.\n",
    "    - Implement feature elimination using GPU.\n",
    "    - Tested with Linear Models - will be extremely slow in iteration.\n",
    "    - Develop feature engineering pipeline\n",
    "- 14/06/25\n",
    "    - redevelop feature engineering pipeline - pipe results into downloadable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-14T03:31:56.599777Z",
     "iopub.status.busy": "2025-06-14T03:31:56.599412Z",
     "iopub.status.idle": "2025-06-14T03:32:10.475548Z",
     "shell.execute_reply": "2025-06-14T03:32:10.474594Z",
     "shell.execute_reply.started": "2025-06-14T03:31:56.599754Z"
    },
    "papermill": {
     "duration": 2.595897,
     "end_time": "2025-06-03T09:47:08.514965",
     "exception": false,
     "start_time": "2025-06-03T09:47:05.919068",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10, 897)\n",
      "┌─────────┬─────────┬─────────┬──────────┬───┬──────────┬──────────┬───────────┬──────────────┐\n",
      "│ bid_qty ┆ ask_qty ┆ buy_qty ┆ sell_qty ┆ … ┆ X889     ┆ X890     ┆ label     ┆ timestamp    │\n",
      "│ ---     ┆ ---     ┆ ---     ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---       ┆ ---          │\n",
      "│ f64     ┆ f64     ┆ f64     ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64       ┆ datetime[ns] │\n",
      "╞═════════╪═════════╪═════════╪══════════╪═══╪══════════╪══════════╪═══════════╪══════════════╡\n",
      "│ 7.739   ┆ 2.904   ┆ 22.151  ┆ 53.889   ┆ … ┆ 0.137442 ┆ 0.244015 ┆ -0.166322 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:50:00     │\n",
      "│ 3.304   ┆ 6.292   ┆ 18.19   ┆ 104.161  ┆ … ┆ 0.137251 ┆ 0.243846 ┆ 0.02131   ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:51:00     │\n",
      "│ 3.718   ┆ 3.63    ┆ 54.177  ┆ 77.472   ┆ … ┆ 0.137061 ┆ 0.243677 ┆ 0.096966  ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:52:00     │\n",
      "│ 11.419  ┆ 1.506   ┆ 32.794  ┆ 129.55   ┆ … ┆ 0.136871 ┆ 0.243508 ┆ 0.271085  ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:53:00     │\n",
      "│ 7.353   ┆ 0.566   ┆ 70.766  ┆ 55.178   ┆ … ┆ 0.136683 ┆ 0.243341 ┆ 0.372164  ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:54:00     │\n",
      "│ 4.163   ┆ 6.805   ┆ 39.037  ┆ 55.351   ┆ … ┆ 0.136494 ┆ 0.243172 ┆ 0.396289  ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:55:00     │\n",
      "│ 2.29    ┆ 4.058   ┆ 110.201 ┆ 67.171   ┆ … ┆ 0.136305 ┆ 0.243004 ┆ 0.328993  ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:56:00     │\n",
      "│ 5.237   ┆ 3.64    ┆ 70.499  ┆ 30.753   ┆ … ┆ 0.136117 ┆ 0.242836 ┆ 0.189909  ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:57:00     │\n",
      "│ 5.731   ┆ 4.901   ┆ 22.365  ┆ 52.195   ┆ … ┆ 0.135928 ┆ 0.242668 ┆ 0.410831  ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:58:00     │\n",
      "│ 3.925   ┆ 3.865   ┆ 86.585  ┆ 217.137  ┆ … ┆ 0.135741 ┆ 0.242501 ┆ 0.731542  ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆           ┆ 23:59:00     │\n",
      "└─────────┴─────────┴─────────┴──────────┴───┴──────────┴──────────┴───────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "train_splits = {\n",
    "    \"full\" : pl.datetime(2023, 3, 1, 0, 0, 0),\n",
    "    \"last_12m\" : pl.datetime(2023, 6, 1, 0, 0, 0),\n",
    "    \"last_9m\" : pl.datetime(2023, 9, 1, 0, 0, 0),\n",
    "    \"last_3m\" : pl.datetime(2024, 2, 1, 0, 0, 0),\n",
    "    \"last_6m\": pl.datetime(2024, 12, 1, 0, 0, 0),\n",
    "}\n",
    "\n",
    "PATHS = {\n",
    "    \"TRAIN_PATH\" :\"/kaggle/input/drw-crypto-market-prediction/train.parquet\",\n",
    "    \"TEST_PATH\" : \"/kaggle/input/drw-crypto-market-prediction/test.parquet\",\n",
    "    \"SUBMISSION_PATH\" : \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\",\n",
    "}\n",
    "\n",
    "# features = []\n",
    "\n",
    "def load_data(TRAIN_PATH: str, TEST_PATH: str):\n",
    "    if not os.getenv(key='user'):\n",
    "        TRAIN_PATH = \".\" + TRAIN_PATH\n",
    "        TEST_PATH = \".\" + TEST_PATH\n",
    "    train_data = pl.read_parquet(TRAIN_PATH).sort(\"timestamp\", descending = False)\n",
    "    test_data = pl.read_parquet(TEST_PATH)\n",
    "    # print(f\"Train data shape: {train_data.shape}\")\n",
    "    # print(f\"Test data shape: {test_data.shape}\")\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = load_data(\n",
    "    TRAIN_PATH = PATHS[\"TRAIN_PATH\"],\n",
    "    TEST_PATH = PATHS[\"TEST_PATH\"],\n",
    ")\n",
    "print(train_data.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.002673,
     "end_time": "2025-06-03T09:47:08.520904",
     "exception": false,
     "start_time": "2025-06-03T09:47:08.518231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pre-processing / Feature Engineering\n",
    "\n",
    "**Pre-Processing**\n",
    "1. inf/-inf columns: `['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']`\n",
    "2. columns with NaN values: `[]`\n",
    "3. 0 std columns : `['X864', 'X867', 'X869', 'X870', 'X871', 'X872']`\n",
    "\n",
    "\n",
    "**Feature Engineering**\n",
    "1. `bidask_ratio`\n",
    "2. `buysell_ratio`\n",
    "3. `bidask_delta`\n",
    "4. `buysell_delta`\n",
    "5. `buysell_size`\n",
    "6. `bidask_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:45:45.215015Z",
     "iopub.status.busy": "2025-06-10T10:45:45.214698Z",
     "iopub.status.idle": "2025-06-10T10:45:45.225691Z",
     "shell.execute_reply": "2025-06-10T10:45:45.225020Z",
     "shell.execute_reply.started": "2025-06-10T10:45:45.214969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_cols_inf(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names that contain any positive or negative infinity.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        # df[col] is a Series; .is_infinite() → Boolean Series; .any() → Python bool\n",
    "        try:\n",
    "            if df[col].is_infinite().any():\n",
    "                cols.append(col)\n",
    "        except Exception:\n",
    "            # if the column isn’t numeric, .is_infinite() might error—just skip it\n",
    "            continue\n",
    "    return cols\n",
    "\n",
    "def get_nan_columns(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names with any NaN/null values.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        if df.select(pl.col(col).is_null().any()).item():\n",
    "            cols.append(col)\n",
    "    return cols\n",
    "\n",
    "def get_cols_zerostd(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names whose standard deviation is zero\n",
    "    (or whose std returns None because all values are null).\n",
    "    Non-numeric columns (e.g. datetime) are skipped.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col, dtype in zip(df.columns, df.dtypes):\n",
    "        # Only attempt std() on numeric dtypes\n",
    "        if dtype.is_numeric():  \n",
    "            # df[col] is a Series; .std() returns a Python float or None\n",
    "            std_val = df[col].std()\n",
    "            if std_val == 0.0 or std_val is None:\n",
    "                cols.append(col)\n",
    "    return cols\n",
    "\n",
    "\n",
    "def feature_engineering(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Feature engineering\n",
    "    df = df.with_columns([\n",
    "        # bidask_ratio = bid_qty / ask_qty\n",
    "        (pl.col(\"bid_qty\") / pl.col(\"ask_qty\")).alias(\"bidask_ratio\"),\n",
    "\n",
    "        # buysell_ratio = 0 if volume == 0 else buy_qty / sell_qty\n",
    "        pl.when(pl.col(\"volume\") == 0)\n",
    "        .then(0)\n",
    "        .otherwise(pl.col(\"buy_qty\") / pl.col(\"sell_qty\"))\n",
    "        .alias(\"buysell_ratio\"),\n",
    "\n",
    "        # bidask_delta = bid_qty - ask_qty\n",
    "        (pl.col(\"bid_qty\") - pl.col(\"ask_qty\")).alias(\"bidask_delta\"),\n",
    "\n",
    "        # buysell_delta = buy_qty - sell_qty\n",
    "        (pl.col(\"buy_qty\") - pl.col(\"sell_qty\")).alias(\"buysell_delta\"),\n",
    "\n",
    "        # buysell_size = buy_qty + sell_qty\n",
    "        (pl.col(\"buy_qty\") + pl.col(\"sell_qty\")).alias(\"buysell_size\"),\n",
    "\n",
    "        # bidask_size = bid_qty + ask_qty\n",
    "        (pl.col(\"bid_qty\") + pl.col(\"ask_qty\")).alias(\"bidask_size\"),\n",
    "    ])\n",
    "    return df\n",
    "def preprocess_train(train: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Mirror of the original pandas workflow, but using polars.\n",
    "    1. Identify columns with infinite, NaN, or zero‐std and drop them.\n",
    "    2. Drop any user‐specified columns (e.g. label or order‐book columns).\n",
    "    3. (You can add normalized/scaling steps here if needed.)\n",
    "    \"\"\"\n",
    "    df = train.clone()\n",
    "\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    #### Preprocessing\n",
    "    cols_inf = get_cols_inf(df)\n",
    "    print(\"Columns with infinite values:\", cols_inf)\n",
    "\n",
    "    cols_nan = get_nan_columns(df)\n",
    "    print(\"Columns with NaN values:\", cols_nan)\n",
    "\n",
    "    cols_zerostd = get_cols_zerostd(df)\n",
    "    print(\"Columns with zero standard deviation:\", cols_zerostd)\n",
    "    # Drop columns with infinite, NaN, or zero‐std values\n",
    "    drop_columns = list(set(cols_inf) | set(cols_nan) | set(cols_zerostd) | set(columns_to_drop))\n",
    "    if drop_columns:\n",
    "        df = df.drop(drop_columns)\n",
    "    # df = df.sort(\"timestamp\", descending=False)\n",
    "    return df, drop_columns\n",
    "\n",
    "def preprocess_test(test: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n",
    "    df = test.clone()\n",
    "    df = feature_engineering(df)\n",
    "    df = df.drop(columns_to_drop)\n",
    "    print(\"Columns dropped from test set:\", columns_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:45:45.227439Z",
     "iopub.status.busy": "2025-06-10T10:45:45.227227Z",
     "iopub.status.idle": "2025-06-10T10:45:47.095667Z",
     "shell.execute_reply": "2025-06-10T10:45:47.095071Z",
     "shell.execute_reply.started": "2025-06-10T10:45:45.227417Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with infinite values: ['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']\n",
      "Columns with NaN values: []\n",
      "Columns with zero standard deviation: ['X864', 'X867', 'X869', 'X870', 'X871', 'X872']\n",
      "shape: (525_887, 871)\n",
      "┌─────────┬──────────┬───────────┬───────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ volume  ┆ X1       ┆ X2        ┆ X3        ┆ … ┆ bidask_del ┆ buysell_de ┆ buysell_s ┆ bidask_si │\n",
      "│ ---     ┆ ---      ┆ ---       ┆ ---       ┆   ┆ ta         ┆ lta        ┆ ize       ┆ ze        │\n",
      "│ f64     ┆ f64      ┆ f64       ┆ f64       ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
      "│         ┆          ┆           ┆           ┆   ┆ f64        ┆ f64        ┆ f64       ┆ f64       │\n",
      "╞═════════╪══════════╪═══════════╪═══════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ 221.389 ┆ 0.121263 ┆ -0.41769  ┆ 0.005399  ┆ … ┆ 6.858      ┆ 131.421    ┆ 221.389   ┆ 23.708    │\n",
      "│ 847.796 ┆ 0.302841 ┆ -0.049576 ┆ 0.356667  ┆ … ┆ 36.254     ┆ 203.896    ┆ 847.796   ┆ 40.926    │\n",
      "│ 295.596 ┆ 0.167462 ┆ -0.291212 ┆ 0.083138  ┆ … ┆ -59.808    ┆ 22.858     ┆ 295.596   ┆ 60.692    │\n",
      "│ 460.705 ┆ 0.072944 ┆ -0.43659  ┆ -0.102483 ┆ … ┆ -16.151    ┆ 210.779    ┆ 460.705   ┆ 25.881    │\n",
      "│ 142.818 ┆ 0.17382  ┆ -0.213489 ┆ 0.096067  ┆ … ┆ 23.707     ┆ 54.004     ┆ 142.818   ┆ 30.609    │\n",
      "│ …       ┆ …        ┆ …         ┆ …         ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n",
      "│ 94.388  ┆ 0.020155 ┆ 0.076565  ┆ 0.228994  ┆ … ┆ -2.642     ┆ -16.314    ┆ 94.388    ┆ 10.968    │\n",
      "│ 177.372 ┆ 0.016262 ┆ 0.062527  ┆ 0.214072  ┆ … ┆ -1.768     ┆ 43.03      ┆ 177.372   ┆ 6.348     │\n",
      "│ 101.252 ┆ 0.045407 ┆ 0.109834  ┆ 0.263577  ┆ … ┆ 1.597      ┆ 39.746     ┆ 101.252   ┆ 8.877     │\n",
      "│ 74.56   ┆ 0.124783 ┆ 0.244168  ┆ 0.408704  ┆ … ┆ 0.83       ┆ -29.83     ┆ 74.56     ┆ 10.632    │\n",
      "│ 303.722 ┆ 0.368659 ┆ 0.665382  ┆ 0.867538  ┆ … ┆ 0.06       ┆ -130.552   ┆ 303.722   ┆ 7.79      │\n",
      "└─────────┴──────────┴───────────┴───────────┴───┴────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "y = train_data[\"label\"]\n",
    "X, drop_columns = preprocess_train(\n",
    "    train_data,\n",
    "    columns_to_drop=[\"label\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n",
    ")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.003008,
     "end_time": "2025-06-03T09:47:24.436311",
     "exception": false,
     "start_time": "2025-06-03T09:47:24.433303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1.2 Time Series Split Functions\n",
    "\n",
    "1. `split_rollingwindow` - rolling window.\n",
    "3. `split_overlapwindow` - overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:45:47.096492Z",
     "iopub.status.busy": "2025-06-10T10:45:47.096283Z",
     "iopub.status.idle": "2025-06-10T10:45:47.101467Z",
     "shell.execute_reply": "2025-06-10T10:45:47.100826Z",
     "shell.execute_reply.started": "2025-06-10T10:45:47.096475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_rollingwindow(X, n_splits=5, train_ratio=0.8):\n",
    "    \"\"\"\n",
    "    Rolling window time series splitter with fixed train/test ratio and number of splits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like or DataFrame\n",
    "        Dataset with time-ordering preserved.\n",
    "    n_splits : int\n",
    "        Number of rolling splits.\n",
    "    train_ratio : float\n",
    "        Proportion of each window used for training (e.g. 0.8 for 80/20 split).\n",
    "    \n",
    "    Yields\n",
    "    ------\n",
    "    train_idx, test_idx : range, range\n",
    "        Index ranges for training and testing.\n",
    "    \"\"\"\n",
    "    n_obs = len(X)\n",
    "    window_size = n_obs // (n_splits + 1)\n",
    "    train_size = int(train_ratio * window_size)\n",
    "    test_size = window_size - train_size\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        start = i * window_size\n",
    "        train_start = start\n",
    "        train_end = train_start + train_size\n",
    "        test_start = train_end\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        if test_end > n_obs:\n",
    "            break\n",
    "\n",
    "        yield range(train_start, train_end), range(test_start, test_end)\n",
    "\n",
    "# splits = rolling_window_split(X=X, n_splits = 5, train_ratio=0.5)\n",
    "# for train_idx, test_idx in splits:\n",
    "#     print(train_idx, test_idx)\n",
    "# #     df_train = X.slice(train_idx.start, len(train_idx))\n",
    "# #     df_test = X.slice(test_idx.start, len(test_idx))\n",
    "\n",
    "# for i, (train_idx, test_idx) in enumerate(splits):\n",
    "#     if i == 1:  # second batch (index 1)\n",
    "#         df_train = X.slice(train_idx.start, len(train_idx))\n",
    "#         df_test = X.slice(test_idx.start, len(test_idx))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:45:47.102495Z",
     "iopub.status.busy": "2025-06-10T10:45:47.102265Z",
     "iopub.status.idle": "2025-06-10T10:45:47.117823Z",
     "shell.execute_reply": "2025-06-10T10:45:47.117156Z",
     "shell.execute_reply.started": "2025-06-10T10:45:47.102481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_overlapwindow(X, train_size, test_size, step):\n",
    "    \"\"\"\n",
    "    Rolling window splitter with overlapping train/test splits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like or DataFrame\n",
    "        Dataset with time-ordering preserved.\n",
    "    train_size : int\n",
    "        Number of observations in each training window.\n",
    "    test_size : int\n",
    "        Number of observations in each test window.\n",
    "    step : int\n",
    "        Forward step size between each split.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    train_idx, test_idx : range, range\n",
    "        Index ranges for training and testing.\n",
    "    \"\"\"\n",
    "    n_obs = len(X)\n",
    "    start = 0\n",
    "    while (start + train_size + test_size) <= n_obs:\n",
    "        train_start = start\n",
    "        train_end = train_start + train_size\n",
    "        test_start = train_end\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        yield range(train_start, train_end), range(test_start, test_end)\n",
    "        start += step\n",
    "\n",
    "# train_size = 100000\n",
    "# test_size = 100000\n",
    "# step = 10000\n",
    "# splits = split_overlapwindow(X, train_size, test_size, step)\n",
    "# for train_idx, test_idx in splits:\n",
    "#     print(train_idx, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Iterative Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:45:47.118796Z",
     "iopub.status.busy": "2025-06-10T10:45:47.118541Z",
     "iopub.status.idle": "2025-06-10T10:45:47.627403Z",
     "shell.execute_reply": "2025-06-10T10:45:47.626863Z",
     "shell.execute_reply.started": "2025-06-10T10:45:47.118776Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "def instantiate_model():\n",
    "    if not os.getenv(key='USER'):\n",
    "        print(\"KAGGLE ENVIRONMENT DETECTED - GPU\")\n",
    "        model = XGBRegressor(\n",
    "            tree_method = \"hist\", \n",
    "            device = \"cuda\",\n",
    "            n_estimators=1000,\n",
    "            max_depth=100,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        print(\"LOCAL ENVIRONMENT - CPU\")\n",
    "        model = XGBRegressor(\n",
    "            tree_method=\"hist\",\n",
    "            n_estimators=1000,\n",
    "            max_depth=100,\n",
    "            learning_rate=0.05,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:45:47.628519Z",
     "iopub.status.busy": "2025-06-10T10:45:47.628217Z",
     "iopub.status.idle": "2025-06-10T10:45:49.636795Z",
     "shell.execute_reply": "2025-06-10T10:45:49.635990Z",
     "shell.execute_reply.started": "2025-06-10T10:45:47.628504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import uuid\n",
    "# import cupy as cp  # You'll need cupy installed\n",
    "if not os.getenv(key='USER'):\n",
    "    import cupy as cp  # Ensure you have CuPy installed for GPU support\n",
    "else:\n",
    "    cp = None\n",
    "import shap  # Ensure you have SHAP installed for feature importance\n",
    "\n",
    "def iterative_featureselection(\n",
    "    model,\n",
    "    X: pl.DataFrame,\n",
    "    y: pl.Series,\n",
    "    split_fn,\n",
    "    scorers: dict,\n",
    "    drop_fraction: float = 0.1,\n",
    "    min_features: int = 10,\n",
    "    use_shap: bool = False,\n",
    "    use_gpu : bool = True\n",
    ") -> list:\n",
    "    ID = uuid.uuid4().hex\n",
    "    results = []\n",
    "    splits = list(split_fn(X))\n",
    "    n_splits = len(splits)\n",
    "\n",
    "    current_X = X.clone()\n",
    "    feature_cols = current_X.columns\n",
    "    iteration = 0\n",
    "\n",
    "    while len(feature_cols) > min_features:\n",
    "        iteration += 1\n",
    "        print(f\"\\n[+] Iteration {iteration} - {len(feature_cols)} features\")\n",
    "\n",
    "        # Extract feature matrix\n",
    "        X_np = current_X.select(feature_cols).to_numpy()\n",
    "        y_np = y.to_numpy().flatten()\n",
    "        if use_gpu:\n",
    "            X_np = cp.asarray(X_np)\n",
    "            y_np = cp.asarray(y_np)\n",
    "        \n",
    "        print(f\"Summary: {len(feature_cols)} features, X: {len(X_np)} y: {len(y_np)}\")\n",
    "        # Fit full model for feature importances\n",
    "        training_model = clone(model)\n",
    "        training_model.fit(X_np, y_np)\n",
    "\n",
    "        # 1. Compute raw importances as a NumPy array\n",
    "        if use_shap:\n",
    "            explainer   = shap.TreeExplainer(training_model)\n",
    "            shap_vals   = explainer.shap_values(X_np)          # shape (n_samples, n_features)\n",
    "            imp_array   = np.abs(shap_vals).mean(axis=0)        # mean(|SHAP|) per feature\n",
    "        else:\n",
    "            imp_array   = np.array(training_model.feature_importances_)\n",
    "        \n",
    "        # 2. Build a Polars DataFrame of (feature, importance)\n",
    "        importances_df = pl.DataFrame({\n",
    "            \"feature\": feature_cols,\n",
    "            \"importance\": imp_array.tolist()\n",
    "        })\n",
    "        \n",
    "        # Cross-validation\n",
    "        metrics = {k: [] for k in scorers}\n",
    "\n",
    "        for i, (train_idx, test_idx) in enumerate(splits, start=1):\n",
    "            print(f\"    [Fold {i}/{n_splits}] ...\", end=\"\\r\")\n",
    "\n",
    "            X_train, y_train = X_np[train_idx], y_np[train_idx]\n",
    "            X_test, y_test = X_np[test_idx], y_np[test_idx]\n",
    "\n",
    "            fold_model = clone(model)\n",
    "            fold_model.fit(X_train, y_train)\n",
    "            y_pred = fold_model.predict(X_test)\n",
    "\n",
    "            for name, func in scorers.items():\n",
    "                y_test = y_test.get() if hasattr(y_test, 'get') else y_test\n",
    "                y_pred = y_pred.get() if hasattr(y_pred, 'get') else y_pred\n",
    "                metrics[name].append(func(y_test, y_pred))\n",
    "\n",
    "        # Mean metric values\n",
    "        mean_metrics = {name: np.mean(vals) for name, vals in metrics.items()}\n",
    "\n",
    "        results.append({\n",
    "            \"ID\": ID,\n",
    "            \"params\": {\n",
    "                \"drop_fraction\" : drop_fraction,\n",
    "                \"min_features\": min_features,\n",
    "                \"use_shap\" : use_shap,\n",
    "                \"use_gpu\" : use_gpu\n",
    "            },\n",
    "            \"num_features\": len(feature_cols),\n",
    "            \"features\": feature_cols.copy(),\n",
    "            **{f\"scores_{name}_mean\": val for name, val in mean_metrics.items()}\n",
    "        })\n",
    "\n",
    "        print(\"    → \" + \" | \".join([f\"{k.upper()}: {v:.4f}\" for k, v in mean_metrics.items()]))\n",
    "\n",
    "        # 3. Identify the n_drop least important features\n",
    "        n_drop    = max(10, int(len(feature_cols) * drop_fraction))\n",
    "        drop_cols = (\n",
    "            importances_df\n",
    "            .sort(\"importance\")        # ascending\n",
    "            .head(n_drop)              # take the smallest n_drop\n",
    "            .get_column(\"feature\")     # extract the feature column\n",
    "            .to_list()                 # into a Python list for filtering\n",
    "        )\n",
    "        \n",
    "        # 4. Filter out dropped features in your Polars DataFrame\n",
    "        feature_cols = [f for f in feature_cols if f not in drop_cols]\n",
    "        current_X    = current_X.select(feature_cols)\n",
    "\n",
    "    print(f\"\"\"\\n {\"[\"+ \"*\" * 10 + \"]\"} Finished: {len(feature_cols)} features remaining (≤ min_features={min_features}) {\"[\"+ \"*\" * 10 + \"]\"}\"\"\")\n",
    "    return results\n",
    "    \n",
    "def pearson_corr(y_true, y_pred):\n",
    "    return pearsonr(y_true, y_pred)[0]\n",
    "\n",
    "scorers = {\n",
    "    \"pearson\": pearson_corr,\n",
    "    \"mse\": mean_squared_error,\n",
    "    \"mae\": mean_absolute_error\n",
    "}\n",
    "\n",
    "# Test\n",
    "# results = iterative_featureselection(\n",
    "#     model=model,\n",
    "#     X=X,\n",
    "#     y=y,\n",
    "#     split_fn=lambda X:split_rollingwindow(X=X, n_splits = 2, train_ratio=0.5),\n",
    "#     scorers = scorers,\n",
    "#     drop_fraction=0.9,\n",
    "#     min_features=100,\n",
    "#     use_shap = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:45:49.637877Z",
     "iopub.status.busy": "2025-06-10T10:45:49.637539Z",
     "iopub.status.idle": "2025-06-10T10:45:49.643624Z",
     "shell.execute_reply": "2025-06-10T10:45:49.642824Z",
     "shell.execute_reply.started": "2025-06-10T10:45:49.637860Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Training for Iteration 1 (shap=True)...\")\n",
    "# iter_1 = iterative_featureselection(\n",
    "#     model=model,\n",
    "#     X=X,\n",
    "#     y=y,\n",
    "#     split_fn=lambda X:split_rollingwindow(X=X, n_splits = 10, train_ratio=0.5),\n",
    "#     # split_fn=lambda X:split_overlapwindow(X, 50000, 50000, 25000),\n",
    "#     scorers = scorers,\n",
    "#     drop_fraction=0.1,\n",
    "#     min_features=10,\n",
    "#     use_shap = True\n",
    "# )\n",
    "# out_path = \"/kaggle/working/iterative_featuresselection.json\"\n",
    "# save_results_to_json(iter_1, out_path)\n",
    "\n",
    "# print(\"Training for Iteration 2 (shap = False)...\")\n",
    "# iter_2 = iterative_featureselection(\n",
    "#     model=model,\n",
    "#     X=X,\n",
    "#     y=y,\n",
    "#     # split_fn=lambda X:split_overlapwindow(X, 50000, 50000, 25000),\n",
    "#     split_fn=lambda X:split_rollingwindow(X=X, n_splits = 10, train_ratio=0.5),\n",
    "#     scorers = scorers,\n",
    "#     drop_fraction=0.1,\n",
    "#     min_features=10,\n",
    "#     use_shap = False\n",
    "# )\n",
    "# out_path = \"/kaggle/working/iterative_featuresselection.json\"\n",
    "# save_results_to_json(iter_2, out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T10:45:49.646609Z",
     "iopub.status.busy": "2025-06-10T10:45:49.646346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL ENVIRONMENT - CPU\n",
      "Columns with infinite values: ['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']\n",
      "Columns with NaN values: []\n",
      "Columns with zero standard deviation: ['X864', 'X867', 'X869', 'X870', 'X871', 'X872']\n",
      "\n",
      "==================================================\n",
      "Running experiment: \n",
      "==================================================\n",
      "\n",
      "(525887, 871)\n",
      "(525887, 1)\n",
      "\n",
      "[+] Iteration 1 - 871 features\n",
      "Summary: 871 features, X: 525887 y: 525887\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def run_feature_selection_experiments(data, experiments, splits, scorers, output_dir=\"/kaggle/working\"):\n",
    "    \"\"\"\n",
    "    Feature Selection Experiments\n",
    "    \n",
    "    Parameters:\n",
    "    - experiments\n",
    "    - splits : timeframe splits\n",
    "    - scorers : model scorers\n",
    "    \n",
    "    \"\"\"\n",
    "    # Process data - split based on time.\n",
    "    y = data.select([\n",
    "        pl.col(\"timestamp\").alias(\"timestamp\"),\n",
    "        pl.col(\"label\").alias(\"label\")\n",
    "    ])\n",
    "    X, drop_columns = preprocess_train(\n",
    "        data,\n",
    "        columns_to_drop=[\"label\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n",
    "    )\n",
    "    data_store = {}\n",
    "    for s_name, s_time in splits.items():\n",
    "        data_store[s_name] = {\n",
    "            \"X\": X.filter(pl.col(\"timestamp\") >= s_time),\n",
    "            \"y\": y.filter(pl.col(\"timestamp\") >= s_time).drop(\"timestamp\")\n",
    "        }\n",
    "    \n",
    "    all_results = []\n",
    "\n",
    "    for config in experiments:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Running experiment: \")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "        for s_name in splits.keys():\n",
    "            try:\n",
    "                print(data_store[s_name][\"X\"].shape)\n",
    "                print(data_store[s_name][\"y\"].shape)\n",
    "                results = iterative_featureselection(\n",
    "                    model=model,\n",
    "                    X=data_store[s_name][\"X\"],\n",
    "                    y=data_store[s_name][\"y\"],\n",
    "                    split_fn=config['split_fn'],\n",
    "                    scorers=scorers,\n",
    "                    drop_fraction=0.9,\n",
    "                    min_features=200,\n",
    "                    use_shap=config['use_shap'],\n",
    "                    use_gpu=config.get('use_gpu', False)\n",
    "                )\n",
    "                \n",
    "                results_df = pl.DataFrame(results).with_columns([\n",
    "                    pl.lit(config['name']).alias(\"experiment_name\"),\n",
    "                    pl.lit(config['use_shap']).alias(\"use_shap\"),\n",
    "                    pl.lit(s_name).alias(\"split_name\"),\n",
    "                ])\n",
    "\n",
    "                return results_df\n",
    "                results_df.to_pandas().to_excel(\n",
    "                    f\"{output_dir}/feature_selection_{config['name']}_{s_name}.xlsx\",\n",
    "                    index=False\n",
    "                )\n",
    "                # # Save results for this configuration\n",
    "                # out_path = f\"{output_dir}/feature_selection_{config['desc']}.json\"\n",
    "                # save_results_to_json(results, out_path)\n",
    "                # print(f\"Saved results to {out_path}\")\n",
    "                \n",
    "                # all_results.append({\n",
    "                #     \"config\": config,\n",
    "                #     \"results\": results,\n",
    "                #     \"output_path\": out_path\n",
    "                # })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in experiment {config['desc']}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "    return all_results\n",
    "\n",
    "model = instantiate_model()\n",
    "\n",
    "# Define the experiment configurations\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"ex1\",\n",
    "        \"desc\" : \"Test Experiment\",\n",
    "        \"model\" : clone(model),\n",
    "        \"split_fn\" : lambda X: split_rollingwindow(X=X, n_splits=5, train_ratio=0.5),\n",
    "        \"use_shap\" : True,\n",
    "        \"use_gpu\" : False,\n",
    "    }\n",
    "]\n",
    "\n",
    "data = run_feature_selection_experiments(\n",
    "    data=train_data,\n",
    "    experiments=experiments,\n",
    "    splits=train_splits,\n",
    "    scorers=scorers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Data 4\n",
    "# start = pd.Timestamp('2023-12-01 00:00:00')\n",
    "# end = pd.Timestamp('2024-02-29 23:59:00')\n",
    "\n",
    "# # Data 4\n",
    "# # start = pd.Timestamp('2024-02-01 00:00:00')\n",
    "# # end = pd.Timestamp('2024-02-29 23:59:00')\n",
    "\n",
    "# X_period = X[(X.index >= start) & (X.index <= end)]\n",
    "# y_period = y[(X.index >= start) & (X.index <= end)]\n",
    "# X_period.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"Training for Iteration 3 (shap = False) / Smaller Window...\")\n",
    "# iter_2 = iterative_featureselection(\n",
    "#     model=model,\n",
    "#     X=X,\n",
    "#     y=y,\n",
    "#     # split_fn=lambda X:split_overlapwindow(X, 50000, 50000, 25000),\n",
    "#     split_fn=lambda X:split_rollingwindow(X=X, n_splits = 1, train_ratio=0.5),\n",
    "#     scorers = scorers,\n",
    "#     drop_fraction=0.1,\n",
    "#     min_features=10,\n",
    "#     use_shap = False\n",
    "# )\n",
    "# out_path = \"/kaggle/working/iterative_featuresselection.json\"\n",
    "# save_results_to_json(iter_2, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/working/iterative_featuresselection.json\", \"r\") as f:\n",
    "    current_results = json.load(f)\n",
    "\n",
    "current_results_df = pl.DataFrame(current_results).sort('scores_pearson_mean', descending = True)\n",
    "current_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "current_results_df.to_excel(\"iterative_featureselection.xlsx\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ryant'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ.get(\"USER\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11418275,
     "sourceId": 96164,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 773.161209,
   "end_time": "2025-06-03T09:59:54.014100",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-03T09:47:00.852891",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

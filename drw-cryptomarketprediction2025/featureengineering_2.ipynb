{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c0e8e0",
   "metadata": {},
   "source": [
    "# Fundamental Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b87b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (525_887, 897)\n",
      "┌─────────┬─────────┬─────────┬──────────┬───┬──────────┬──────────┬──────────┬──────────────┐\n",
      "│ bid_qty ┆ ask_qty ┆ buy_qty ┆ sell_qty ┆ … ┆ X889     ┆ X890     ┆ label    ┆ timestamp    │\n",
      "│ ---     ┆ ---     ┆ ---     ┆ ---      ┆   ┆ ---      ┆ ---      ┆ ---      ┆ ---          │\n",
      "│ f64     ┆ f64     ┆ f64     ┆ f64      ┆   ┆ f64      ┆ f64      ┆ f64      ┆ datetime[ns] │\n",
      "╞═════════╪═════════╪═════════╪══════════╪═══╪══════════╪══════════╪══════════╪══════════════╡\n",
      "│ 15.283  ┆ 8.425   ┆ 176.405 ┆ 44.984   ┆ … ┆ 0.159183 ┆ 0.530636 ┆ 0.562539 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:00:00     │\n",
      "│ 38.59   ┆ 2.336   ┆ 525.846 ┆ 321.95   ┆ … ┆ 0.158963 ┆ 0.530269 ┆ 0.533686 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:01:00     │\n",
      "│ 0.442   ┆ 60.25   ┆ 159.227 ┆ 136.369  ┆ … ┆ 0.158744 ┆ 0.529901 ┆ 0.546505 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:02:00     │\n",
      "│ 4.865   ┆ 21.016  ┆ 335.742 ┆ 124.963  ┆ … ┆ 0.158524 ┆ 0.529534 ┆ 0.357703 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:03:00     │\n",
      "│ 27.158  ┆ 3.451   ┆ 98.411  ┆ 44.407   ┆ … ┆ 0.158304 ┆ 0.529167 ┆ 0.362452 ┆ 2023-03-01   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 00:04:00     │\n",
      "│ …       ┆ …       ┆ …       ┆ …        ┆ … ┆ …        ┆ …        ┆ …        ┆ …            │\n",
      "│ 4.163   ┆ 6.805   ┆ 39.037  ┆ 55.351   ┆ … ┆ 0.136494 ┆ 0.243172 ┆ 0.396289 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:55:00     │\n",
      "│ 2.29    ┆ 4.058   ┆ 110.201 ┆ 67.171   ┆ … ┆ 0.136305 ┆ 0.243004 ┆ 0.328993 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:56:00     │\n",
      "│ 5.237   ┆ 3.64    ┆ 70.499  ┆ 30.753   ┆ … ┆ 0.136117 ┆ 0.242836 ┆ 0.189909 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:57:00     │\n",
      "│ 5.731   ┆ 4.901   ┆ 22.365  ┆ 52.195   ┆ … ┆ 0.135928 ┆ 0.242668 ┆ 0.410831 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:58:00     │\n",
      "│ 3.925   ┆ 3.865   ┆ 86.585  ┆ 217.137  ┆ … ┆ 0.135741 ┆ 0.242501 ┆ 0.731542 ┆ 2024-02-29   │\n",
      "│         ┆         ┆         ┆          ┆   ┆          ┆          ┆          ┆ 23:59:00     │\n",
      "└─────────┴─────────┴─────────┴──────────┴───┴──────────┴──────────┴──────────┴──────────────┘\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "train_splits = {\n",
    "    \"full\" : pl.datetime(2023, 3, 1, 0, 0, 0),\n",
    "    \"last_9m\" : pl.datetime(2023, 6, 1, 0, 0, 0),\n",
    "    \"last_6m\" : pl.datetime(2023, 9, 1, 0, 0, 0),\n",
    "    \"last_3m\" : pl.datetime(2023, 12, 1, 0, 0, 0),\n",
    "    \"last_1m\": pl.datetime(2024, 2, 1, 0, 0, 0),\n",
    "    \"last_2w\": pl.datetime(2024, 2, 15, 0, 0, 0),\n",
    "}\n",
    "\n",
    "PATHS = {\n",
    "    \"TRAIN_PATH\" :\"./kaggle/kaggle/input/drw-crypto-market-prediction/train.parquet\",\n",
    "    \"TEST_PATH\" : \"./kaggle/kaggle/input/drw-crypto-market-prediction/test.parquet\",\n",
    "    \"SUBMISSION_PATH\" : \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\",\n",
    "}\n",
    "\n",
    "train_data = pl.read_parquet(PATHS[\"TRAIN_PATH\"]).sort(\"timestamp\", descending = False)\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cffa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with infinite values: ['X697', 'X698', 'X699', 'X700', 'X701', 'X702', 'X703', 'X704', 'X705', 'X706', 'X707', 'X708', 'X709', 'X710', 'X711', 'X712', 'X713', 'X714', 'X715', 'X716', 'X717']\n",
      "Columns with NaN values: []\n",
      "Columns with zero standard deviation: ['X864', 'X867', 'X869', 'X870', 'X871', 'X872']\n",
      "shape: (525_887, 871)\n",
      "┌─────────┬──────────┬───────────┬───────────┬───┬────────────┬────────────┬───────────┬───────────┐\n",
      "│ volume  ┆ X1       ┆ X2        ┆ X3        ┆ … ┆ bidask_del ┆ buysell_de ┆ buysell_s ┆ bidask_si │\n",
      "│ ---     ┆ ---      ┆ ---       ┆ ---       ┆   ┆ ta         ┆ lta        ┆ ize       ┆ ze        │\n",
      "│ f64     ┆ f64      ┆ f64       ┆ f64       ┆   ┆ ---        ┆ ---        ┆ ---       ┆ ---       │\n",
      "│         ┆          ┆           ┆           ┆   ┆ f64        ┆ f64        ┆ f64       ┆ f64       │\n",
      "╞═════════╪══════════╪═══════════╪═══════════╪═══╪════════════╪════════════╪═══════════╪═══════════╡\n",
      "│ 221.389 ┆ 0.121263 ┆ -0.41769  ┆ 0.005399  ┆ … ┆ 6.858      ┆ 131.421    ┆ 221.389   ┆ 23.708    │\n",
      "│ 847.796 ┆ 0.302841 ┆ -0.049576 ┆ 0.356667  ┆ … ┆ 36.254     ┆ 203.896    ┆ 847.796   ┆ 40.926    │\n",
      "│ 295.596 ┆ 0.167462 ┆ -0.291212 ┆ 0.083138  ┆ … ┆ -59.808    ┆ 22.858     ┆ 295.596   ┆ 60.692    │\n",
      "│ 460.705 ┆ 0.072944 ┆ -0.43659  ┆ -0.102483 ┆ … ┆ -16.151    ┆ 210.779    ┆ 460.705   ┆ 25.881    │\n",
      "│ 142.818 ┆ 0.17382  ┆ -0.213489 ┆ 0.096067  ┆ … ┆ 23.707     ┆ 54.004     ┆ 142.818   ┆ 30.609    │\n",
      "│ …       ┆ …        ┆ …         ┆ …         ┆ … ┆ …          ┆ …          ┆ …         ┆ …         │\n",
      "│ 94.388  ┆ 0.020155 ┆ 0.076565  ┆ 0.228994  ┆ … ┆ -2.642     ┆ -16.314    ┆ 94.388    ┆ 10.968    │\n",
      "│ 177.372 ┆ 0.016262 ┆ 0.062527  ┆ 0.214072  ┆ … ┆ -1.768     ┆ 43.03      ┆ 177.372   ┆ 6.348     │\n",
      "│ 101.252 ┆ 0.045407 ┆ 0.109834  ┆ 0.263577  ┆ … ┆ 1.597      ┆ 39.746     ┆ 101.252   ┆ 8.877     │\n",
      "│ 74.56   ┆ 0.124783 ┆ 0.244168  ┆ 0.408704  ┆ … ┆ 0.83       ┆ -29.83     ┆ 74.56     ┆ 10.632    │\n",
      "│ 303.722 ┆ 0.368659 ┆ 0.665382  ┆ 0.867538  ┆ … ┆ 0.06       ┆ -130.552   ┆ 303.722   ┆ 7.79      │\n",
      "└─────────┴──────────┴───────────┴───────────┴───┴────────────┴────────────┴───────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "def get_cols_inf(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names that contain any positive or negative infinity.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        # df[col] is a Series; .is_infinite() → Boolean Series; .any() → Python bool\n",
    "        try:\n",
    "            if df[col].is_infinite().any():\n",
    "                cols.append(col)\n",
    "        except Exception:\n",
    "            # if the column isn’t numeric, .is_infinite() might error—just skip it\n",
    "            continue\n",
    "    return cols\n",
    "\n",
    "def get_nan_columns(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names with any NaN/null values.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col in df.columns:\n",
    "        if df.select(pl.col(col).is_null().any()).item():\n",
    "            cols.append(col)\n",
    "    return cols\n",
    "\n",
    "def get_cols_zerostd(df: pl.DataFrame) -> list[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of column names whose standard deviation is zero\n",
    "    (or whose std returns None because all values are null).\n",
    "    Non-numeric columns (e.g. datetime) are skipped.\n",
    "    \"\"\"\n",
    "    cols = []\n",
    "    for col, dtype in zip(df.columns, df.dtypes):\n",
    "        # Only attempt std() on numeric dtypes\n",
    "        if dtype.is_numeric():  \n",
    "            # df[col] is a Series; .std() returns a Python float or None\n",
    "            std_val = df[col].std()\n",
    "            if std_val == 0.0 or std_val is None:\n",
    "                cols.append(col)\n",
    "    return cols\n",
    "\n",
    "\n",
    "def feature_engineering(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Feature engineering\n",
    "    df = df.with_columns([\n",
    "        # bidask_ratio = bid_qty / ask_qty\n",
    "        (pl.col(\"bid_qty\") / pl.col(\"ask_qty\")).alias(\"bidask_ratio\"),\n",
    "\n",
    "        # buysell_ratio = 0 if volume == 0 else buy_qty / sell_qty\n",
    "        pl.when(pl.col(\"volume\") == 0)\n",
    "        .then(0)\n",
    "        .otherwise(pl.col(\"buy_qty\") / pl.col(\"sell_qty\"))\n",
    "        .alias(\"buysell_ratio\"),\n",
    "\n",
    "        # bidask_delta = bid_qty - ask_qty\n",
    "        (pl.col(\"bid_qty\") - pl.col(\"ask_qty\")).alias(\"bidask_delta\"),\n",
    "\n",
    "        # buysell_delta = buy_qty - sell_qty\n",
    "        (pl.col(\"buy_qty\") - pl.col(\"sell_qty\")).alias(\"buysell_delta\"),\n",
    "\n",
    "        # buysell_size = buy_qty + sell_qty\n",
    "        (pl.col(\"buy_qty\") + pl.col(\"sell_qty\")).alias(\"buysell_size\"),\n",
    "\n",
    "        # bidask_size = bid_qty + ask_qty\n",
    "        (pl.col(\"bid_qty\") + pl.col(\"ask_qty\")).alias(\"bidask_size\"),\n",
    "    ])\n",
    "    return df\n",
    "def preprocess_train(train: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Mirror of the original pandas workflow, but using polars.\n",
    "    1. Identify columns with infinite, NaN, or zero‐std and drop them.\n",
    "    2. Drop any user‐specified columns (e.g. label or order‐book columns).\n",
    "    3. (You can add normalized/scaling steps here if needed.)\n",
    "    \"\"\"\n",
    "    df = train.clone()\n",
    "\n",
    "    df = feature_engineering(df)\n",
    "    \n",
    "    #### Preprocessing\n",
    "    cols_inf = get_cols_inf(df)\n",
    "    print(\"Columns with infinite values:\", cols_inf)\n",
    "\n",
    "    cols_nan = get_nan_columns(df)\n",
    "    print(\"Columns with NaN values:\", cols_nan)\n",
    "\n",
    "    cols_zerostd = get_cols_zerostd(df)\n",
    "    print(\"Columns with zero standard deviation:\", cols_zerostd)\n",
    "    # Drop columns with infinite, NaN, or zero‐std values\n",
    "    drop_columns = list(set(cols_inf) | set(cols_nan) | set(cols_zerostd) | set(columns_to_drop))\n",
    "    if drop_columns:\n",
    "        df = df.drop(drop_columns)\n",
    "    # df = df.sort(\"timestamp\", descending=False)\n",
    "    return df, drop_columns\n",
    "\n",
    "def preprocess_test(test: pl.DataFrame, columns_to_drop: list[str] = []) -> pl.DataFrame:\n",
    "    df = test.clone()\n",
    "    df = feature_engineering(df)\n",
    "    df = df.drop(columns_to_drop)\n",
    "    print(\"Columns dropped from test set:\", columns_to_drop)\n",
    "    return df\n",
    "\n",
    "y = train_data[[\"timestamp\", \"label\"]]\n",
    "X, drop_columns = preprocess_train(\n",
    "    train_data,\n",
    "    columns_to_drop=[\"label\", \"bid_qty\", \"ask_qty\", \"buy_qty\", \"sell_qty\"]\n",
    ")\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de691f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X888</th>\n",
       "      <th>X889</th>\n",
       "      <th>X890</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>bidask_ratio</th>\n",
       "      <th>buysell_ratio</th>\n",
       "      <th>bidask_delta</th>\n",
       "      <th>buysell_delta</th>\n",
       "      <th>buysell_size</th>\n",
       "      <th>bidask_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>181.331</td>\n",
       "      <td>-0.606128</td>\n",
       "      <td>-0.026132</td>\n",
       "      <td>0.240236</td>\n",
       "      <td>0.226543</td>\n",
       "      <td>0.009880</td>\n",
       "      <td>-0.207929</td>\n",
       "      <td>-0.308342</td>\n",
       "      <td>-0.414538</td>\n",
       "      <td>-0.952021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.209209</td>\n",
       "      <td>0.137915</td>\n",
       "      <td>0.319477</td>\n",
       "      <td>2024-02-15 00:00:00</td>\n",
       "      <td>1.307461</td>\n",
       "      <td>0.502154</td>\n",
       "      <td>1.125</td>\n",
       "      <td>-60.097</td>\n",
       "      <td>181.331</td>\n",
       "      <td>8.443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110.604</td>\n",
       "      <td>-0.613099</td>\n",
       "      <td>-0.036430</td>\n",
       "      <td>0.218995</td>\n",
       "      <td>0.209116</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>-0.214304</td>\n",
       "      <td>-0.314875</td>\n",
       "      <td>-0.421216</td>\n",
       "      <td>-0.687147</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208630</td>\n",
       "      <td>0.137725</td>\n",
       "      <td>0.319256</td>\n",
       "      <td>2024-02-15 00:01:00</td>\n",
       "      <td>0.017929</td>\n",
       "      <td>0.400831</td>\n",
       "      <td>-10.517</td>\n",
       "      <td>-47.308</td>\n",
       "      <td>110.604</td>\n",
       "      <td>10.901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.052</td>\n",
       "      <td>-0.475340</td>\n",
       "      <td>0.217352</td>\n",
       "      <td>0.478561</td>\n",
       "      <td>0.476689</td>\n",
       "      <td>0.139481</td>\n",
       "      <td>-0.076333</td>\n",
       "      <td>-0.176871</td>\n",
       "      <td>-0.283260</td>\n",
       "      <td>-0.614209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208053</td>\n",
       "      <td>0.137534</td>\n",
       "      <td>0.319035</td>\n",
       "      <td>2024-02-15 00:02:00</td>\n",
       "      <td>1.998738</td>\n",
       "      <td>1.051123</td>\n",
       "      <td>3.957</td>\n",
       "      <td>1.746</td>\n",
       "      <td>70.052</td>\n",
       "      <td>11.881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>124.021</td>\n",
       "      <td>-0.518950</td>\n",
       "      <td>0.118303</td>\n",
       "      <td>0.378718</td>\n",
       "      <td>0.383085</td>\n",
       "      <td>0.095079</td>\n",
       "      <td>-0.119611</td>\n",
       "      <td>-0.220175</td>\n",
       "      <td>-0.326643</td>\n",
       "      <td>-0.833682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207478</td>\n",
       "      <td>0.137344</td>\n",
       "      <td>0.318814</td>\n",
       "      <td>2024-02-15 00:03:00</td>\n",
       "      <td>0.476843</td>\n",
       "      <td>1.597897</td>\n",
       "      <td>-4.846</td>\n",
       "      <td>28.543</td>\n",
       "      <td>124.021</td>\n",
       "      <td>13.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131.966</td>\n",
       "      <td>-0.693000</td>\n",
       "      <td>-0.208907</td>\n",
       "      <td>0.029630</td>\n",
       "      <td>0.034411</td>\n",
       "      <td>-0.078318</td>\n",
       "      <td>-0.292848</td>\n",
       "      <td>-0.393678</td>\n",
       "      <td>-0.500346</td>\n",
       "      <td>-0.841456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.206905</td>\n",
       "      <td>0.137154</td>\n",
       "      <td>0.318594</td>\n",
       "      <td>2024-02-15 00:04:00</td>\n",
       "      <td>3.877078</td>\n",
       "      <td>0.917581</td>\n",
       "      <td>6.577</td>\n",
       "      <td>-5.672</td>\n",
       "      <td>131.966</td>\n",
       "      <td>11.149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21594</th>\n",
       "      <td>94.388</td>\n",
       "      <td>0.020155</td>\n",
       "      <td>0.076565</td>\n",
       "      <td>0.228994</td>\n",
       "      <td>0.288856</td>\n",
       "      <td>0.151634</td>\n",
       "      <td>0.108347</td>\n",
       "      <td>0.088073</td>\n",
       "      <td>0.073729</td>\n",
       "      <td>0.071211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212651</td>\n",
       "      <td>0.136494</td>\n",
       "      <td>0.243172</td>\n",
       "      <td>2024-02-29 23:55:00</td>\n",
       "      <td>0.611756</td>\n",
       "      <td>0.705263</td>\n",
       "      <td>-2.642</td>\n",
       "      <td>-16.314</td>\n",
       "      <td>94.388</td>\n",
       "      <td>10.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21595</th>\n",
       "      <td>177.372</td>\n",
       "      <td>0.016262</td>\n",
       "      <td>0.062527</td>\n",
       "      <td>0.214072</td>\n",
       "      <td>0.276463</td>\n",
       "      <td>0.146521</td>\n",
       "      <td>0.104164</td>\n",
       "      <td>0.084063</td>\n",
       "      <td>0.069788</td>\n",
       "      <td>0.024066</td>\n",
       "      <td>...</td>\n",
       "      <td>0.212063</td>\n",
       "      <td>0.136305</td>\n",
       "      <td>0.243004</td>\n",
       "      <td>2024-02-29 23:56:00</td>\n",
       "      <td>0.564317</td>\n",
       "      <td>1.640604</td>\n",
       "      <td>-1.768</td>\n",
       "      <td>43.030</td>\n",
       "      <td>177.372</td>\n",
       "      <td>6.348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>101.252</td>\n",
       "      <td>0.045407</td>\n",
       "      <td>0.109834</td>\n",
       "      <td>0.263577</td>\n",
       "      <td>0.329266</td>\n",
       "      <td>0.174214</td>\n",
       "      <td>0.132940</td>\n",
       "      <td>0.113052</td>\n",
       "      <td>0.098865</td>\n",
       "      <td>-0.057370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211477</td>\n",
       "      <td>0.136117</td>\n",
       "      <td>0.242836</td>\n",
       "      <td>2024-02-29 23:57:00</td>\n",
       "      <td>1.438736</td>\n",
       "      <td>2.292427</td>\n",
       "      <td>1.597</td>\n",
       "      <td>39.746</td>\n",
       "      <td>101.252</td>\n",
       "      <td>8.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21597</th>\n",
       "      <td>74.560</td>\n",
       "      <td>0.124783</td>\n",
       "      <td>0.244168</td>\n",
       "      <td>0.408704</td>\n",
       "      <td>0.480016</td>\n",
       "      <td>0.251493</td>\n",
       "      <td>0.211727</td>\n",
       "      <td>0.192160</td>\n",
       "      <td>0.178116</td>\n",
       "      <td>0.111335</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210892</td>\n",
       "      <td>0.135928</td>\n",
       "      <td>0.242668</td>\n",
       "      <td>2024-02-29 23:58:00</td>\n",
       "      <td>1.169353</td>\n",
       "      <td>0.428489</td>\n",
       "      <td>0.830</td>\n",
       "      <td>-29.830</td>\n",
       "      <td>74.560</td>\n",
       "      <td>10.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21598</th>\n",
       "      <td>303.722</td>\n",
       "      <td>0.368659</td>\n",
       "      <td>0.665382</td>\n",
       "      <td>0.867538</td>\n",
       "      <td>0.951903</td>\n",
       "      <td>0.491276</td>\n",
       "      <td>0.454342</td>\n",
       "      <td>0.435431</td>\n",
       "      <td>0.421700</td>\n",
       "      <td>0.330298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210310</td>\n",
       "      <td>0.135741</td>\n",
       "      <td>0.242501</td>\n",
       "      <td>2024-02-29 23:59:00</td>\n",
       "      <td>1.015524</td>\n",
       "      <td>0.398757</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-130.552</td>\n",
       "      <td>303.722</td>\n",
       "      <td>7.790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21599 rows × 871 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        volume        X1        X2        X3        X4        X5        X6  \\\n",
       "0      181.331 -0.606128 -0.026132  0.240236  0.226543  0.009880 -0.207929   \n",
       "1      110.604 -0.613099 -0.036430  0.218995  0.209116  0.002885 -0.214304   \n",
       "2       70.052 -0.475340  0.217352  0.478561  0.476689  0.139481 -0.076333   \n",
       "3      124.021 -0.518950  0.118303  0.378718  0.383085  0.095079 -0.119611   \n",
       "4      131.966 -0.693000 -0.208907  0.029630  0.034411 -0.078318 -0.292848   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "21594   94.388  0.020155  0.076565  0.228994  0.288856  0.151634  0.108347   \n",
       "21595  177.372  0.016262  0.062527  0.214072  0.276463  0.146521  0.104164   \n",
       "21596  101.252  0.045407  0.109834  0.263577  0.329266  0.174214  0.132940   \n",
       "21597   74.560  0.124783  0.244168  0.408704  0.480016  0.251493  0.211727   \n",
       "21598  303.722  0.368659  0.665382  0.867538  0.951903  0.491276  0.454342   \n",
       "\n",
       "             X7        X8        X9  ...      X888      X889      X890  \\\n",
       "0     -0.308342 -0.414538 -0.952021  ...  0.209209  0.137915  0.319477   \n",
       "1     -0.314875 -0.421216 -0.687147  ...  0.208630  0.137725  0.319256   \n",
       "2     -0.176871 -0.283260 -0.614209  ...  0.208053  0.137534  0.319035   \n",
       "3     -0.220175 -0.326643 -0.833682  ...  0.207478  0.137344  0.318814   \n",
       "4     -0.393678 -0.500346 -0.841456  ...  0.206905  0.137154  0.318594   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "21594  0.088073  0.073729  0.071211  ...  0.212651  0.136494  0.243172   \n",
       "21595  0.084063  0.069788  0.024066  ...  0.212063  0.136305  0.243004   \n",
       "21596  0.113052  0.098865 -0.057370  ...  0.211477  0.136117  0.242836   \n",
       "21597  0.192160  0.178116  0.111335  ...  0.210892  0.135928  0.242668   \n",
       "21598  0.435431  0.421700  0.330298  ...  0.210310  0.135741  0.242501   \n",
       "\n",
       "                timestamp  bidask_ratio  buysell_ratio  bidask_delta  \\\n",
       "0     2024-02-15 00:00:00      1.307461       0.502154         1.125   \n",
       "1     2024-02-15 00:01:00      0.017929       0.400831       -10.517   \n",
       "2     2024-02-15 00:02:00      1.998738       1.051123         3.957   \n",
       "3     2024-02-15 00:03:00      0.476843       1.597897        -4.846   \n",
       "4     2024-02-15 00:04:00      3.877078       0.917581         6.577   \n",
       "...                   ...           ...            ...           ...   \n",
       "21594 2024-02-29 23:55:00      0.611756       0.705263        -2.642   \n",
       "21595 2024-02-29 23:56:00      0.564317       1.640604        -1.768   \n",
       "21596 2024-02-29 23:57:00      1.438736       2.292427         1.597   \n",
       "21597 2024-02-29 23:58:00      1.169353       0.428489         0.830   \n",
       "21598 2024-02-29 23:59:00      1.015524       0.398757         0.060   \n",
       "\n",
       "       buysell_delta  buysell_size  bidask_size  \n",
       "0            -60.097       181.331        8.443  \n",
       "1            -47.308       110.604       10.901  \n",
       "2              1.746        70.052       11.881  \n",
       "3             28.543       124.021       13.680  \n",
       "4             -5.672       131.966       11.149  \n",
       "...              ...           ...          ...  \n",
       "21594        -16.314        94.388       10.968  \n",
       "21595         43.030       177.372        6.348  \n",
       "21596         39.746       101.252        8.877  \n",
       "21597        -29.830        74.560       10.632  \n",
       "21598       -130.552       303.722        7.790  \n",
       "\n",
       "[21599 rows x 871 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X.filter([\n",
    "    pl.col(\"timestamp\") >= train_splits[\"last_2w\"]\n",
    "]).to_pandas()\n",
    "\n",
    "y = y.filter([\n",
    "    pl.col(\"timestamp\") >= train_splits[\"last_2w\"]\n",
    "]).select(\n",
    "    pl.col(\"label\")\n",
    ").to_pandas()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aa210c",
   "metadata": {},
   "source": [
    "# 1 Sklearn Feature Selection Decomposition + Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1749915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SklearnFeatureEngineering Instantiated\n",
      "... Fitting XGBRegressor on 21599 rows and 870 features.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339307ef217740958a38a48977f011c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running feature selection:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import (\n",
    "    VarianceThreshold, SelectKBest, SelectPercentile,\n",
    "    GenericUnivariateSelect, SelectFpr, SelectFdr, SelectFwe,\n",
    "    RFE, RFECV, SelectFromModel, SequentialFeatureSelector,\n",
    "    f_regression, mutual_info_regression\n",
    ")\n",
    "from sklearn.decomposition import (\n",
    "    PCA, IncrementalPCA, TruncatedSVD,\n",
    "    FastICA, SparsePCA, MiniBatchSparsePCA,\n",
    "    DictionaryLearning, MiniBatchDictionaryLearning,\n",
    "    FactorAnalysis, NMF, LatentDirichletAllocation\n",
    ")\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from sklearn.cluster import FeatureAgglomeration\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm.auto import tqdm\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import clone\n",
    "\n",
    "class SklearnFeatureEngineeringRegression:\n",
    "    def __init__(self, X: pd.DataFrame, y: pd.Series):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : pd.DataFrame\n",
    "            Predictor matrix (n_samples × P features)\n",
    "        y : pd.Series\n",
    "            Target vector (n_samples,)\n",
    "        \"\"\"\n",
    "        self.X_df = X.copy()\n",
    "        self.X = X.to_numpy()\n",
    "        self.y = y.to_numpy()\n",
    "        self.features = X.columns.tolist()\n",
    "        self.results_df: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "        # … your import logic deciding CPU vs GPU …\n",
    "        if os.environ.get(\"USER\"):\n",
    "            self.tree_model = XGBRegressor(\n",
    "                tree_method=\"hist\", n_estimators=1000, max_depth=100,\n",
    "                learning_rate=0.05, random_state=42, n_jobs=-1\n",
    "            )\n",
    "        else:\n",
    "            import cupy as cp\n",
    "            self.X = cp.asarray(X)\n",
    "            self.y = cp.asarray(y)\n",
    "            self.tree_model = XGBRegressor(\n",
    "                tree_method =\"hist\", device=\"cuda\",\n",
    "                n_estimators=10, max_depth=10,\n",
    "                learning_rate=0.1, random_state=42, n_jobs=-1\n",
    "            )\n",
    "        print(f\"\"\"SklearnFeatureEngineering Instantiated\"\"\")\n",
    "        print(f\"... Fitting {self.tree_model.__class__.__name__} on {self.X_df.shape[0]} rows and {self.X_df.shape[1]} features.\")\n",
    "        self.tree_model.fit(self.X, self.y)\n",
    "\n",
    "    # ---------- Filter methods ----------\n",
    "    # VarianceThreshold is a filter method that removes features with low variance.\n",
    "    def _variance_threshold(self, thresh: float = 0.0) -> pd.Series:\n",
    "        sel = VarianceThreshold(threshold=thresh).fit(self.X)\n",
    "        return pd.Series(sel.get_support(), index=self.features).astype(int)\n",
    "    # SelectKBest and SelectPercentile are filter methods that select features based on univariate statistical tests.\n",
    "    def _select_kbest_freg(self, k: int = 10) -> pd.Series:\n",
    "        sel = SelectKBest(score_func=f_regression, k=min(k, self.X.shape[1]))\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "    def _select_kbest_mutualinfo(self, k: int = 10) -> pd.Series:\n",
    "        sel = SelectKBest(score_func=mutual_info_regression, k=min(k, self.X.shape[1]))\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "    def _select_percentile_freg(self, p: float = 10) -> pd.Series:\n",
    "        sel = SelectPercentile(score_func=f_regression, percentile=min(p, 100))\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "    def _select_percentile_mutualinfo(self, p: float = 10) -> pd.Series:\n",
    "        sel = SelectPercentile(score_func=mutual_info_regression, percentile=min(p, 100))\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "    # # GenericUnivariateSelect is a filter method that allows for more flexible selection criteria.\n",
    "    # def _generic_univariate(self, k: int = 10) -> pd.Series:\n",
    "    #     sel = GenericUnivariateSelect(score_func=f_regression, mode='k_best', param=min(k, self.X.shape[1]))\n",
    "    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    def _select_fpr_freg(self, alpha: float = 0.05) -> pd.Series:\n",
    "        sel = SelectFpr(score_func=f_regression, alpha=alpha)\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    def _select_fdr_freg(self, alpha: float = 0.05) -> pd.Series:\n",
    "        sel = SelectFdr(score_func=f_regression, alpha=alpha)\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    def _select_fwe_freg(self, alpha: float = 0.05) -> pd.Series:\n",
    "        sel = SelectFwe(score_func=f_regression, alpha=alpha)\n",
    "        return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    # ---------- Wrapper and Embedded methods ----------\n",
    "    # def _rfe_lasso(self, n_features: int = 10) -> pd.Series:\n",
    "    #     estimator = LassoCV(cv=5, max_iter=5000).fit(self.X, self.y)\n",
    "    #     sel = RFE(estimator, n_features_to_select=min(n_features, self.X.shape[1]))\n",
    "    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    # def _rfecv_lasso(self) -> pd.Series:\n",
    "    #     estimator = LassoCV(cv=5, max_iter=5000).fit(self.X, self.y)\n",
    "    #     sel = RFECV(estimator, cv=5)\n",
    "    #     return pd.Series(sel.fit(self.X, self.y).get_support(), index=self.features).astype(int)\n",
    "\n",
    "    def _rfe_xgb(self, n_features: int = 10) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Recursive Feature Elimination with a fresh clone of self.tree_model.\n",
    "        \"\"\"\n",
    "        # clone preserves GPU/CPU config + hyper‐parameters\n",
    "        estimator = clone(self.tree_model)\n",
    "        sel = RFE(estimator, n_features_to_select=min(n_features, self.X.shape[1]))\n",
    "        mask = sel.fit(self.X, self.y).get_support()\n",
    "        return pd.Series(mask.astype(int), index=self.features).astype(int)\n",
    "\n",
    "    def _rfecv_xgb(self, cv: int = 5) -> pd.Series:\n",
    "        \"\"\"\n",
    "        RFECV with a fresh clone of self.tree_model.\n",
    "        \"\"\"\n",
    "        estimator = clone(self.tree_model)\n",
    "        sel = RFECV(estimator, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "        mask = sel.fit(self.X, self.y).get_support()\n",
    "        return pd.Series(mask.astype(int), index=self.features).astype(int)\n",
    "    \n",
    "    def _select_from_model_tree(self, threshold=\"median\") -> pd.Series:\n",
    "        \"\"\"\n",
    "        Uses the pre‐fitted self.tree_model via prefit=True.\n",
    "        \"\"\"\n",
    "        sel = SelectFromModel(self.tree_model, threshold=threshold, prefit=True)\n",
    "        mask = sel.get_support()\n",
    "        return pd.Series(mask.astype(int), index=self.features)\n",
    "\n",
    "    def _sequential_tree(\n",
    "        self,\n",
    "        n_features: int = 10,\n",
    "        direction: str = \"forward\",\n",
    "        cv: int = 5,\n",
    "        n_jobs: int = -1\n",
    "    ) -> pd.Series:\n",
    "        \"\"\"\n",
    "        SequentialFeatureSelector with XGBRegressor.\n",
    "\n",
    "        - Clones self.tree_model to preserve GPU/CPU config.\n",
    "        - direction: 'forward' or 'backward'\n",
    "        - cv: number of cross‐validation folds\n",
    "        - n_jobs: parallel jobs for CV\n",
    "        \"\"\"\n",
    "        # ensure we don't modify the original fitted model\n",
    "        estimator = clone(self.tree_model)\n",
    "\n",
    "        sfs = SequentialFeatureSelector(\n",
    "            estimator,\n",
    "            n_features_to_select=min(n_features, self.X.shape[1]),\n",
    "            direction=direction,\n",
    "            cv=cv,\n",
    "            n_jobs=n_jobs\n",
    "        )\n",
    "        mask = sfs.fit(self.X, self.y).get_support()\n",
    "        return pd.Series(mask.astype(int), index=self.features)\n",
    "\n",
    "    # ---------- Decomposition methods ----------\n",
    "    def _pca(self, n_components: float = 0.95) -> pd.Series:\n",
    "        pca = PCA(n_components=n_components).fit(self.X)\n",
    "        load = pd.DataFrame(pca.components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _incremental_pca(self, n_components: float = 0.95) -> pd.Series:\n",
    "        ipca = IncrementalPCA(n_components=n_components)\n",
    "        load = pd.DataFrame(ipca.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _truncated_svd(self, n_components: int = 10) -> pd.Series:\n",
    "        k = min(n_components, self.X.shape[1])\n",
    "        ts = TruncatedSVD(n_components=k)\n",
    "        load = pd.DataFrame(ts.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _fast_ica(self, n_components: int = 10) -> pd.Series:\n",
    "        ic = FastICA(n_components=min(n_components, self.X.shape[1]), max_iter=200)\n",
    "        load = pd.DataFrame(ic.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _sparse_pca(self, n_components: int = 10) -> pd.Series:\n",
    "        spca = SparsePCA(n_components=min(n_components, self.X.shape[1]), alpha=1, max_iter=1000)\n",
    "        load = pd.DataFrame(spca.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _minibatch_sparse_pca(self, n_components: int = 10) -> pd.Series:\n",
    "        mbspca = MiniBatchSparsePCA(n_components=min(n_components, self.X.shape[1]), alpha=1)\n",
    "        load = pd.DataFrame(mbspca.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _dict_learning(self, n_components: int = 10) -> pd.Series:\n",
    "        dl = DictionaryLearning(n_components=min(n_components, self.X.shape[1]), alpha=1, max_iter=1000)\n",
    "        load = pd.DataFrame(dl.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _minibatch_dict_learning(self, n_components: int = 10) -> pd.Series:\n",
    "        mbdl = MiniBatchDictionaryLearning(n_components=min(n_components, self.X.shape[1]), alpha=1)\n",
    "        load = pd.DataFrame(mbdl.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _factor_analysis(self, n_components: int = 10) -> pd.Series:\n",
    "        fa = FactorAnalysis(n_components=min(n_components, self.X.shape[1]))\n",
    "        load = pd.DataFrame(fa.fit(self.X).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _nmf(self, n_components: int = 10) -> pd.Series:\n",
    "        nmf = NMF(n_components=min(n_components, self.X.shape[1]), init='nndsvda', max_iter=500)\n",
    "        data_pos = self.X.clip(lower=0)\n",
    "        load = pd.DataFrame(nmf.fit(data_pos).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    def _lda(self, n_components: int = 10) -> pd.Series:\n",
    "        lda = LatentDirichletAllocation(n_components=min(n_components, self.X.shape[1]), max_iter=5)\n",
    "        data_pos = self.X.clip(lower=0)\n",
    "        load = pd.DataFrame(lda.fit(data_pos).components_.T, index=self.features)\n",
    "        thresh = load.abs().mean().mean()\n",
    "        return (load.abs().max(axis=1) >= thresh).astype(int)\n",
    "\n",
    "    # ---- Random projections ----\n",
    "    def _gaussian_random_projection(self, n_components: int = 10) -> pd.Series:\n",
    "        rp = GaussianRandomProjection(n_components=min(n_components, self.X.shape[1]))\n",
    "        comp = pd.DataFrame(rp.fit(self.X).components_.T, index=self.features)\n",
    "        return self._decomp_mask(comp)\n",
    "\n",
    "    def _sparse_random_projection(self, n_components: int = 10) -> pd.Series:\n",
    "        srp = SparseRandomProjection(n_components=min(n_components, self.X.shape[1]))\n",
    "        comp = pd.DataFrame(srp.fit(self.X).components_.T, index=self.features)\n",
    "        return self._decomp_mask(comp)\n",
    "\n",
    "    # ---- Feature grouping ----\n",
    "    def _feature_agglomeration(self, n_clusters: int = 10) -> pd.Series:\n",
    "        # Fit agglomeration on the array\n",
    "        agg = FeatureAgglomeration(n_clusters=min(n_clusters, self.X.shape[1]))\n",
    "        agg.fit(self.X)\n",
    "        labels       = agg.labels_\n",
    "        cluster_data = agg.transform(self.X)\n",
    "\n",
    "        # Build a temporary DataFrame for cluster components\n",
    "        df_clust = pd.DataFrame(\n",
    "            cluster_data,\n",
    "            columns=[f\"clus_{i}\" for i in range(cluster_data.shape[1])],\n",
    "            index=self.X_df.index\n",
    "        )\n",
    "\n",
    "        # For each cluster, pick the feature with highest corr to its component\n",
    "        mask = pd.Series(0, index=self.features)\n",
    "        for j in range(cluster_data.shape[1]):\n",
    "            members = [f for f, l in zip(self.features, labels) if l == j]\n",
    "            corrs   = self.X_df[members].corrwith(df_clust.iloc[:, j]).abs()\n",
    "            mask[corrs.idxmax()] = 1\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def run(self, methods=None) -> None:\n",
    "        \"\"\"\n",
    "        Execute each selection/decomposition method with a progress bar,\n",
    "        accepting either:\n",
    "          - a dict mapping method names to parameter dicts, or\n",
    "          - a list of method names (no parameters).\n",
    "\n",
    "        Assembles `results_df` with columns ['model', features..., 'total_score'].\n",
    "        \"\"\"\n",
    "        # default methods with default params\n",
    "        default_list = [\n",
    "            '_variance_threshold',\n",
    "            '_select_kbest_freg','_select_percentile_freg',\n",
    "            '_select_kbest_mutualinfo','_select_percentile_mutualinfo',\n",
    "            '_select_fpr_freg','_select_fdr_freg','_select_fwe_freg',\n",
    "            '_rfe_xgb','_rfecv_xgb',\n",
    "            '_select_from_model_tree','_sequential_tree',\n",
    "            '_pca','_incremental_pca','_truncated_svd','_fast_ica',\n",
    "            '_sparse_pca','_minibatch_sparse_pca','_dict_learning',\n",
    "            '_minibatch_dict_learning','_factor_analysis','_nmf','_lda',\n",
    "            '_gaussian_random_projection','_sparse_random_projection',\n",
    "            '_feature_agglomeration'\n",
    "        ]\n",
    "        if methods is None:\n",
    "            methods = {m: {} for m in default_list}\n",
    "        elif isinstance(methods, list):\n",
    "            methods = {m: {} for m in methods}\n",
    "        elif not isinstance(methods, dict):\n",
    "            raise ValueError(\"`methods` must be None, list, or dict\")\n",
    "\n",
    "        records = []\n",
    "        for m, params in tqdm(methods.items(), desc='Running feature selection'):\n",
    "            if not hasattr(self, m):\n",
    "                raise KeyError(f\"Method {m} not found in class\")\n",
    "            fn = getattr(self, m)\n",
    "            mask = fn(**params)\n",
    "            records.append({'model': m.lstrip('_'), **mask.to_dict(), 'total_score': int(mask.sum())})\n",
    "        self.results_df = pd.DataFrame.from_records(records)\n",
    "\n",
    "    def get_results(self) -> pd.DataFrame:\n",
    "        \"\"\"Return the raw selection matrix (n_models × (P + 2)).\"\"\"\n",
    "        return self.results_df\n",
    "\n",
    "    def get_top_features(self, N: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Aggregate across models (vote count = sum of 1’s per feature),\n",
    "        and return the top N feature names.\n",
    "        \"\"\"\n",
    "        return pd.DataFrame(features.results_df.loc[:, features.features].sum().sort_values(ascending=False), columns=['score']).head(N)\n",
    "    \n",
    "    def get_model_featuresincluded(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns a DataFrame with number of features included per model.\n",
    "        \"\"\"\n",
    "        model_features = self.results_df.set_index(\"model\").sum(axis=1).reset_index()\n",
    "        model_features.columns = [\"model\", \"features_included\"]\n",
    "        return model_features\n",
    "\n",
    "features = SklearnFeatureEngineeringRegression(\n",
    "    X = X.drop(columns=[\"timestamp\"]),\n",
    "    y = y\n",
    ")\n",
    "features.run(methods=[\"_pca\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a86f1c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>volume</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>...</th>\n",
       "      <th>X888</th>\n",
       "      <th>X889</th>\n",
       "      <th>X890</th>\n",
       "      <th>bidask_ratio</th>\n",
       "      <th>buysell_ratio</th>\n",
       "      <th>bidask_delta</th>\n",
       "      <th>buysell_delta</th>\n",
       "      <th>buysell_size</th>\n",
       "      <th>bidask_size</th>\n",
       "      <th>total_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pca</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 872 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  volume  X1  X2  X3  X4  X5  X6  X7  X8  ...  X888  X889  X890  \\\n",
       "0   pca       1   0   0   0   0   0   0   0   0  ...     0     0     0   \n",
       "\n",
       "   bidask_ratio  buysell_ratio  bidask_delta  buysell_delta  buysell_size  \\\n",
       "0             1              0             1              1             1   \n",
       "\n",
       "   bidask_size  total_score  \n",
       "0            0            5  \n",
       "\n",
       "[1 rows x 872 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "851e6398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buysell_size</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buysell_delta</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bidask_delta</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bidask_ratio</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X577</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X581</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X580</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X579</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X578</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X574</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X576</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X575</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X583</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X573</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X572</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X582</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X585</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X584</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X570</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               score\n",
       "volume             1\n",
       "buysell_size       1\n",
       "buysell_delta      1\n",
       "bidask_delta       1\n",
       "bidask_ratio       1\n",
       "X577               0\n",
       "X581               0\n",
       "X580               0\n",
       "X579               0\n",
       "X578               0\n",
       "X574               0\n",
       "X576               0\n",
       "X575               0\n",
       "X583               0\n",
       "X573               0\n",
       "X572               0\n",
       "X582               0\n",
       "X585               0\n",
       "X584               0\n",
       "X570               0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.get_top_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "156f855b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>features_included</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pca</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  features_included\n",
       "0   pca                 10"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.get_model_featuresincluded()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1320a14",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)-based selection**\n",
    "\n",
    "  1. Fit PCA on the centered feature matrix.\n",
    "  2. Examine the loading matrix $L\\in\\mathbb R^{p\\times k}$, where each column $L_j$ gives coefficients of the $j$ th principal component in the original feature basis.\n",
    "  3. For each original feature $i$, compute its overall importance score, e.g.\\ $\\sum_{j=1}^k |L_{ij}|\\times\\lambda_j$, where $\\lambda_j$ is the variance explained by component $j$.\n",
    "  4. Select the top-$m$ features by that score or apply a threshold on absolute loading magnitude.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58572c99",
   "metadata": {},
   "source": [
    "* **Factor Analysis (FA)-based selection**\n",
    "\n",
    "  1. Fit a common‐factor model to capture the shared covariance among features.\n",
    "  2. Obtain the factor loading matrix $\\Lambda\\in\\mathbb R^{p\\times q}$ and communalities $h_i^2=\\sum_{j}\\Lambda_{ij}^2$.\n",
    "  3. Use each feature’s communality $h_i^2$ as its importance (higher = more explained by common factors).\n",
    "  4. Retain features with highest communalities up to your budget.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6c49b5",
   "metadata": {},
   "source": [
    "**Independent Component Analysis (ICA)-based selection**\n",
    "\n",
    "  1. Perform ICA to decompose features into statistically independent sources: $X = A\\,S$.\n",
    "  2. Use the absolute mixing‐matrix weights $|A_{i\\,j}|$ as indications of how strongly feature $i$ contributes to source $j$.\n",
    "  3. Aggregate per-feature scores, e.g.\\ $\\sum_j|A_{ij}|$, and select those above a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bda4a4",
   "metadata": {},
   "source": [
    "* **Canonical Correlation Analysis (CCA)-based selection**\n",
    "\n",
    "  1. Given feature block $X$ and target $y$, form a second “block” $Y=y$ (or include engineered target lags).\n",
    "  2. Solve for weight vectors $u,v$ that maximize $\\mathrm{corr}(X u, Y v)$.\n",
    "  3. The canonical weights $u_i$ indicate feature relevance; select features with largest $|u_i|$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb74c8",
   "metadata": {},
   "source": [
    "* **Correlation (Pearson) filter**\n",
    "\n",
    "  1. Compute Pearson correlation $r_i = \\mathrm{corr}(X_i, y)$ for each feature $X_i$.\n",
    "  2. Rank by $|r_i|$ and choose top-$m$ features or those exceeding a predefined $|r|$ threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dcb653",
   "metadata": {},
   "source": [
    "\n",
    "* **Mutual Information (MI) filter**\n",
    "\n",
    "  1. Estimate $I(X_i;y)$ nonparametrically (e.g. via k-nearest neighbors).\n",
    "  2. Rank features by MI score and select the top fraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1b7dc0",
   "metadata": {},
   "source": [
    "* **F-regression (univariate linear F-test)**\n",
    "\n",
    "  1. For each feature $X_i$, fit the simple linear model $y = \\beta_i X_i + \\varepsilon$.\n",
    "  2. Compute the F statistic $F_i = \\frac{\\mathrm{SSR}/1}{\\mathrm{SSE}/(n-2)}$.\n",
    "  3. Select features with the highest $F_i$ values (or lowest p-values).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9955a",
   "metadata": {},
   "source": [
    "* **RReliefF (regression version of ReliefF)**\n",
    "\n",
    "  1. For a random sample of observations, find nearest‐neighbor pairs weighted by distance.\n",
    "  2. Update feature weights by how well differences in $X_i$ predict differences in $y$.\n",
    "  3. After many iterations, keep features with the largest Relief scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
